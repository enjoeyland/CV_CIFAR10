{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nrd4rdBvX46W",
        "outputId": "02217649-2e46-4c16-a56b-bcc0c2fbc802"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.11.0+cu113\n",
            "Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJO4AenrYK7N",
        "outputId": "17cfd4c2-a3a1-468c-a647-3f9e8309af84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "USE 1 GPUs!\n",
            "Epoch: 0 | Batch_idx: 0 |  Loss: (2.5304) | Acc: (7.03%) (9/128)\n",
            "Epoch: 0 | Batch_idx: 10 |  Loss: (19.0394) | Acc: (8.66%) (122/1408)\n",
            "Epoch: 0 | Batch_idx: 20 |  Loss: (14.0556) | Acc: (10.31%) (277/2688)\n",
            "Epoch: 0 | Batch_idx: 30 |  Loss: (10.3583) | Acc: (10.03%) (398/3968)\n",
            "Epoch: 0 | Batch_idx: 40 |  Loss: (8.4115) | Acc: (10.14%) (532/5248)\n",
            "Epoch: 0 | Batch_idx: 50 |  Loss: (7.2158) | Acc: (9.93%) (648/6528)\n",
            "Epoch: 0 | Batch_idx: 60 |  Loss: (6.4111) | Acc: (10.09%) (788/7808)\n",
            "Epoch: 0 | Batch_idx: 70 |  Loss: (5.8330) | Acc: (10.09%) (917/9088)\n",
            "Epoch: 0 | Batch_idx: 80 |  Loss: (5.3970) | Acc: (10.02%) (1039/10368)\n",
            "Epoch: 0 | Batch_idx: 90 |  Loss: (5.0572) | Acc: (10.04%) (1170/11648)\n",
            "Epoch: 0 | Batch_idx: 100 |  Loss: (4.7842) | Acc: (10.16%) (1313/12928)\n",
            "Epoch: 0 | Batch_idx: 110 |  Loss: (4.5600) | Acc: (10.15%) (1442/14208)\n",
            "Epoch: 0 | Batch_idx: 120 |  Loss: (4.3730) | Acc: (10.27%) (1591/15488)\n",
            "Epoch: 0 | Batch_idx: 130 |  Loss: (4.2145) | Acc: (10.33%) (1732/16768)\n",
            "Epoch: 0 | Batch_idx: 140 |  Loss: (4.0788) | Acc: (10.39%) (1875/18048)\n",
            "Epoch: 0 | Batch_idx: 150 |  Loss: (3.9615) | Acc: (10.48%) (2026/19328)\n",
            "Epoch: 0 | Batch_idx: 160 |  Loss: (3.8583) | Acc: (10.48%) (2160/20608)\n",
            "Epoch: 0 | Batch_idx: 170 |  Loss: (3.7673) | Acc: (10.55%) (2309/21888)\n",
            "Epoch: 0 | Batch_idx: 180 |  Loss: (3.6858) | Acc: (10.58%) (2452/23168)\n",
            "Epoch: 0 | Batch_idx: 190 |  Loss: (3.6133) | Acc: (10.67%) (2608/24448)\n",
            "Epoch: 0 | Batch_idx: 200 |  Loss: (3.5479) | Acc: (10.66%) (2742/25728)\n",
            "Epoch: 0 | Batch_idx: 210 |  Loss: (3.4886) | Acc: (10.69%) (2886/27008)\n",
            "Epoch: 0 | Batch_idx: 220 |  Loss: (3.4347) | Acc: (10.71%) (3031/28288)\n",
            "Epoch: 0 | Batch_idx: 230 |  Loss: (3.3851) | Acc: (10.74%) (3175/29568)\n",
            "Epoch: 0 | Batch_idx: 240 |  Loss: (3.3399) | Acc: (10.81%) (3335/30848)\n",
            "Epoch: 0 | Batch_idx: 250 |  Loss: (3.2980) | Acc: (10.85%) (3486/32128)\n",
            "Epoch: 0 | Batch_idx: 260 |  Loss: (3.2595) | Acc: (10.85%) (3625/33408)\n",
            "Epoch: 0 | Batch_idx: 270 |  Loss: (3.2238) | Acc: (10.88%) (3774/34688)\n",
            "Epoch: 0 | Batch_idx: 280 |  Loss: (3.1904) | Acc: (10.99%) (3954/35968)\n",
            "Epoch: 0 | Batch_idx: 290 |  Loss: (3.1593) | Acc: (11.06%) (4119/37248)\n",
            "Epoch: 0 | Batch_idx: 300 |  Loss: (3.1303) | Acc: (11.15%) (4296/38528)\n",
            "Epoch: 0 | Batch_idx: 310 |  Loss: (3.1028) | Acc: (11.20%) (4460/39808)\n",
            "Epoch: 0 | Batch_idx: 320 |  Loss: (3.0776) | Acc: (11.17%) (4588/41088)\n",
            "Epoch: 0 | Batch_idx: 330 |  Loss: (3.0537) | Acc: (11.23%) (4760/42368)\n",
            "Epoch: 0 | Batch_idx: 340 |  Loss: (3.0313) | Acc: (11.25%) (4911/43648)\n",
            "Epoch: 0 | Batch_idx: 350 |  Loss: (3.0099) | Acc: (11.32%) (5088/44928)\n",
            "Epoch: 0 | Batch_idx: 360 |  Loss: (2.9899) | Acc: (11.35%) (5243/46208)\n",
            "Epoch: 0 | Batch_idx: 370 |  Loss: (2.9708) | Acc: (11.43%) (5426/47488)\n",
            "Epoch: 0 | Batch_idx: 380 |  Loss: (2.9527) | Acc: (11.50%) (5608/48768)\n",
            "Epoch: 0 | Batch_idx: 390 |  Loss: (2.9357) | Acc: (11.52%) (5762/50000)\n",
            "# TEST : Loss: (2.2873) | Acc: (12.05%) (1205/10000)\n",
            "Epoch: 1 | Batch_idx: 0 |  Loss: (2.2923) | Acc: (11.72%) (15/128)\n",
            "Epoch: 1 | Batch_idx: 10 |  Loss: (2.2761) | Acc: (13.92%) (196/1408)\n",
            "Epoch: 1 | Batch_idx: 20 |  Loss: (2.2763) | Acc: (13.58%) (365/2688)\n",
            "Epoch: 1 | Batch_idx: 30 |  Loss: (2.2797) | Acc: (13.46%) (534/3968)\n",
            "Epoch: 1 | Batch_idx: 40 |  Loss: (2.2805) | Acc: (12.98%) (681/5248)\n",
            "Epoch: 1 | Batch_idx: 50 |  Loss: (2.2810) | Acc: (12.78%) (834/6528)\n",
            "Epoch: 1 | Batch_idx: 60 |  Loss: (2.2788) | Acc: (13.06%) (1020/7808)\n",
            "Epoch: 1 | Batch_idx: 70 |  Loss: (2.2791) | Acc: (13.00%) (1181/9088)\n",
            "Epoch: 1 | Batch_idx: 80 |  Loss: (2.2778) | Acc: (12.98%) (1346/10368)\n",
            "Epoch: 1 | Batch_idx: 90 |  Loss: (2.2771) | Acc: (13.08%) (1523/11648)\n",
            "Epoch: 1 | Batch_idx: 100 |  Loss: (2.2753) | Acc: (13.13%) (1698/12928)\n",
            "Epoch: 1 | Batch_idx: 110 |  Loss: (2.2745) | Acc: (13.22%) (1878/14208)\n",
            "Epoch: 1 | Batch_idx: 120 |  Loss: (2.2747) | Acc: (13.25%) (2052/15488)\n",
            "Epoch: 1 | Batch_idx: 130 |  Loss: (2.2749) | Acc: (13.31%) (2232/16768)\n",
            "Epoch: 1 | Batch_idx: 140 |  Loss: (2.2756) | Acc: (13.23%) (2387/18048)\n",
            "Epoch: 1 | Batch_idx: 150 |  Loss: (2.2763) | Acc: (13.27%) (2564/19328)\n",
            "Epoch: 1 | Batch_idx: 160 |  Loss: (2.2761) | Acc: (13.31%) (2743/20608)\n",
            "Epoch: 1 | Batch_idx: 170 |  Loss: (2.2759) | Acc: (13.34%) (2919/21888)\n",
            "Epoch: 1 | Batch_idx: 180 |  Loss: (2.2762) | Acc: (13.26%) (3071/23168)\n",
            "Epoch: 1 | Batch_idx: 190 |  Loss: (2.2750) | Acc: (13.28%) (3246/24448)\n",
            "Epoch: 1 | Batch_idx: 200 |  Loss: (2.2746) | Acc: (13.31%) (3425/25728)\n",
            "Epoch: 1 | Batch_idx: 210 |  Loss: (2.2750) | Acc: (13.33%) (3599/27008)\n",
            "Epoch: 1 | Batch_idx: 220 |  Loss: (2.2752) | Acc: (13.34%) (3774/28288)\n",
            "Epoch: 1 | Batch_idx: 230 |  Loss: (2.2746) | Acc: (13.45%) (3976/29568)\n",
            "Epoch: 1 | Batch_idx: 240 |  Loss: (2.2744) | Acc: (13.45%) (4148/30848)\n",
            "Epoch: 1 | Batch_idx: 250 |  Loss: (2.2737) | Acc: (13.49%) (4335/32128)\n",
            "Epoch: 1 | Batch_idx: 260 |  Loss: (2.2739) | Acc: (13.46%) (4498/33408)\n",
            "Epoch: 1 | Batch_idx: 270 |  Loss: (2.2736) | Acc: (13.49%) (4681/34688)\n",
            "Epoch: 1 | Batch_idx: 280 |  Loss: (2.2732) | Acc: (13.54%) (4870/35968)\n",
            "Epoch: 1 | Batch_idx: 290 |  Loss: (2.2736) | Acc: (13.53%) (5039/37248)\n",
            "Epoch: 1 | Batch_idx: 300 |  Loss: (2.2735) | Acc: (13.51%) (5204/38528)\n",
            "Epoch: 1 | Batch_idx: 310 |  Loss: (2.2737) | Acc: (13.49%) (5372/39808)\n",
            "Epoch: 1 | Batch_idx: 320 |  Loss: (2.2737) | Acc: (13.51%) (5549/41088)\n",
            "Epoch: 1 | Batch_idx: 330 |  Loss: (2.2735) | Acc: (13.51%) (5725/42368)\n",
            "Epoch: 1 | Batch_idx: 340 |  Loss: (2.2731) | Acc: (13.58%) (5928/43648)\n",
            "Epoch: 1 | Batch_idx: 350 |  Loss: (2.2729) | Acc: (13.64%) (6126/44928)\n",
            "Epoch: 1 | Batch_idx: 360 |  Loss: (2.2725) | Acc: (13.66%) (6312/46208)\n",
            "Epoch: 1 | Batch_idx: 370 |  Loss: (2.2724) | Acc: (13.69%) (6500/47488)\n",
            "Epoch: 1 | Batch_idx: 380 |  Loss: (2.2721) | Acc: (13.74%) (6702/48768)\n",
            "Epoch: 1 | Batch_idx: 390 |  Loss: (2.2721) | Acc: (13.72%) (6859/50000)\n",
            "# TEST : Loss: (2.2695) | Acc: (13.41%) (1341/10000)\n",
            "Epoch: 2 | Batch_idx: 0 |  Loss: (2.2784) | Acc: (17.97%) (23/128)\n",
            "Epoch: 2 | Batch_idx: 10 |  Loss: (2.2694) | Acc: (14.84%) (209/1408)\n",
            "Epoch: 2 | Batch_idx: 20 |  Loss: (2.2686) | Acc: (14.62%) (393/2688)\n",
            "Epoch: 2 | Batch_idx: 30 |  Loss: (2.2646) | Acc: (14.67%) (582/3968)\n",
            "Epoch: 2 | Batch_idx: 40 |  Loss: (2.2634) | Acc: (14.60%) (766/5248)\n",
            "Epoch: 2 | Batch_idx: 50 |  Loss: (2.2609) | Acc: (15.07%) (984/6528)\n",
            "Epoch: 2 | Batch_idx: 60 |  Loss: (2.2593) | Acc: (14.96%) (1168/7808)\n",
            "Epoch: 2 | Batch_idx: 70 |  Loss: (2.2583) | Acc: (14.81%) (1346/9088)\n",
            "Epoch: 2 | Batch_idx: 80 |  Loss: (2.2580) | Acc: (14.96%) (1551/10368)\n",
            "Epoch: 2 | Batch_idx: 90 |  Loss: (2.2598) | Acc: (14.77%) (1720/11648)\n",
            "Epoch: 2 | Batch_idx: 100 |  Loss: (2.2600) | Acc: (14.71%) (1902/12928)\n",
            "Epoch: 2 | Batch_idx: 110 |  Loss: (2.2602) | Acc: (14.67%) (2084/14208)\n",
            "Epoch: 2 | Batch_idx: 120 |  Loss: (2.2593) | Acc: (14.66%) (2271/15488)\n",
            "Epoch: 2 | Batch_idx: 130 |  Loss: (2.2585) | Acc: (14.70%) (2465/16768)\n",
            "Epoch: 2 | Batch_idx: 140 |  Loss: (2.2576) | Acc: (14.78%) (2668/18048)\n",
            "Epoch: 2 | Batch_idx: 150 |  Loss: (2.2568) | Acc: (14.96%) (2892/19328)\n",
            "Epoch: 2 | Batch_idx: 160 |  Loss: (2.2563) | Acc: (15.07%) (3105/20608)\n",
            "Epoch: 2 | Batch_idx: 170 |  Loss: (2.2546) | Acc: (15.19%) (3324/21888)\n",
            "Epoch: 2 | Batch_idx: 180 |  Loss: (2.2526) | Acc: (15.24%) (3530/23168)\n",
            "Epoch: 2 | Batch_idx: 190 |  Loss: (2.2498) | Acc: (15.35%) (3753/24448)\n",
            "Epoch: 2 | Batch_idx: 200 |  Loss: (2.2476) | Acc: (15.41%) (3964/25728)\n",
            "Epoch: 2 | Batch_idx: 210 |  Loss: (2.2445) | Acc: (15.57%) (4205/27008)\n",
            "Epoch: 2 | Batch_idx: 220 |  Loss: (2.2416) | Acc: (15.69%) (4438/28288)\n",
            "Epoch: 2 | Batch_idx: 230 |  Loss: (2.2387) | Acc: (15.87%) (4692/29568)\n",
            "Epoch: 2 | Batch_idx: 240 |  Loss: (2.2350) | Acc: (16.07%) (4956/30848)\n",
            "Epoch: 2 | Batch_idx: 250 |  Loss: (2.2310) | Acc: (16.17%) (5196/32128)\n",
            "Epoch: 2 | Batch_idx: 260 |  Loss: (2.2279) | Acc: (16.37%) (5470/33408)\n",
            "Epoch: 2 | Batch_idx: 270 |  Loss: (2.2253) | Acc: (16.42%) (5696/34688)\n",
            "Epoch: 2 | Batch_idx: 280 |  Loss: (2.2204) | Acc: (16.65%) (5987/35968)\n",
            "Epoch: 2 | Batch_idx: 290 |  Loss: (2.2170) | Acc: (16.81%) (6262/37248)\n",
            "Epoch: 2 | Batch_idx: 300 |  Loss: (2.2131) | Acc: (16.98%) (6541/38528)\n",
            "Epoch: 2 | Batch_idx: 310 |  Loss: (2.2092) | Acc: (17.14%) (6822/39808)\n",
            "Epoch: 2 | Batch_idx: 320 |  Loss: (2.2047) | Acc: (17.27%) (7096/41088)\n",
            "Epoch: 2 | Batch_idx: 330 |  Loss: (2.1997) | Acc: (17.43%) (7383/42368)\n",
            "Epoch: 2 | Batch_idx: 340 |  Loss: (2.1959) | Acc: (17.53%) (7650/43648)\n",
            "Epoch: 2 | Batch_idx: 350 |  Loss: (2.1909) | Acc: (17.74%) (7968/44928)\n",
            "Epoch: 2 | Batch_idx: 360 |  Loss: (2.1863) | Acc: (17.88%) (8264/46208)\n",
            "Epoch: 2 | Batch_idx: 370 |  Loss: (2.1815) | Acc: (18.00%) (8548/47488)\n",
            "Epoch: 2 | Batch_idx: 380 |  Loss: (2.1759) | Acc: (18.18%) (8868/48768)\n",
            "Epoch: 2 | Batch_idx: 390 |  Loss: (2.1712) | Acc: (18.30%) (9152/50000)\n",
            "# TEST : Loss: (1.9440) | Acc: (25.39%) (2539/10000)\n",
            "Epoch: 3 | Batch_idx: 0 |  Loss: (1.9832) | Acc: (26.56%) (34/128)\n",
            "Epoch: 3 | Batch_idx: 10 |  Loss: (1.9864) | Acc: (24.22%) (341/1408)\n",
            "Epoch: 3 | Batch_idx: 20 |  Loss: (1.9918) | Acc: (24.44%) (657/2688)\n",
            "Epoch: 3 | Batch_idx: 30 |  Loss: (1.9895) | Acc: (24.75%) (982/3968)\n",
            "Epoch: 3 | Batch_idx: 40 |  Loss: (1.9776) | Acc: (25.17%) (1321/5248)\n",
            "Epoch: 3 | Batch_idx: 50 |  Loss: (1.9738) | Acc: (24.85%) (1622/6528)\n",
            "Epoch: 3 | Batch_idx: 60 |  Loss: (1.9722) | Acc: (24.46%) (1910/7808)\n",
            "Epoch: 3 | Batch_idx: 70 |  Loss: (1.9711) | Acc: (24.36%) (2214/9088)\n",
            "Epoch: 3 | Batch_idx: 80 |  Loss: (1.9629) | Acc: (24.65%) (2556/10368)\n",
            "Epoch: 3 | Batch_idx: 90 |  Loss: (1.9617) | Acc: (24.77%) (2885/11648)\n",
            "Epoch: 3 | Batch_idx: 100 |  Loss: (1.9574) | Acc: (24.93%) (3223/12928)\n",
            "Epoch: 3 | Batch_idx: 110 |  Loss: (1.9581) | Acc: (24.73%) (3514/14208)\n",
            "Epoch: 3 | Batch_idx: 120 |  Loss: (1.9569) | Acc: (24.85%) (3849/15488)\n",
            "Epoch: 3 | Batch_idx: 130 |  Loss: (1.9521) | Acc: (24.91%) (4177/16768)\n",
            "Epoch: 3 | Batch_idx: 140 |  Loss: (1.9512) | Acc: (24.91%) (4495/18048)\n",
            "Epoch: 3 | Batch_idx: 150 |  Loss: (1.9481) | Acc: (24.95%) (4823/19328)\n",
            "Epoch: 3 | Batch_idx: 160 |  Loss: (1.9461) | Acc: (25.01%) (5155/20608)\n",
            "Epoch: 3 | Batch_idx: 170 |  Loss: (1.9431) | Acc: (25.21%) (5517/21888)\n",
            "Epoch: 3 | Batch_idx: 180 |  Loss: (1.9409) | Acc: (25.31%) (5863/23168)\n",
            "Epoch: 3 | Batch_idx: 190 |  Loss: (1.9413) | Acc: (25.39%) (6207/24448)\n",
            "Epoch: 3 | Batch_idx: 200 |  Loss: (1.9383) | Acc: (25.55%) (6573/25728)\n",
            "Epoch: 3 | Batch_idx: 210 |  Loss: (1.9359) | Acc: (25.61%) (6917/27008)\n",
            "Epoch: 3 | Batch_idx: 220 |  Loss: (1.9339) | Acc: (25.60%) (7242/28288)\n",
            "Epoch: 3 | Batch_idx: 230 |  Loss: (1.9335) | Acc: (25.46%) (7529/29568)\n",
            "Epoch: 3 | Batch_idx: 240 |  Loss: (1.9339) | Acc: (25.46%) (7855/30848)\n",
            "Epoch: 3 | Batch_idx: 250 |  Loss: (1.9332) | Acc: (25.38%) (8155/32128)\n",
            "Epoch: 3 | Batch_idx: 260 |  Loss: (1.9317) | Acc: (25.45%) (8501/33408)\n",
            "Epoch: 3 | Batch_idx: 270 |  Loss: (1.9302) | Acc: (25.52%) (8853/34688)\n",
            "Epoch: 3 | Batch_idx: 280 |  Loss: (1.9272) | Acc: (25.64%) (9221/35968)\n",
            "Epoch: 3 | Batch_idx: 290 |  Loss: (1.9255) | Acc: (25.69%) (9569/37248)\n",
            "Epoch: 3 | Batch_idx: 300 |  Loss: (1.9242) | Acc: (25.72%) (9910/38528)\n",
            "Epoch: 3 | Batch_idx: 310 |  Loss: (1.9217) | Acc: (25.83%) (10284/39808)\n",
            "Epoch: 3 | Batch_idx: 320 |  Loss: (1.9195) | Acc: (25.93%) (10654/41088)\n",
            "Epoch: 3 | Batch_idx: 330 |  Loss: (1.9181) | Acc: (25.97%) (11005/42368)\n",
            "Epoch: 3 | Batch_idx: 340 |  Loss: (1.9156) | Acc: (26.07%) (11377/43648)\n",
            "Epoch: 3 | Batch_idx: 350 |  Loss: (1.9152) | Acc: (26.08%) (11718/44928)\n",
            "Epoch: 3 | Batch_idx: 360 |  Loss: (1.9139) | Acc: (26.13%) (12072/46208)\n",
            "Epoch: 3 | Batch_idx: 370 |  Loss: (1.9119) | Acc: (26.20%) (12442/47488)\n",
            "Epoch: 3 | Batch_idx: 380 |  Loss: (1.9112) | Acc: (26.28%) (12816/48768)\n",
            "Epoch: 3 | Batch_idx: 390 |  Loss: (1.9097) | Acc: (26.34%) (13169/50000)\n",
            "# TEST : Loss: (1.8092) | Acc: (30.00%) (3000/10000)\n",
            "Epoch: 4 | Batch_idx: 0 |  Loss: (1.7910) | Acc: (26.56%) (34/128)\n",
            "Epoch: 4 | Batch_idx: 10 |  Loss: (1.8177) | Acc: (29.05%) (409/1408)\n",
            "Epoch: 4 | Batch_idx: 20 |  Loss: (1.8181) | Acc: (29.43%) (791/2688)\n",
            "Epoch: 4 | Batch_idx: 30 |  Loss: (1.8113) | Acc: (30.39%) (1206/3968)\n",
            "Epoch: 4 | Batch_idx: 40 |  Loss: (1.8066) | Acc: (30.58%) (1605/5248)\n",
            "Epoch: 4 | Batch_idx: 50 |  Loss: (1.8133) | Acc: (30.06%) (1962/6528)\n",
            "Epoch: 4 | Batch_idx: 60 |  Loss: (1.8137) | Acc: (30.26%) (2363/7808)\n",
            "Epoch: 4 | Batch_idx: 70 |  Loss: (1.8113) | Acc: (30.55%) (2776/9088)\n",
            "Epoch: 4 | Batch_idx: 80 |  Loss: (1.8087) | Acc: (30.74%) (3187/10368)\n",
            "Epoch: 4 | Batch_idx: 90 |  Loss: (1.8104) | Acc: (30.68%) (3574/11648)\n",
            "Epoch: 4 | Batch_idx: 100 |  Loss: (1.8140) | Acc: (30.46%) (3938/12928)\n",
            "Epoch: 4 | Batch_idx: 110 |  Loss: (1.8123) | Acc: (30.77%) (4372/14208)\n",
            "Epoch: 4 | Batch_idx: 120 |  Loss: (1.8092) | Acc: (30.91%) (4788/15488)\n",
            "Epoch: 4 | Batch_idx: 130 |  Loss: (1.8119) | Acc: (30.75%) (5156/16768)\n",
            "Epoch: 4 | Batch_idx: 140 |  Loss: (1.8126) | Acc: (30.67%) (5535/18048)\n",
            "Epoch: 4 | Batch_idx: 150 |  Loss: (1.8103) | Acc: (30.74%) (5941/19328)\n",
            "Epoch: 4 | Batch_idx: 160 |  Loss: (1.8054) | Acc: (30.91%) (6369/20608)\n",
            "Epoch: 4 | Batch_idx: 170 |  Loss: (1.8047) | Acc: (30.95%) (6774/21888)\n",
            "Epoch: 4 | Batch_idx: 180 |  Loss: (1.8042) | Acc: (31.06%) (7197/23168)\n",
            "Epoch: 4 | Batch_idx: 190 |  Loss: (1.8030) | Acc: (31.11%) (7606/24448)\n",
            "Epoch: 4 | Batch_idx: 200 |  Loss: (1.8034) | Acc: (31.06%) (7991/25728)\n",
            "Epoch: 4 | Batch_idx: 210 |  Loss: (1.8019) | Acc: (31.08%) (8393/27008)\n",
            "Epoch: 4 | Batch_idx: 220 |  Loss: (1.7991) | Acc: (31.15%) (8813/28288)\n",
            "Epoch: 4 | Batch_idx: 230 |  Loss: (1.7987) | Acc: (31.20%) (9226/29568)\n",
            "Epoch: 4 | Batch_idx: 240 |  Loss: (1.7991) | Acc: (31.19%) (9621/30848)\n",
            "Epoch: 4 | Batch_idx: 250 |  Loss: (1.7956) | Acc: (31.27%) (10047/32128)\n",
            "Epoch: 4 | Batch_idx: 260 |  Loss: (1.7931) | Acc: (31.37%) (10480/33408)\n",
            "Epoch: 4 | Batch_idx: 270 |  Loss: (1.7914) | Acc: (31.48%) (10920/34688)\n",
            "Epoch: 4 | Batch_idx: 280 |  Loss: (1.7901) | Acc: (31.53%) (11342/35968)\n",
            "Epoch: 4 | Batch_idx: 290 |  Loss: (1.7887) | Acc: (31.59%) (11766/37248)\n",
            "Epoch: 4 | Batch_idx: 300 |  Loss: (1.7869) | Acc: (31.69%) (12211/38528)\n",
            "Epoch: 4 | Batch_idx: 310 |  Loss: (1.7854) | Acc: (31.84%) (12674/39808)\n",
            "Epoch: 4 | Batch_idx: 320 |  Loss: (1.7812) | Acc: (32.00%) (13150/41088)\n",
            "Epoch: 4 | Batch_idx: 330 |  Loss: (1.7807) | Acc: (32.05%) (13580/42368)\n",
            "Epoch: 4 | Batch_idx: 340 |  Loss: (1.7794) | Acc: (32.13%) (14022/43648)\n",
            "Epoch: 4 | Batch_idx: 350 |  Loss: (1.7786) | Acc: (32.20%) (14467/44928)\n",
            "Epoch: 4 | Batch_idx: 360 |  Loss: (1.7784) | Acc: (32.22%) (14886/46208)\n",
            "Epoch: 4 | Batch_idx: 370 |  Loss: (1.7778) | Acc: (32.22%) (15301/47488)\n",
            "Epoch: 4 | Batch_idx: 380 |  Loss: (1.7757) | Acc: (32.30%) (15753/48768)\n",
            "Epoch: 4 | Batch_idx: 390 |  Loss: (1.7742) | Acc: (32.40%) (16198/50000)\n",
            "# TEST : Loss: (1.7143) | Acc: (36.29%) (3629/10000)\n",
            "Epoch: 5 | Batch_idx: 0 |  Loss: (1.8961) | Acc: (26.56%) (34/128)\n",
            "Epoch: 5 | Batch_idx: 10 |  Loss: (1.7780) | Acc: (31.46%) (443/1408)\n",
            "Epoch: 5 | Batch_idx: 20 |  Loss: (1.7353) | Acc: (33.63%) (904/2688)\n",
            "Epoch: 5 | Batch_idx: 30 |  Loss: (1.7175) | Acc: (34.32%) (1362/3968)\n",
            "Epoch: 5 | Batch_idx: 40 |  Loss: (1.7203) | Acc: (34.72%) (1822/5248)\n",
            "Epoch: 5 | Batch_idx: 50 |  Loss: (1.7170) | Acc: (34.44%) (2248/6528)\n",
            "Epoch: 5 | Batch_idx: 60 |  Loss: (1.7157) | Acc: (34.59%) (2701/7808)\n",
            "Epoch: 5 | Batch_idx: 70 |  Loss: (1.7133) | Acc: (34.79%) (3162/9088)\n",
            "Epoch: 5 | Batch_idx: 80 |  Loss: (1.7166) | Acc: (34.93%) (3622/10368)\n",
            "Epoch: 5 | Batch_idx: 90 |  Loss: (1.7162) | Acc: (34.93%) (4069/11648)\n",
            "Epoch: 5 | Batch_idx: 100 |  Loss: (1.7139) | Acc: (34.99%) (4524/12928)\n",
            "Epoch: 5 | Batch_idx: 110 |  Loss: (1.7134) | Acc: (35.03%) (4977/14208)\n",
            "Epoch: 5 | Batch_idx: 120 |  Loss: (1.7097) | Acc: (35.24%) (5458/15488)\n",
            "Epoch: 5 | Batch_idx: 130 |  Loss: (1.7091) | Acc: (35.36%) (5929/16768)\n",
            "Epoch: 5 | Batch_idx: 140 |  Loss: (1.7073) | Acc: (35.51%) (6408/18048)\n",
            "Epoch: 5 | Batch_idx: 150 |  Loss: (1.7036) | Acc: (35.77%) (6914/19328)\n",
            "Epoch: 5 | Batch_idx: 160 |  Loss: (1.7043) | Acc: (35.72%) (7362/20608)\n",
            "Epoch: 5 | Batch_idx: 170 |  Loss: (1.7002) | Acc: (35.78%) (7831/21888)\n",
            "Epoch: 5 | Batch_idx: 180 |  Loss: (1.6995) | Acc: (35.79%) (8291/23168)\n",
            "Epoch: 5 | Batch_idx: 190 |  Loss: (1.6983) | Acc: (35.90%) (8776/24448)\n",
            "Epoch: 5 | Batch_idx: 200 |  Loss: (1.6953) | Acc: (36.01%) (9264/25728)\n",
            "Epoch: 5 | Batch_idx: 210 |  Loss: (1.6947) | Acc: (36.11%) (9753/27008)\n",
            "Epoch: 5 | Batch_idx: 220 |  Loss: (1.6925) | Acc: (36.12%) (10218/28288)\n",
            "Epoch: 5 | Batch_idx: 230 |  Loss: (1.6904) | Acc: (36.15%) (10688/29568)\n",
            "Epoch: 5 | Batch_idx: 240 |  Loss: (1.6870) | Acc: (36.21%) (11171/30848)\n",
            "Epoch: 5 | Batch_idx: 250 |  Loss: (1.6859) | Acc: (36.21%) (11632/32128)\n",
            "Epoch: 5 | Batch_idx: 260 |  Loss: (1.6843) | Acc: (36.26%) (12113/33408)\n",
            "Epoch: 5 | Batch_idx: 270 |  Loss: (1.6813) | Acc: (36.41%) (12629/34688)\n",
            "Epoch: 5 | Batch_idx: 280 |  Loss: (1.6790) | Acc: (36.50%) (13129/35968)\n",
            "Epoch: 5 | Batch_idx: 290 |  Loss: (1.6764) | Acc: (36.63%) (13643/37248)\n",
            "Epoch: 5 | Batch_idx: 300 |  Loss: (1.6742) | Acc: (36.71%) (14142/38528)\n",
            "Epoch: 5 | Batch_idx: 310 |  Loss: (1.6726) | Acc: (36.84%) (14667/39808)\n",
            "Epoch: 5 | Batch_idx: 320 |  Loss: (1.6708) | Acc: (36.93%) (15173/41088)\n",
            "Epoch: 5 | Batch_idx: 330 |  Loss: (1.6685) | Acc: (37.01%) (15681/42368)\n",
            "Epoch: 5 | Batch_idx: 340 |  Loss: (1.6673) | Acc: (37.07%) (16180/43648)\n",
            "Epoch: 5 | Batch_idx: 350 |  Loss: (1.6654) | Acc: (37.11%) (16675/44928)\n",
            "Epoch: 5 | Batch_idx: 360 |  Loss: (1.6631) | Acc: (37.17%) (17174/46208)\n",
            "Epoch: 5 | Batch_idx: 370 |  Loss: (1.6603) | Acc: (37.27%) (17699/47488)\n",
            "Epoch: 5 | Batch_idx: 380 |  Loss: (1.6576) | Acc: (37.36%) (18220/48768)\n",
            "Epoch: 5 | Batch_idx: 390 |  Loss: (1.6557) | Acc: (37.40%) (18701/50000)\n",
            "# TEST : Loss: (1.5448) | Acc: (43.04%) (4304/10000)\n",
            "Epoch: 6 | Batch_idx: 0 |  Loss: (1.6584) | Acc: (39.06%) (50/128)\n",
            "Epoch: 6 | Batch_idx: 10 |  Loss: (1.6196) | Acc: (39.99%) (563/1408)\n",
            "Epoch: 6 | Batch_idx: 20 |  Loss: (1.5864) | Acc: (40.77%) (1096/2688)\n",
            "Epoch: 6 | Batch_idx: 30 |  Loss: (1.5857) | Acc: (41.15%) (1633/3968)\n",
            "Epoch: 6 | Batch_idx: 40 |  Loss: (1.6017) | Acc: (40.72%) (2137/5248)\n",
            "Epoch: 6 | Batch_idx: 50 |  Loss: (1.5923) | Acc: (40.95%) (2673/6528)\n",
            "Epoch: 6 | Batch_idx: 60 |  Loss: (1.5837) | Acc: (41.28%) (3223/7808)\n",
            "Epoch: 6 | Batch_idx: 70 |  Loss: (1.5792) | Acc: (41.43%) (3765/9088)\n",
            "Epoch: 6 | Batch_idx: 80 |  Loss: (1.5778) | Acc: (41.15%) (4266/10368)\n",
            "Epoch: 6 | Batch_idx: 90 |  Loss: (1.5758) | Acc: (41.15%) (4793/11648)\n",
            "Epoch: 6 | Batch_idx: 100 |  Loss: (1.5701) | Acc: (41.43%) (5356/12928)\n",
            "Epoch: 6 | Batch_idx: 110 |  Loss: (1.5664) | Acc: (41.43%) (5887/14208)\n",
            "Epoch: 6 | Batch_idx: 120 |  Loss: (1.5631) | Acc: (41.61%) (6445/15488)\n",
            "Epoch: 6 | Batch_idx: 130 |  Loss: (1.5598) | Acc: (41.72%) (6995/16768)\n",
            "Epoch: 6 | Batch_idx: 140 |  Loss: (1.5577) | Acc: (41.80%) (7544/18048)\n",
            "Epoch: 6 | Batch_idx: 150 |  Loss: (1.5599) | Acc: (41.69%) (8058/19328)\n",
            "Epoch: 6 | Batch_idx: 160 |  Loss: (1.5577) | Acc: (41.86%) (8627/20608)\n",
            "Epoch: 6 | Batch_idx: 170 |  Loss: (1.5577) | Acc: (41.94%) (9180/21888)\n",
            "Epoch: 6 | Batch_idx: 180 |  Loss: (1.5556) | Acc: (42.04%) (9739/23168)\n",
            "Epoch: 6 | Batch_idx: 190 |  Loss: (1.5516) | Acc: (42.18%) (10312/24448)\n",
            "Epoch: 6 | Batch_idx: 200 |  Loss: (1.5507) | Acc: (42.21%) (10861/25728)\n",
            "Epoch: 6 | Batch_idx: 210 |  Loss: (1.5487) | Acc: (42.29%) (11422/27008)\n",
            "Epoch: 6 | Batch_idx: 220 |  Loss: (1.5468) | Acc: (42.42%) (12000/28288)\n",
            "Epoch: 6 | Batch_idx: 230 |  Loss: (1.5450) | Acc: (42.50%) (12565/29568)\n",
            "Epoch: 6 | Batch_idx: 240 |  Loss: (1.5422) | Acc: (42.66%) (13159/30848)\n",
            "Epoch: 6 | Batch_idx: 250 |  Loss: (1.5396) | Acc: (42.83%) (13760/32128)\n",
            "Epoch: 6 | Batch_idx: 260 |  Loss: (1.5392) | Acc: (42.81%) (14301/33408)\n",
            "Epoch: 6 | Batch_idx: 270 |  Loss: (1.5358) | Acc: (42.96%) (14902/34688)\n",
            "Epoch: 6 | Batch_idx: 280 |  Loss: (1.5318) | Acc: (43.14%) (15517/35968)\n",
            "Epoch: 6 | Batch_idx: 290 |  Loss: (1.5298) | Acc: (43.34%) (16144/37248)\n",
            "Epoch: 6 | Batch_idx: 300 |  Loss: (1.5259) | Acc: (43.50%) (16761/38528)\n",
            "Epoch: 6 | Batch_idx: 310 |  Loss: (1.5244) | Acc: (43.64%) (17373/39808)\n",
            "Epoch: 6 | Batch_idx: 320 |  Loss: (1.5233) | Acc: (43.67%) (17945/41088)\n",
            "Epoch: 6 | Batch_idx: 330 |  Loss: (1.5214) | Acc: (43.66%) (18497/42368)\n",
            "Epoch: 6 | Batch_idx: 340 |  Loss: (1.5196) | Acc: (43.69%) (19069/43648)\n",
            "Epoch: 6 | Batch_idx: 350 |  Loss: (1.5191) | Acc: (43.74%) (19651/44928)\n",
            "Epoch: 6 | Batch_idx: 360 |  Loss: (1.5169) | Acc: (43.83%) (20254/46208)\n",
            "Epoch: 6 | Batch_idx: 370 |  Loss: (1.5142) | Acc: (43.93%) (20863/47488)\n",
            "Epoch: 6 | Batch_idx: 380 |  Loss: (1.5112) | Acc: (44.09%) (21500/48768)\n",
            "Epoch: 6 | Batch_idx: 390 |  Loss: (1.5104) | Acc: (44.08%) (22038/50000)\n",
            "# TEST : Loss: (1.3637) | Acc: (50.04%) (5004/10000)\n",
            "Epoch: 7 | Batch_idx: 0 |  Loss: (1.4824) | Acc: (51.56%) (66/128)\n",
            "Epoch: 7 | Batch_idx: 10 |  Loss: (1.3795) | Acc: (50.50%) (711/1408)\n",
            "Epoch: 7 | Batch_idx: 20 |  Loss: (1.3933) | Acc: (49.00%) (1317/2688)\n",
            "Epoch: 7 | Batch_idx: 30 |  Loss: (1.4064) | Acc: (48.21%) (1913/3968)\n",
            "Epoch: 7 | Batch_idx: 40 |  Loss: (1.4111) | Acc: (47.69%) (2503/5248)\n",
            "Epoch: 7 | Batch_idx: 50 |  Loss: (1.4046) | Acc: (47.95%) (3130/6528)\n",
            "Epoch: 7 | Batch_idx: 60 |  Loss: (1.4061) | Acc: (48.28%) (3770/7808)\n",
            "Epoch: 7 | Batch_idx: 70 |  Loss: (1.4060) | Acc: (48.27%) (4387/9088)\n",
            "Epoch: 7 | Batch_idx: 80 |  Loss: (1.4075) | Acc: (48.23%) (5001/10368)\n",
            "Epoch: 7 | Batch_idx: 90 |  Loss: (1.4012) | Acc: (48.47%) (5646/11648)\n",
            "Epoch: 7 | Batch_idx: 100 |  Loss: (1.4035) | Acc: (48.48%) (6267/12928)\n",
            "Epoch: 7 | Batch_idx: 110 |  Loss: (1.4035) | Acc: (48.44%) (6883/14208)\n",
            "Epoch: 7 | Batch_idx: 120 |  Loss: (1.4026) | Acc: (48.50%) (7511/15488)\n",
            "Epoch: 7 | Batch_idx: 130 |  Loss: (1.3979) | Acc: (48.53%) (8137/16768)\n",
            "Epoch: 7 | Batch_idx: 140 |  Loss: (1.3958) | Acc: (48.70%) (8789/18048)\n",
            "Epoch: 7 | Batch_idx: 150 |  Loss: (1.3919) | Acc: (48.85%) (9441/19328)\n",
            "Epoch: 7 | Batch_idx: 160 |  Loss: (1.3909) | Acc: (48.89%) (10076/20608)\n",
            "Epoch: 7 | Batch_idx: 170 |  Loss: (1.3873) | Acc: (49.04%) (10733/21888)\n",
            "Epoch: 7 | Batch_idx: 180 |  Loss: (1.3864) | Acc: (49.07%) (11369/23168)\n",
            "Epoch: 7 | Batch_idx: 190 |  Loss: (1.3840) | Acc: (49.23%) (12035/24448)\n",
            "Epoch: 7 | Batch_idx: 200 |  Loss: (1.3823) | Acc: (49.30%) (12685/25728)\n",
            "Epoch: 7 | Batch_idx: 210 |  Loss: (1.3819) | Acc: (49.41%) (13345/27008)\n",
            "Epoch: 7 | Batch_idx: 220 |  Loss: (1.3787) | Acc: (49.49%) (14001/28288)\n",
            "Epoch: 7 | Batch_idx: 230 |  Loss: (1.3755) | Acc: (49.61%) (14670/29568)\n",
            "Epoch: 7 | Batch_idx: 240 |  Loss: (1.3758) | Acc: (49.57%) (15292/30848)\n",
            "Epoch: 7 | Batch_idx: 250 |  Loss: (1.3742) | Acc: (49.59%) (15932/32128)\n",
            "Epoch: 7 | Batch_idx: 260 |  Loss: (1.3730) | Acc: (49.61%) (16575/33408)\n",
            "Epoch: 7 | Batch_idx: 270 |  Loss: (1.3700) | Acc: (49.77%) (17263/34688)\n",
            "Epoch: 7 | Batch_idx: 280 |  Loss: (1.3704) | Acc: (49.74%) (17891/35968)\n",
            "Epoch: 7 | Batch_idx: 290 |  Loss: (1.3721) | Acc: (49.70%) (18511/37248)\n",
            "Epoch: 7 | Batch_idx: 300 |  Loss: (1.3690) | Acc: (49.84%) (19202/38528)\n",
            "Epoch: 7 | Batch_idx: 310 |  Loss: (1.3689) | Acc: (49.82%) (19833/39808)\n",
            "Epoch: 7 | Batch_idx: 320 |  Loss: (1.3653) | Acc: (49.91%) (20508/41088)\n",
            "Epoch: 7 | Batch_idx: 330 |  Loss: (1.3623) | Acc: (50.03%) (21198/42368)\n",
            "Epoch: 7 | Batch_idx: 340 |  Loss: (1.3590) | Acc: (50.17%) (21897/43648)\n",
            "Epoch: 7 | Batch_idx: 350 |  Loss: (1.3561) | Acc: (50.28%) (22590/44928)\n",
            "Epoch: 7 | Batch_idx: 360 |  Loss: (1.3562) | Acc: (50.28%) (23235/46208)\n",
            "Epoch: 7 | Batch_idx: 370 |  Loss: (1.3545) | Acc: (50.36%) (23914/47488)\n",
            "Epoch: 7 | Batch_idx: 380 |  Loss: (1.3518) | Acc: (50.45%) (24605/48768)\n",
            "Epoch: 7 | Batch_idx: 390 |  Loss: (1.3498) | Acc: (50.54%) (25268/50000)\n",
            "# TEST : Loss: (1.2717) | Acc: (53.84%) (5384/10000)\n",
            "Epoch: 8 | Batch_idx: 0 |  Loss: (1.4032) | Acc: (46.88%) (60/128)\n",
            "Epoch: 8 | Batch_idx: 10 |  Loss: (1.2985) | Acc: (52.98%) (746/1408)\n",
            "Epoch: 8 | Batch_idx: 20 |  Loss: (1.2806) | Acc: (53.39%) (1435/2688)\n",
            "Epoch: 8 | Batch_idx: 30 |  Loss: (1.2706) | Acc: (53.86%) (2137/3968)\n",
            "Epoch: 8 | Batch_idx: 40 |  Loss: (1.2640) | Acc: (54.48%) (2859/5248)\n",
            "Epoch: 8 | Batch_idx: 50 |  Loss: (1.2609) | Acc: (54.47%) (3556/6528)\n",
            "Epoch: 8 | Batch_idx: 60 |  Loss: (1.2555) | Acc: (54.61%) (4264/7808)\n",
            "Epoch: 8 | Batch_idx: 70 |  Loss: (1.2501) | Acc: (54.92%) (4991/9088)\n",
            "Epoch: 8 | Batch_idx: 80 |  Loss: (1.2488) | Acc: (54.88%) (5690/10368)\n",
            "Epoch: 8 | Batch_idx: 90 |  Loss: (1.2501) | Acc: (54.64%) (6364/11648)\n",
            "Epoch: 8 | Batch_idx: 100 |  Loss: (1.2504) | Acc: (54.55%) (7052/12928)\n",
            "Epoch: 8 | Batch_idx: 110 |  Loss: (1.2468) | Acc: (54.69%) (7770/14208)\n",
            "Epoch: 8 | Batch_idx: 120 |  Loss: (1.2450) | Acc: (54.67%) (8467/15488)\n",
            "Epoch: 8 | Batch_idx: 130 |  Loss: (1.2381) | Acc: (55.00%) (9222/16768)\n",
            "Epoch: 8 | Batch_idx: 140 |  Loss: (1.2357) | Acc: (55.09%) (9943/18048)\n",
            "Epoch: 8 | Batch_idx: 150 |  Loss: (1.2375) | Acc: (55.02%) (10635/19328)\n",
            "Epoch: 8 | Batch_idx: 160 |  Loss: (1.2314) | Acc: (55.27%) (11390/20608)\n",
            "Epoch: 8 | Batch_idx: 170 |  Loss: (1.2282) | Acc: (55.33%) (12110/21888)\n",
            "Epoch: 8 | Batch_idx: 180 |  Loss: (1.2283) | Acc: (55.39%) (12833/23168)\n",
            "Epoch: 8 | Batch_idx: 190 |  Loss: (1.2289) | Acc: (55.45%) (13556/24448)\n",
            "Epoch: 8 | Batch_idx: 200 |  Loss: (1.2305) | Acc: (55.34%) (14239/25728)\n",
            "Epoch: 8 | Batch_idx: 210 |  Loss: (1.2303) | Acc: (55.38%) (14958/27008)\n",
            "Epoch: 8 | Batch_idx: 220 |  Loss: (1.2284) | Acc: (55.40%) (15671/28288)\n",
            "Epoch: 8 | Batch_idx: 230 |  Loss: (1.2255) | Acc: (55.43%) (16391/29568)\n",
            "Epoch: 8 | Batch_idx: 240 |  Loss: (1.2241) | Acc: (55.50%) (17121/30848)\n",
            "Epoch: 8 | Batch_idx: 250 |  Loss: (1.2210) | Acc: (55.64%) (17876/32128)\n",
            "Epoch: 8 | Batch_idx: 260 |  Loss: (1.2182) | Acc: (55.70%) (18608/33408)\n",
            "Epoch: 8 | Batch_idx: 270 |  Loss: (1.2159) | Acc: (55.78%) (19349/34688)\n",
            "Epoch: 8 | Batch_idx: 280 |  Loss: (1.2141) | Acc: (55.87%) (20096/35968)\n",
            "Epoch: 8 | Batch_idx: 290 |  Loss: (1.2121) | Acc: (55.93%) (20832/37248)\n",
            "Epoch: 8 | Batch_idx: 300 |  Loss: (1.2104) | Acc: (56.00%) (21576/38528)\n",
            "Epoch: 8 | Batch_idx: 310 |  Loss: (1.2058) | Acc: (56.18%) (22366/39808)\n",
            "Epoch: 8 | Batch_idx: 320 |  Loss: (1.2058) | Acc: (56.15%) (23070/41088)\n",
            "Epoch: 8 | Batch_idx: 330 |  Loss: (1.2047) | Acc: (56.19%) (23808/42368)\n",
            "Epoch: 8 | Batch_idx: 340 |  Loss: (1.2016) | Acc: (56.35%) (24594/43648)\n",
            "Epoch: 8 | Batch_idx: 350 |  Loss: (1.1995) | Acc: (56.39%) (25333/44928)\n",
            "Epoch: 8 | Batch_idx: 360 |  Loss: (1.1960) | Acc: (56.52%) (26115/46208)\n",
            "Epoch: 8 | Batch_idx: 370 |  Loss: (1.1960) | Acc: (56.53%) (26843/47488)\n",
            "Epoch: 8 | Batch_idx: 380 |  Loss: (1.1947) | Acc: (56.56%) (27581/48768)\n",
            "Epoch: 8 | Batch_idx: 390 |  Loss: (1.1934) | Acc: (56.64%) (28322/50000)\n",
            "# TEST : Loss: (1.1427) | Acc: (59.36%) (5936/10000)\n",
            "Epoch: 9 | Batch_idx: 0 |  Loss: (1.1343) | Acc: (59.38%) (76/128)\n",
            "Epoch: 9 | Batch_idx: 10 |  Loss: (1.0496) | Acc: (62.36%) (878/1408)\n",
            "Epoch: 9 | Batch_idx: 20 |  Loss: (1.0575) | Acc: (61.20%) (1645/2688)\n",
            "Epoch: 9 | Batch_idx: 30 |  Loss: (1.0756) | Acc: (60.48%) (2400/3968)\n",
            "Epoch: 9 | Batch_idx: 40 |  Loss: (1.0570) | Acc: (61.01%) (3202/5248)\n",
            "Epoch: 9 | Batch_idx: 50 |  Loss: (1.0594) | Acc: (60.98%) (3981/6528)\n",
            "Epoch: 9 | Batch_idx: 60 |  Loss: (1.0600) | Acc: (60.95%) (4759/7808)\n",
            "Epoch: 9 | Batch_idx: 70 |  Loss: (1.0729) | Acc: (60.64%) (5511/9088)\n",
            "Epoch: 9 | Batch_idx: 80 |  Loss: (1.0791) | Acc: (60.46%) (6268/10368)\n",
            "Epoch: 9 | Batch_idx: 90 |  Loss: (1.0803) | Acc: (60.30%) (7024/11648)\n",
            "Epoch: 9 | Batch_idx: 100 |  Loss: (1.0857) | Acc: (60.07%) (7766/12928)\n",
            "Epoch: 9 | Batch_idx: 110 |  Loss: (1.0852) | Acc: (60.09%) (8537/14208)\n",
            "Epoch: 9 | Batch_idx: 120 |  Loss: (1.0841) | Acc: (60.13%) (9313/15488)\n",
            "Epoch: 9 | Batch_idx: 130 |  Loss: (1.0806) | Acc: (60.40%) (10128/16768)\n",
            "Epoch: 9 | Batch_idx: 140 |  Loss: (1.0771) | Acc: (60.51%) (10920/18048)\n",
            "Epoch: 9 | Batch_idx: 150 |  Loss: (1.0764) | Acc: (60.53%) (11700/19328)\n",
            "Epoch: 9 | Batch_idx: 160 |  Loss: (1.0749) | Acc: (60.72%) (12514/20608)\n",
            "Epoch: 9 | Batch_idx: 170 |  Loss: (1.0742) | Acc: (60.82%) (13312/21888)\n",
            "Epoch: 9 | Batch_idx: 180 |  Loss: (1.0760) | Acc: (60.79%) (14084/23168)\n",
            "Epoch: 9 | Batch_idx: 190 |  Loss: (1.0755) | Acc: (60.75%) (14852/24448)\n",
            "Epoch: 9 | Batch_idx: 200 |  Loss: (1.0728) | Acc: (60.82%) (15649/25728)\n",
            "Epoch: 9 | Batch_idx: 210 |  Loss: (1.0707) | Acc: (60.91%) (16451/27008)\n",
            "Epoch: 9 | Batch_idx: 220 |  Loss: (1.0675) | Acc: (61.06%) (17272/28288)\n",
            "Epoch: 9 | Batch_idx: 230 |  Loss: (1.0673) | Acc: (61.13%) (18074/29568)\n",
            "Epoch: 9 | Batch_idx: 240 |  Loss: (1.0675) | Acc: (61.12%) (18854/30848)\n",
            "Epoch: 9 | Batch_idx: 250 |  Loss: (1.0656) | Acc: (61.29%) (19692/32128)\n",
            "Epoch: 9 | Batch_idx: 260 |  Loss: (1.0660) | Acc: (61.32%) (20486/33408)\n",
            "Epoch: 9 | Batch_idx: 270 |  Loss: (1.0649) | Acc: (61.38%) (21291/34688)\n",
            "Epoch: 9 | Batch_idx: 280 |  Loss: (1.0669) | Acc: (61.31%) (22053/35968)\n",
            "Epoch: 9 | Batch_idx: 290 |  Loss: (1.0657) | Acc: (61.30%) (22832/37248)\n",
            "Epoch: 9 | Batch_idx: 300 |  Loss: (1.0643) | Acc: (61.37%) (23643/38528)\n",
            "Epoch: 9 | Batch_idx: 310 |  Loss: (1.0636) | Acc: (61.40%) (24443/39808)\n",
            "Epoch: 9 | Batch_idx: 320 |  Loss: (1.0628) | Acc: (61.46%) (25253/41088)\n",
            "Epoch: 9 | Batch_idx: 330 |  Loss: (1.0613) | Acc: (61.52%) (26063/42368)\n",
            "Epoch: 9 | Batch_idx: 340 |  Loss: (1.0614) | Acc: (61.53%) (26855/43648)\n",
            "Epoch: 9 | Batch_idx: 350 |  Loss: (1.0609) | Acc: (61.51%) (27637/44928)\n",
            "Epoch: 9 | Batch_idx: 360 |  Loss: (1.0588) | Acc: (61.64%) (28484/46208)\n",
            "Epoch: 9 | Batch_idx: 370 |  Loss: (1.0568) | Acc: (61.70%) (29299/47488)\n",
            "Epoch: 9 | Batch_idx: 380 |  Loss: (1.0557) | Acc: (61.76%) (30119/48768)\n",
            "Epoch: 9 | Batch_idx: 390 |  Loss: (1.0545) | Acc: (61.80%) (30901/50000)\n",
            "# TEST : Loss: (1.0473) | Acc: (62.60%) (6260/10000)\n",
            "Epoch: 10 | Batch_idx: 0 |  Loss: (0.9792) | Acc: (64.06%) (82/128)\n",
            "Epoch: 10 | Batch_idx: 10 |  Loss: (0.9624) | Acc: (64.77%) (912/1408)\n",
            "Epoch: 10 | Batch_idx: 20 |  Loss: (0.9748) | Acc: (65.55%) (1762/2688)\n",
            "Epoch: 10 | Batch_idx: 30 |  Loss: (0.9645) | Acc: (65.65%) (2605/3968)\n",
            "Epoch: 10 | Batch_idx: 40 |  Loss: (0.9609) | Acc: (65.78%) (3452/5248)\n",
            "Epoch: 10 | Batch_idx: 50 |  Loss: (0.9726) | Acc: (65.29%) (4262/6528)\n",
            "Epoch: 10 | Batch_idx: 60 |  Loss: (0.9772) | Acc: (64.96%) (5072/7808)\n",
            "Epoch: 10 | Batch_idx: 70 |  Loss: (0.9735) | Acc: (65.02%) (5909/9088)\n",
            "Epoch: 10 | Batch_idx: 80 |  Loss: (0.9734) | Acc: (65.01%) (6740/10368)\n",
            "Epoch: 10 | Batch_idx: 90 |  Loss: (0.9726) | Acc: (64.99%) (7570/11648)\n",
            "Epoch: 10 | Batch_idx: 100 |  Loss: (0.9661) | Acc: (65.10%) (8416/12928)\n",
            "Epoch: 10 | Batch_idx: 110 |  Loss: (0.9655) | Acc: (65.10%) (9250/14208)\n",
            "Epoch: 10 | Batch_idx: 120 |  Loss: (0.9672) | Acc: (65.14%) (10089/15488)\n",
            "Epoch: 10 | Batch_idx: 130 |  Loss: (0.9698) | Acc: (65.14%) (10923/16768)\n",
            "Epoch: 10 | Batch_idx: 140 |  Loss: (0.9660) | Acc: (65.17%) (11761/18048)\n",
            "Epoch: 10 | Batch_idx: 150 |  Loss: (0.9666) | Acc: (65.17%) (12596/19328)\n",
            "Epoch: 10 | Batch_idx: 160 |  Loss: (0.9693) | Acc: (65.06%) (13408/20608)\n",
            "Epoch: 10 | Batch_idx: 170 |  Loss: (0.9665) | Acc: (64.99%) (14226/21888)\n",
            "Epoch: 10 | Batch_idx: 180 |  Loss: (0.9667) | Acc: (64.98%) (15055/23168)\n",
            "Epoch: 10 | Batch_idx: 190 |  Loss: (0.9668) | Acc: (65.02%) (15897/24448)\n",
            "Epoch: 10 | Batch_idx: 200 |  Loss: (0.9656) | Acc: (65.05%) (16736/25728)\n",
            "Epoch: 10 | Batch_idx: 210 |  Loss: (0.9678) | Acc: (64.94%) (17538/27008)\n",
            "Epoch: 10 | Batch_idx: 220 |  Loss: (0.9663) | Acc: (64.97%) (18380/28288)\n",
            "Epoch: 10 | Batch_idx: 230 |  Loss: (0.9658) | Acc: (64.95%) (19203/29568)\n",
            "Epoch: 10 | Batch_idx: 240 |  Loss: (0.9646) | Acc: (65.00%) (20051/30848)\n",
            "Epoch: 10 | Batch_idx: 250 |  Loss: (0.9632) | Acc: (65.08%) (20908/32128)\n",
            "Epoch: 10 | Batch_idx: 260 |  Loss: (0.9609) | Acc: (65.18%) (21774/33408)\n",
            "Epoch: 10 | Batch_idx: 270 |  Loss: (0.9586) | Acc: (65.26%) (22638/34688)\n",
            "Epoch: 10 | Batch_idx: 280 |  Loss: (0.9555) | Acc: (65.42%) (23531/35968)\n",
            "Epoch: 10 | Batch_idx: 290 |  Loss: (0.9547) | Acc: (65.49%) (24392/37248)\n",
            "Epoch: 10 | Batch_idx: 300 |  Loss: (0.9540) | Acc: (65.53%) (25247/38528)\n",
            "Epoch: 10 | Batch_idx: 310 |  Loss: (0.9526) | Acc: (65.61%) (26119/39808)\n",
            "Epoch: 10 | Batch_idx: 320 |  Loss: (0.9513) | Acc: (65.68%) (26985/41088)\n",
            "Epoch: 10 | Batch_idx: 330 |  Loss: (0.9502) | Acc: (65.74%) (27851/42368)\n",
            "Epoch: 10 | Batch_idx: 340 |  Loss: (0.9496) | Acc: (65.76%) (28703/43648)\n",
            "Epoch: 10 | Batch_idx: 350 |  Loss: (0.9487) | Acc: (65.79%) (29558/44928)\n",
            "Epoch: 10 | Batch_idx: 360 |  Loss: (0.9476) | Acc: (65.80%) (30407/46208)\n",
            "Epoch: 10 | Batch_idx: 370 |  Loss: (0.9455) | Acc: (65.83%) (31262/47488)\n",
            "Epoch: 10 | Batch_idx: 380 |  Loss: (0.9428) | Acc: (65.91%) (32144/48768)\n",
            "Epoch: 10 | Batch_idx: 390 |  Loss: (0.9407) | Acc: (66.01%) (33006/50000)\n",
            "# TEST : Loss: (0.9277) | Acc: (67.27%) (6727/10000)\n",
            "Epoch: 11 | Batch_idx: 0 |  Loss: (0.7745) | Acc: (73.44%) (94/128)\n",
            "Epoch: 11 | Batch_idx: 10 |  Loss: (0.8467) | Acc: (69.74%) (982/1408)\n",
            "Epoch: 11 | Batch_idx: 20 |  Loss: (0.8678) | Acc: (69.38%) (1865/2688)\n",
            "Epoch: 11 | Batch_idx: 30 |  Loss: (0.8431) | Acc: (70.19%) (2785/3968)\n",
            "Epoch: 11 | Batch_idx: 40 |  Loss: (0.8367) | Acc: (70.16%) (3682/5248)\n",
            "Epoch: 11 | Batch_idx: 50 |  Loss: (0.8495) | Acc: (69.56%) (4541/6528)\n",
            "Epoch: 11 | Batch_idx: 60 |  Loss: (0.8568) | Acc: (69.10%) (5395/7808)\n",
            "Epoch: 11 | Batch_idx: 70 |  Loss: (0.8544) | Acc: (69.18%) (6287/9088)\n",
            "Epoch: 11 | Batch_idx: 80 |  Loss: (0.8571) | Acc: (69.02%) (7156/10368)\n",
            "Epoch: 11 | Batch_idx: 90 |  Loss: (0.8570) | Acc: (69.21%) (8062/11648)\n",
            "Epoch: 11 | Batch_idx: 100 |  Loss: (0.8554) | Acc: (69.15%) (8940/12928)\n",
            "Epoch: 11 | Batch_idx: 110 |  Loss: (0.8526) | Acc: (69.37%) (9856/14208)\n",
            "Epoch: 11 | Batch_idx: 120 |  Loss: (0.8521) | Acc: (69.32%) (10737/15488)\n",
            "Epoch: 11 | Batch_idx: 130 |  Loss: (0.8513) | Acc: (69.42%) (11640/16768)\n",
            "Epoch: 11 | Batch_idx: 140 |  Loss: (0.8533) | Acc: (69.28%) (12503/18048)\n",
            "Epoch: 11 | Batch_idx: 150 |  Loss: (0.8498) | Acc: (69.51%) (13434/19328)\n",
            "Epoch: 11 | Batch_idx: 160 |  Loss: (0.8525) | Acc: (69.42%) (14307/20608)\n",
            "Epoch: 11 | Batch_idx: 170 |  Loss: (0.8544) | Acc: (69.33%) (15176/21888)\n",
            "Epoch: 11 | Batch_idx: 180 |  Loss: (0.8537) | Acc: (69.39%) (16077/23168)\n",
            "Epoch: 11 | Batch_idx: 190 |  Loss: (0.8524) | Acc: (69.48%) (16987/24448)\n",
            "Epoch: 11 | Batch_idx: 200 |  Loss: (0.8529) | Acc: (69.41%) (17859/25728)\n",
            "Epoch: 11 | Batch_idx: 210 |  Loss: (0.8532) | Acc: (69.39%) (18742/27008)\n",
            "Epoch: 11 | Batch_idx: 220 |  Loss: (0.8526) | Acc: (69.45%) (19646/28288)\n",
            "Epoch: 11 | Batch_idx: 230 |  Loss: (0.8539) | Acc: (69.44%) (20531/29568)\n",
            "Epoch: 11 | Batch_idx: 240 |  Loss: (0.8525) | Acc: (69.44%) (21420/30848)\n",
            "Epoch: 11 | Batch_idx: 250 |  Loss: (0.8521) | Acc: (69.45%) (22313/32128)\n",
            "Epoch: 11 | Batch_idx: 260 |  Loss: (0.8520) | Acc: (69.48%) (23212/33408)\n",
            "Epoch: 11 | Batch_idx: 270 |  Loss: (0.8507) | Acc: (69.54%) (24123/34688)\n",
            "Epoch: 11 | Batch_idx: 280 |  Loss: (0.8500) | Acc: (69.58%) (25026/35968)\n",
            "Epoch: 11 | Batch_idx: 290 |  Loss: (0.8469) | Acc: (69.71%) (25966/37248)\n",
            "Epoch: 11 | Batch_idx: 300 |  Loss: (0.8456) | Acc: (69.78%) (26886/38528)\n",
            "Epoch: 11 | Batch_idx: 310 |  Loss: (0.8441) | Acc: (69.80%) (27786/39808)\n",
            "Epoch: 11 | Batch_idx: 320 |  Loss: (0.8428) | Acc: (69.84%) (28696/41088)\n",
            "Epoch: 11 | Batch_idx: 330 |  Loss: (0.8423) | Acc: (69.86%) (29598/42368)\n",
            "Epoch: 11 | Batch_idx: 340 |  Loss: (0.8411) | Acc: (69.90%) (30508/43648)\n",
            "Epoch: 11 | Batch_idx: 350 |  Loss: (0.8391) | Acc: (70.00%) (31448/44928)\n",
            "Epoch: 11 | Batch_idx: 360 |  Loss: (0.8387) | Acc: (69.97%) (32334/46208)\n",
            "Epoch: 11 | Batch_idx: 370 |  Loss: (0.8398) | Acc: (69.91%) (33201/47488)\n",
            "Epoch: 11 | Batch_idx: 380 |  Loss: (0.8387) | Acc: (69.96%) (34119/48768)\n",
            "Epoch: 11 | Batch_idx: 390 |  Loss: (0.8376) | Acc: (69.99%) (34995/50000)\n",
            "# TEST : Loss: (0.8644) | Acc: (69.93%) (6993/10000)\n",
            "Epoch: 12 | Batch_idx: 0 |  Loss: (0.8069) | Acc: (73.44%) (94/128)\n",
            "Epoch: 12 | Batch_idx: 10 |  Loss: (0.7790) | Acc: (72.44%) (1020/1408)\n",
            "Epoch: 12 | Batch_idx: 20 |  Loss: (0.7986) | Acc: (71.76%) (1929/2688)\n",
            "Epoch: 12 | Batch_idx: 30 |  Loss: (0.7766) | Acc: (72.71%) (2885/3968)\n",
            "Epoch: 12 | Batch_idx: 40 |  Loss: (0.7793) | Acc: (72.58%) (3809/5248)\n",
            "Epoch: 12 | Batch_idx: 50 |  Loss: (0.7866) | Acc: (72.64%) (4742/6528)\n",
            "Epoch: 12 | Batch_idx: 60 |  Loss: (0.7950) | Acc: (72.00%) (5622/7808)\n",
            "Epoch: 12 | Batch_idx: 70 |  Loss: (0.7950) | Acc: (71.89%) (6533/9088)\n",
            "Epoch: 12 | Batch_idx: 80 |  Loss: (0.7868) | Acc: (72.14%) (7479/10368)\n",
            "Epoch: 12 | Batch_idx: 90 |  Loss: (0.7861) | Acc: (72.23%) (8413/11648)\n",
            "Epoch: 12 | Batch_idx: 100 |  Loss: (0.7810) | Acc: (72.39%) (9359/12928)\n",
            "Epoch: 12 | Batch_idx: 110 |  Loss: (0.7860) | Acc: (72.32%) (10275/14208)\n",
            "Epoch: 12 | Batch_idx: 120 |  Loss: (0.7899) | Acc: (72.26%) (11192/15488)\n",
            "Epoch: 12 | Batch_idx: 130 |  Loss: (0.7910) | Acc: (72.25%) (12115/16768)\n",
            "Epoch: 12 | Batch_idx: 140 |  Loss: (0.7895) | Acc: (72.31%) (13051/18048)\n",
            "Epoch: 12 | Batch_idx: 150 |  Loss: (0.7888) | Acc: (72.25%) (13964/19328)\n",
            "Epoch: 12 | Batch_idx: 160 |  Loss: (0.7896) | Acc: (72.13%) (14864/20608)\n",
            "Epoch: 12 | Batch_idx: 170 |  Loss: (0.7886) | Acc: (72.14%) (15790/21888)\n",
            "Epoch: 12 | Batch_idx: 180 |  Loss: (0.7867) | Acc: (72.23%) (16735/23168)\n",
            "Epoch: 12 | Batch_idx: 190 |  Loss: (0.7877) | Acc: (72.14%) (17637/24448)\n",
            "Epoch: 12 | Batch_idx: 200 |  Loss: (0.7873) | Acc: (72.16%) (18565/25728)\n",
            "Epoch: 12 | Batch_idx: 210 |  Loss: (0.7856) | Acc: (72.13%) (19481/27008)\n",
            "Epoch: 12 | Batch_idx: 220 |  Loss: (0.7838) | Acc: (72.23%) (20433/28288)\n",
            "Epoch: 12 | Batch_idx: 230 |  Loss: (0.7840) | Acc: (72.24%) (21361/29568)\n",
            "Epoch: 12 | Batch_idx: 240 |  Loss: (0.7821) | Acc: (72.30%) (22304/30848)\n",
            "Epoch: 12 | Batch_idx: 250 |  Loss: (0.7817) | Acc: (72.36%) (23247/32128)\n",
            "Epoch: 12 | Batch_idx: 260 |  Loss: (0.7809) | Acc: (72.40%) (24186/33408)\n",
            "Epoch: 12 | Batch_idx: 270 |  Loss: (0.7808) | Acc: (72.39%) (25109/34688)\n",
            "Epoch: 12 | Batch_idx: 280 |  Loss: (0.7812) | Acc: (72.40%) (26042/35968)\n",
            "Epoch: 12 | Batch_idx: 290 |  Loss: (0.7803) | Acc: (72.40%) (26969/37248)\n",
            "Epoch: 12 | Batch_idx: 300 |  Loss: (0.7782) | Acc: (72.46%) (27919/38528)\n",
            "Epoch: 12 | Batch_idx: 310 |  Loss: (0.7756) | Acc: (72.58%) (28892/39808)\n",
            "Epoch: 12 | Batch_idx: 320 |  Loss: (0.7756) | Acc: (72.63%) (29841/41088)\n",
            "Epoch: 12 | Batch_idx: 330 |  Loss: (0.7753) | Acc: (72.63%) (30772/42368)\n",
            "Epoch: 12 | Batch_idx: 340 |  Loss: (0.7741) | Acc: (72.65%) (31709/43648)\n",
            "Epoch: 12 | Batch_idx: 350 |  Loss: (0.7735) | Acc: (72.67%) (32650/44928)\n",
            "Epoch: 12 | Batch_idx: 360 |  Loss: (0.7729) | Acc: (72.68%) (33583/46208)\n",
            "Epoch: 12 | Batch_idx: 370 |  Loss: (0.7723) | Acc: (72.70%) (34525/47488)\n",
            "Epoch: 12 | Batch_idx: 380 |  Loss: (0.7719) | Acc: (72.75%) (35477/48768)\n",
            "Epoch: 12 | Batch_idx: 390 |  Loss: (0.7717) | Acc: (72.80%) (36399/50000)\n",
            "# TEST : Loss: (0.8621) | Acc: (69.95%) (6995/10000)\n",
            "Epoch: 13 | Batch_idx: 0 |  Loss: (0.7804) | Acc: (73.44%) (94/128)\n",
            "Epoch: 13 | Batch_idx: 10 |  Loss: (0.7503) | Acc: (72.37%) (1019/1408)\n",
            "Epoch: 13 | Batch_idx: 20 |  Loss: (0.7395) | Acc: (72.77%) (1956/2688)\n",
            "Epoch: 13 | Batch_idx: 30 |  Loss: (0.7439) | Acc: (73.21%) (2905/3968)\n",
            "Epoch: 13 | Batch_idx: 40 |  Loss: (0.7346) | Acc: (73.69%) (3867/5248)\n",
            "Epoch: 13 | Batch_idx: 50 |  Loss: (0.7339) | Acc: (73.71%) (4812/6528)\n",
            "Epoch: 13 | Batch_idx: 60 |  Loss: (0.7373) | Acc: (73.63%) (5749/7808)\n",
            "Epoch: 13 | Batch_idx: 70 |  Loss: (0.7355) | Acc: (73.69%) (6697/9088)\n",
            "Epoch: 13 | Batch_idx: 80 |  Loss: (0.7314) | Acc: (74.15%) (7688/10368)\n",
            "Epoch: 13 | Batch_idx: 90 |  Loss: (0.7348) | Acc: (73.99%) (8618/11648)\n",
            "Epoch: 13 | Batch_idx: 100 |  Loss: (0.7345) | Acc: (74.05%) (9573/12928)\n",
            "Epoch: 13 | Batch_idx: 110 |  Loss: (0.7324) | Acc: (74.04%) (10520/14208)\n",
            "Epoch: 13 | Batch_idx: 120 |  Loss: (0.7294) | Acc: (74.23%) (11497/15488)\n",
            "Epoch: 13 | Batch_idx: 130 |  Loss: (0.7332) | Acc: (74.14%) (12431/16768)\n",
            "Epoch: 13 | Batch_idx: 140 |  Loss: (0.7349) | Acc: (74.09%) (13372/18048)\n",
            "Epoch: 13 | Batch_idx: 150 |  Loss: (0.7321) | Acc: (74.15%) (14332/19328)\n",
            "Epoch: 13 | Batch_idx: 160 |  Loss: (0.7303) | Acc: (74.27%) (15306/20608)\n",
            "Epoch: 13 | Batch_idx: 170 |  Loss: (0.7282) | Acc: (74.31%) (16264/21888)\n",
            "Epoch: 13 | Batch_idx: 180 |  Loss: (0.7265) | Acc: (74.33%) (17221/23168)\n",
            "Epoch: 13 | Batch_idx: 190 |  Loss: (0.7247) | Acc: (74.43%) (18197/24448)\n",
            "Epoch: 13 | Batch_idx: 200 |  Loss: (0.7241) | Acc: (74.41%) (19143/25728)\n",
            "Epoch: 13 | Batch_idx: 210 |  Loss: (0.7242) | Acc: (74.42%) (20100/27008)\n",
            "Epoch: 13 | Batch_idx: 220 |  Loss: (0.7226) | Acc: (74.47%) (21067/28288)\n",
            "Epoch: 13 | Batch_idx: 230 |  Loss: (0.7216) | Acc: (74.49%) (22026/29568)\n",
            "Epoch: 13 | Batch_idx: 240 |  Loss: (0.7205) | Acc: (74.54%) (22993/30848)\n",
            "Epoch: 13 | Batch_idx: 250 |  Loss: (0.7181) | Acc: (74.62%) (23973/32128)\n",
            "Epoch: 13 | Batch_idx: 260 |  Loss: (0.7170) | Acc: (74.68%) (24949/33408)\n",
            "Epoch: 13 | Batch_idx: 270 |  Loss: (0.7152) | Acc: (74.77%) (25936/34688)\n",
            "Epoch: 13 | Batch_idx: 280 |  Loss: (0.7131) | Acc: (74.85%) (26921/35968)\n",
            "Epoch: 13 | Batch_idx: 290 |  Loss: (0.7139) | Acc: (74.87%) (27888/37248)\n",
            "Epoch: 13 | Batch_idx: 300 |  Loss: (0.7142) | Acc: (74.87%) (28847/38528)\n",
            "Epoch: 13 | Batch_idx: 310 |  Loss: (0.7142) | Acc: (74.90%) (29817/39808)\n",
            "Epoch: 13 | Batch_idx: 320 |  Loss: (0.7128) | Acc: (74.96%) (30800/41088)\n",
            "Epoch: 13 | Batch_idx: 330 |  Loss: (0.7126) | Acc: (75.02%) (31785/42368)\n",
            "Epoch: 13 | Batch_idx: 340 |  Loss: (0.7125) | Acc: (75.00%) (32738/43648)\n",
            "Epoch: 13 | Batch_idx: 350 |  Loss: (0.7121) | Acc: (75.05%) (33719/44928)\n",
            "Epoch: 13 | Batch_idx: 360 |  Loss: (0.7122) | Acc: (75.06%) (34682/46208)\n",
            "Epoch: 13 | Batch_idx: 370 |  Loss: (0.7102) | Acc: (75.17%) (35695/47488)\n",
            "Epoch: 13 | Batch_idx: 380 |  Loss: (0.7094) | Acc: (75.21%) (36678/48768)\n",
            "Epoch: 13 | Batch_idx: 390 |  Loss: (0.7078) | Acc: (75.26%) (37630/50000)\n",
            "# TEST : Loss: (0.8026) | Acc: (72.11%) (7211/10000)\n",
            "Epoch: 14 | Batch_idx: 0 |  Loss: (0.6429) | Acc: (77.34%) (99/128)\n",
            "Epoch: 14 | Batch_idx: 10 |  Loss: (0.6421) | Acc: (76.63%) (1079/1408)\n",
            "Epoch: 14 | Batch_idx: 20 |  Loss: (0.6498) | Acc: (77.12%) (2073/2688)\n",
            "Epoch: 14 | Batch_idx: 30 |  Loss: (0.6638) | Acc: (76.18%) (3023/3968)\n",
            "Epoch: 14 | Batch_idx: 40 |  Loss: (0.6731) | Acc: (76.16%) (3997/5248)\n",
            "Epoch: 14 | Batch_idx: 50 |  Loss: (0.6738) | Acc: (76.26%) (4978/6528)\n",
            "Epoch: 14 | Batch_idx: 60 |  Loss: (0.6790) | Acc: (76.05%) (5938/7808)\n",
            "Epoch: 14 | Batch_idx: 70 |  Loss: (0.6823) | Acc: (75.98%) (6905/9088)\n",
            "Epoch: 14 | Batch_idx: 80 |  Loss: (0.6742) | Acc: (76.37%) (7918/10368)\n",
            "Epoch: 14 | Batch_idx: 90 |  Loss: (0.6721) | Acc: (76.53%) (8914/11648)\n",
            "Epoch: 14 | Batch_idx: 100 |  Loss: (0.6732) | Acc: (76.51%) (9891/12928)\n",
            "Epoch: 14 | Batch_idx: 110 |  Loss: (0.6731) | Acc: (76.53%) (10874/14208)\n",
            "Epoch: 14 | Batch_idx: 120 |  Loss: (0.6699) | Acc: (76.74%) (11886/15488)\n",
            "Epoch: 14 | Batch_idx: 130 |  Loss: (0.6686) | Acc: (76.78%) (12874/16768)\n",
            "Epoch: 14 | Batch_idx: 140 |  Loss: (0.6702) | Acc: (76.81%) (13862/18048)\n",
            "Epoch: 14 | Batch_idx: 150 |  Loss: (0.6714) | Acc: (76.73%) (14831/19328)\n",
            "Epoch: 14 | Batch_idx: 160 |  Loss: (0.6691) | Acc: (76.81%) (15829/20608)\n",
            "Epoch: 14 | Batch_idx: 170 |  Loss: (0.6690) | Acc: (76.85%) (16821/21888)\n",
            "Epoch: 14 | Batch_idx: 180 |  Loss: (0.6704) | Acc: (76.75%) (17781/23168)\n",
            "Epoch: 14 | Batch_idx: 190 |  Loss: (0.6698) | Acc: (76.71%) (18755/24448)\n",
            "Epoch: 14 | Batch_idx: 200 |  Loss: (0.6690) | Acc: (76.74%) (19743/25728)\n",
            "Epoch: 14 | Batch_idx: 210 |  Loss: (0.6688) | Acc: (76.71%) (20718/27008)\n",
            "Epoch: 14 | Batch_idx: 220 |  Loss: (0.6684) | Acc: (76.78%) (21720/28288)\n",
            "Epoch: 14 | Batch_idx: 230 |  Loss: (0.6684) | Acc: (76.71%) (22682/29568)\n",
            "Epoch: 14 | Batch_idx: 240 |  Loss: (0.6665) | Acc: (76.77%) (23682/30848)\n",
            "Epoch: 14 | Batch_idx: 250 |  Loss: (0.6665) | Acc: (76.81%) (24678/32128)\n",
            "Epoch: 14 | Batch_idx: 260 |  Loss: (0.6655) | Acc: (76.83%) (25667/33408)\n",
            "Epoch: 14 | Batch_idx: 270 |  Loss: (0.6634) | Acc: (76.93%) (26685/34688)\n",
            "Epoch: 14 | Batch_idx: 280 |  Loss: (0.6645) | Acc: (76.88%) (27653/35968)\n",
            "Epoch: 14 | Batch_idx: 290 |  Loss: (0.6650) | Acc: (76.85%) (28624/37248)\n",
            "Epoch: 14 | Batch_idx: 300 |  Loss: (0.6664) | Acc: (76.81%) (29595/38528)\n",
            "Epoch: 14 | Batch_idx: 310 |  Loss: (0.6650) | Acc: (76.81%) (30575/39808)\n",
            "Epoch: 14 | Batch_idx: 320 |  Loss: (0.6649) | Acc: (76.81%) (31560/41088)\n",
            "Epoch: 14 | Batch_idx: 330 |  Loss: (0.6655) | Acc: (76.80%) (32537/42368)\n",
            "Epoch: 14 | Batch_idx: 340 |  Loss: (0.6649) | Acc: (76.80%) (33522/43648)\n",
            "Epoch: 14 | Batch_idx: 350 |  Loss: (0.6645) | Acc: (76.83%) (34516/44928)\n",
            "Epoch: 14 | Batch_idx: 360 |  Loss: (0.6639) | Acc: (76.86%) (35514/46208)\n",
            "Epoch: 14 | Batch_idx: 370 |  Loss: (0.6623) | Acc: (76.92%) (36526/47488)\n",
            "Epoch: 14 | Batch_idx: 380 |  Loss: (0.6610) | Acc: (76.98%) (37544/48768)\n",
            "Epoch: 14 | Batch_idx: 390 |  Loss: (0.6597) | Acc: (77.05%) (38523/50000)\n",
            "# TEST : Loss: (0.9836) | Acc: (67.36%) (6736/10000)\n",
            "Epoch: 15 | Batch_idx: 0 |  Loss: (0.6903) | Acc: (78.91%) (101/128)\n",
            "Epoch: 15 | Batch_idx: 10 |  Loss: (0.6204) | Acc: (79.26%) (1116/1408)\n",
            "Epoch: 15 | Batch_idx: 20 |  Loss: (0.5966) | Acc: (79.09%) (2126/2688)\n",
            "Epoch: 15 | Batch_idx: 30 |  Loss: (0.6079) | Acc: (78.70%) (3123/3968)\n",
            "Epoch: 15 | Batch_idx: 40 |  Loss: (0.6174) | Acc: (78.16%) (4102/5248)\n",
            "Epoch: 15 | Batch_idx: 50 |  Loss: (0.6229) | Acc: (78.12%) (5100/6528)\n",
            "Epoch: 15 | Batch_idx: 60 |  Loss: (0.6204) | Acc: (78.21%) (6107/7808)\n",
            "Epoch: 15 | Batch_idx: 70 |  Loss: (0.6183) | Acc: (78.39%) (7124/9088)\n",
            "Epoch: 15 | Batch_idx: 80 |  Loss: (0.6183) | Acc: (78.44%) (8133/10368)\n",
            "Epoch: 15 | Batch_idx: 90 |  Loss: (0.6188) | Acc: (78.43%) (9135/11648)\n",
            "Epoch: 15 | Batch_idx: 100 |  Loss: (0.6172) | Acc: (78.41%) (10137/12928)\n",
            "Epoch: 15 | Batch_idx: 110 |  Loss: (0.6149) | Acc: (78.49%) (11152/14208)\n",
            "Epoch: 15 | Batch_idx: 120 |  Loss: (0.6154) | Acc: (78.53%) (12162/15488)\n",
            "Epoch: 15 | Batch_idx: 130 |  Loss: (0.6149) | Acc: (78.54%) (13170/16768)\n",
            "Epoch: 15 | Batch_idx: 140 |  Loss: (0.6168) | Acc: (78.49%) (14166/18048)\n",
            "Epoch: 15 | Batch_idx: 150 |  Loss: (0.6186) | Acc: (78.52%) (15177/19328)\n",
            "Epoch: 15 | Batch_idx: 160 |  Loss: (0.6173) | Acc: (78.55%) (16187/20608)\n",
            "Epoch: 15 | Batch_idx: 170 |  Loss: (0.6187) | Acc: (78.44%) (17169/21888)\n",
            "Epoch: 15 | Batch_idx: 180 |  Loss: (0.6186) | Acc: (78.41%) (18166/23168)\n",
            "Epoch: 15 | Batch_idx: 190 |  Loss: (0.6190) | Acc: (78.44%) (19178/24448)\n",
            "Epoch: 15 | Batch_idx: 200 |  Loss: (0.6207) | Acc: (78.43%) (20178/25728)\n",
            "Epoch: 15 | Batch_idx: 210 |  Loss: (0.6204) | Acc: (78.40%) (21174/27008)\n",
            "Epoch: 15 | Batch_idx: 220 |  Loss: (0.6203) | Acc: (78.41%) (22182/28288)\n",
            "Epoch: 15 | Batch_idx: 230 |  Loss: (0.6205) | Acc: (78.44%) (23192/29568)\n",
            "Epoch: 15 | Batch_idx: 240 |  Loss: (0.6200) | Acc: (78.43%) (24193/30848)\n",
            "Epoch: 15 | Batch_idx: 250 |  Loss: (0.6215) | Acc: (78.36%) (25176/32128)\n",
            "Epoch: 15 | Batch_idx: 260 |  Loss: (0.6220) | Acc: (78.37%) (26183/33408)\n",
            "Epoch: 15 | Batch_idx: 270 |  Loss: (0.6238) | Acc: (78.26%) (27148/34688)\n",
            "Epoch: 15 | Batch_idx: 280 |  Loss: (0.6241) | Acc: (78.26%) (28147/35968)\n",
            "Epoch: 15 | Batch_idx: 290 |  Loss: (0.6231) | Acc: (78.30%) (29165/37248)\n",
            "Epoch: 15 | Batch_idx: 300 |  Loss: (0.6226) | Acc: (78.36%) (30189/38528)\n",
            "Epoch: 15 | Batch_idx: 310 |  Loss: (0.6225) | Acc: (78.37%) (31198/39808)\n",
            "Epoch: 15 | Batch_idx: 320 |  Loss: (0.6219) | Acc: (78.37%) (32199/41088)\n",
            "Epoch: 15 | Batch_idx: 330 |  Loss: (0.6208) | Acc: (78.39%) (33214/42368)\n",
            "Epoch: 15 | Batch_idx: 340 |  Loss: (0.6194) | Acc: (78.47%) (34249/43648)\n",
            "Epoch: 15 | Batch_idx: 350 |  Loss: (0.6193) | Acc: (78.47%) (35255/44928)\n",
            "Epoch: 15 | Batch_idx: 360 |  Loss: (0.6200) | Acc: (78.46%) (36255/46208)\n",
            "Epoch: 15 | Batch_idx: 370 |  Loss: (0.6197) | Acc: (78.49%) (37272/47488)\n",
            "Epoch: 15 | Batch_idx: 380 |  Loss: (0.6197) | Acc: (78.48%) (38274/48768)\n",
            "Epoch: 15 | Batch_idx: 390 |  Loss: (0.6183) | Acc: (78.54%) (39270/50000)\n",
            "# TEST : Loss: (0.7652) | Acc: (75.12%) (7512/10000)\n",
            "Epoch: 16 | Batch_idx: 0 |  Loss: (0.7809) | Acc: (67.97%) (87/128)\n",
            "Epoch: 16 | Batch_idx: 10 |  Loss: (0.6561) | Acc: (76.99%) (1084/1408)\n",
            "Epoch: 16 | Batch_idx: 20 |  Loss: (0.6243) | Acc: (77.60%) (2086/2688)\n",
            "Epoch: 16 | Batch_idx: 30 |  Loss: (0.6051) | Acc: (78.28%) (3106/3968)\n",
            "Epoch: 16 | Batch_idx: 40 |  Loss: (0.5950) | Acc: (78.77%) (4134/5248)\n",
            "Epoch: 16 | Batch_idx: 50 |  Loss: (0.5981) | Acc: (78.80%) (5144/6528)\n",
            "Epoch: 16 | Batch_idx: 60 |  Loss: (0.5940) | Acc: (79.15%) (6180/7808)\n",
            "Epoch: 16 | Batch_idx: 70 |  Loss: (0.5921) | Acc: (79.31%) (7208/9088)\n",
            "Epoch: 16 | Batch_idx: 80 |  Loss: (0.5962) | Acc: (79.24%) (8216/10368)\n",
            "Epoch: 16 | Batch_idx: 90 |  Loss: (0.5953) | Acc: (79.25%) (9231/11648)\n",
            "Epoch: 16 | Batch_idx: 100 |  Loss: (0.5920) | Acc: (79.44%) (10270/12928)\n",
            "Epoch: 16 | Batch_idx: 110 |  Loss: (0.5891) | Acc: (79.50%) (11295/14208)\n",
            "Epoch: 16 | Batch_idx: 120 |  Loss: (0.5876) | Acc: (79.51%) (12314/15488)\n",
            "Epoch: 16 | Batch_idx: 130 |  Loss: (0.5859) | Acc: (79.51%) (13333/16768)\n",
            "Epoch: 16 | Batch_idx: 140 |  Loss: (0.5848) | Acc: (79.63%) (14371/18048)\n",
            "Epoch: 16 | Batch_idx: 150 |  Loss: (0.5865) | Acc: (79.55%) (15376/19328)\n",
            "Epoch: 16 | Batch_idx: 160 |  Loss: (0.5864) | Acc: (79.61%) (16406/20608)\n",
            "Epoch: 16 | Batch_idx: 170 |  Loss: (0.5862) | Acc: (79.61%) (17425/21888)\n",
            "Epoch: 16 | Batch_idx: 180 |  Loss: (0.5872) | Acc: (79.60%) (18441/23168)\n",
            "Epoch: 16 | Batch_idx: 190 |  Loss: (0.5881) | Acc: (79.50%) (19437/24448)\n",
            "Epoch: 16 | Batch_idx: 200 |  Loss: (0.5865) | Acc: (79.60%) (20479/25728)\n",
            "Epoch: 16 | Batch_idx: 210 |  Loss: (0.5867) | Acc: (79.60%) (21498/27008)\n",
            "Epoch: 16 | Batch_idx: 220 |  Loss: (0.5878) | Acc: (79.60%) (22518/28288)\n",
            "Epoch: 16 | Batch_idx: 230 |  Loss: (0.5890) | Acc: (79.57%) (23526/29568)\n",
            "Epoch: 16 | Batch_idx: 240 |  Loss: (0.5892) | Acc: (79.57%) (24545/30848)\n",
            "Epoch: 16 | Batch_idx: 250 |  Loss: (0.5892) | Acc: (79.58%) (25567/32128)\n",
            "Epoch: 16 | Batch_idx: 260 |  Loss: (0.5902) | Acc: (79.59%) (26589/33408)\n",
            "Epoch: 16 | Batch_idx: 270 |  Loss: (0.5893) | Acc: (79.64%) (27625/34688)\n",
            "Epoch: 16 | Batch_idx: 280 |  Loss: (0.5906) | Acc: (79.55%) (28613/35968)\n",
            "Epoch: 16 | Batch_idx: 290 |  Loss: (0.5920) | Acc: (79.48%) (29604/37248)\n",
            "Epoch: 16 | Batch_idx: 300 |  Loss: (0.5917) | Acc: (79.47%) (30617/38528)\n",
            "Epoch: 16 | Batch_idx: 310 |  Loss: (0.5935) | Acc: (79.39%) (31605/39808)\n",
            "Epoch: 16 | Batch_idx: 320 |  Loss: (0.5921) | Acc: (79.40%) (32625/41088)\n",
            "Epoch: 16 | Batch_idx: 330 |  Loss: (0.5917) | Acc: (79.45%) (33663/42368)\n",
            "Epoch: 16 | Batch_idx: 340 |  Loss: (0.5918) | Acc: (79.44%) (34676/43648)\n",
            "Epoch: 16 | Batch_idx: 350 |  Loss: (0.5921) | Acc: (79.46%) (35700/44928)\n",
            "Epoch: 16 | Batch_idx: 360 |  Loss: (0.5913) | Acc: (79.50%) (36735/46208)\n",
            "Epoch: 16 | Batch_idx: 370 |  Loss: (0.5909) | Acc: (79.56%) (37783/47488)\n",
            "Epoch: 16 | Batch_idx: 380 |  Loss: (0.5913) | Acc: (79.55%) (38794/48768)\n",
            "Epoch: 16 | Batch_idx: 390 |  Loss: (0.5911) | Acc: (79.58%) (39790/50000)\n",
            "# TEST : Loss: (0.6405) | Acc: (77.69%) (7769/10000)\n",
            "Epoch: 17 | Batch_idx: 0 |  Loss: (0.6101) | Acc: (78.12%) (100/128)\n",
            "Epoch: 17 | Batch_idx: 10 |  Loss: (0.5541) | Acc: (80.82%) (1138/1408)\n",
            "Epoch: 17 | Batch_idx: 20 |  Loss: (0.5375) | Acc: (81.25%) (2184/2688)\n",
            "Epoch: 17 | Batch_idx: 30 |  Loss: (0.5545) | Acc: (81.05%) (3216/3968)\n",
            "Epoch: 17 | Batch_idx: 40 |  Loss: (0.5696) | Acc: (80.58%) (4229/5248)\n",
            "Epoch: 17 | Batch_idx: 50 |  Loss: (0.5647) | Acc: (80.67%) (5266/6528)\n",
            "Epoch: 17 | Batch_idx: 60 |  Loss: (0.5590) | Acc: (80.74%) (6304/7808)\n",
            "Epoch: 17 | Batch_idx: 70 |  Loss: (0.5539) | Acc: (80.96%) (7358/9088)\n",
            "Epoch: 17 | Batch_idx: 80 |  Loss: (0.5547) | Acc: (81.02%) (8400/10368)\n",
            "Epoch: 17 | Batch_idx: 90 |  Loss: (0.5562) | Acc: (80.97%) (9431/11648)\n",
            "Epoch: 17 | Batch_idx: 100 |  Loss: (0.5582) | Acc: (80.84%) (10451/12928)\n",
            "Epoch: 17 | Batch_idx: 110 |  Loss: (0.5612) | Acc: (80.65%) (11459/14208)\n",
            "Epoch: 17 | Batch_idx: 120 |  Loss: (0.5616) | Acc: (80.66%) (12492/15488)\n",
            "Epoch: 17 | Batch_idx: 130 |  Loss: (0.5591) | Acc: (80.73%) (13536/16768)\n",
            "Epoch: 17 | Batch_idx: 140 |  Loss: (0.5582) | Acc: (80.85%) (14591/18048)\n",
            "Epoch: 17 | Batch_idx: 150 |  Loss: (0.5599) | Acc: (80.79%) (15616/19328)\n",
            "Epoch: 17 | Batch_idx: 160 |  Loss: (0.5611) | Acc: (80.79%) (16649/20608)\n",
            "Epoch: 17 | Batch_idx: 170 |  Loss: (0.5603) | Acc: (80.77%) (17678/21888)\n",
            "Epoch: 17 | Batch_idx: 180 |  Loss: (0.5575) | Acc: (80.82%) (18725/23168)\n",
            "Epoch: 17 | Batch_idx: 190 |  Loss: (0.5559) | Acc: (80.87%) (19771/24448)\n",
            "Epoch: 17 | Batch_idx: 200 |  Loss: (0.5570) | Acc: (80.84%) (20798/25728)\n",
            "Epoch: 17 | Batch_idx: 210 |  Loss: (0.5567) | Acc: (80.82%) (21827/27008)\n",
            "Epoch: 17 | Batch_idx: 220 |  Loss: (0.5582) | Acc: (80.78%) (22850/28288)\n",
            "Epoch: 17 | Batch_idx: 230 |  Loss: (0.5588) | Acc: (80.78%) (23886/29568)\n",
            "Epoch: 17 | Batch_idx: 240 |  Loss: (0.5581) | Acc: (80.81%) (24927/30848)\n",
            "Epoch: 17 | Batch_idx: 250 |  Loss: (0.5602) | Acc: (80.72%) (25935/32128)\n",
            "Epoch: 17 | Batch_idx: 260 |  Loss: (0.5615) | Acc: (80.71%) (26963/33408)\n",
            "Epoch: 17 | Batch_idx: 270 |  Loss: (0.5616) | Acc: (80.67%) (27982/34688)\n",
            "Epoch: 17 | Batch_idx: 280 |  Loss: (0.5609) | Acc: (80.71%) (29028/35968)\n",
            "Epoch: 17 | Batch_idx: 290 |  Loss: (0.5607) | Acc: (80.68%) (30050/37248)\n",
            "Epoch: 17 | Batch_idx: 300 |  Loss: (0.5601) | Acc: (80.68%) (31084/38528)\n",
            "Epoch: 17 | Batch_idx: 310 |  Loss: (0.5608) | Acc: (80.64%) (32100/39808)\n",
            "Epoch: 17 | Batch_idx: 320 |  Loss: (0.5602) | Acc: (80.66%) (33141/41088)\n",
            "Epoch: 17 | Batch_idx: 330 |  Loss: (0.5610) | Acc: (80.65%) (34168/42368)\n",
            "Epoch: 17 | Batch_idx: 340 |  Loss: (0.5603) | Acc: (80.67%) (35213/43648)\n",
            "Epoch: 17 | Batch_idx: 350 |  Loss: (0.5602) | Acc: (80.69%) (36253/44928)\n",
            "Epoch: 17 | Batch_idx: 360 |  Loss: (0.5588) | Acc: (80.74%) (37307/46208)\n",
            "Epoch: 17 | Batch_idx: 370 |  Loss: (0.5570) | Acc: (80.81%) (38374/47488)\n",
            "Epoch: 17 | Batch_idx: 380 |  Loss: (0.5573) | Acc: (80.82%) (39412/48768)\n",
            "Epoch: 17 | Batch_idx: 390 |  Loss: (0.5582) | Acc: (80.78%) (40390/50000)\n",
            "# TEST : Loss: (0.7248) | Acc: (75.31%) (7531/10000)\n",
            "Epoch: 18 | Batch_idx: 0 |  Loss: (0.7161) | Acc: (75.78%) (97/128)\n",
            "Epoch: 18 | Batch_idx: 10 |  Loss: (0.5357) | Acc: (81.68%) (1150/1408)\n",
            "Epoch: 18 | Batch_idx: 20 |  Loss: (0.5354) | Acc: (81.18%) (2182/2688)\n",
            "Epoch: 18 | Batch_idx: 30 |  Loss: (0.5340) | Acc: (81.33%) (3227/3968)\n",
            "Epoch: 18 | Batch_idx: 40 |  Loss: (0.5288) | Acc: (81.31%) (4267/5248)\n",
            "Epoch: 18 | Batch_idx: 50 |  Loss: (0.5377) | Acc: (80.84%) (5277/6528)\n",
            "Epoch: 18 | Batch_idx: 60 |  Loss: (0.5431) | Acc: (80.72%) (6303/7808)\n",
            "Epoch: 18 | Batch_idx: 70 |  Loss: (0.5412) | Acc: (80.95%) (7357/9088)\n",
            "Epoch: 18 | Batch_idx: 80 |  Loss: (0.5357) | Acc: (81.16%) (8415/10368)\n",
            "Epoch: 18 | Batch_idx: 90 |  Loss: (0.5366) | Acc: (81.22%) (9461/11648)\n",
            "Epoch: 18 | Batch_idx: 100 |  Loss: (0.5397) | Acc: (81.20%) (10497/12928)\n",
            "Epoch: 18 | Batch_idx: 110 |  Loss: (0.5398) | Acc: (81.19%) (11535/14208)\n",
            "Epoch: 18 | Batch_idx: 120 |  Loss: (0.5414) | Acc: (81.10%) (12561/15488)\n",
            "Epoch: 18 | Batch_idx: 130 |  Loss: (0.5414) | Acc: (81.10%) (13599/16768)\n",
            "Epoch: 18 | Batch_idx: 140 |  Loss: (0.5419) | Acc: (81.10%) (14637/18048)\n",
            "Epoch: 18 | Batch_idx: 150 |  Loss: (0.5436) | Acc: (81.04%) (15663/19328)\n",
            "Epoch: 18 | Batch_idx: 160 |  Loss: (0.5407) | Acc: (81.15%) (16723/20608)\n",
            "Epoch: 18 | Batch_idx: 170 |  Loss: (0.5399) | Acc: (81.20%) (17774/21888)\n",
            "Epoch: 18 | Batch_idx: 180 |  Loss: (0.5385) | Acc: (81.30%) (18836/23168)\n",
            "Epoch: 18 | Batch_idx: 190 |  Loss: (0.5392) | Acc: (81.32%) (19880/24448)\n",
            "Epoch: 18 | Batch_idx: 200 |  Loss: (0.5396) | Acc: (81.30%) (20918/25728)\n",
            "Epoch: 18 | Batch_idx: 210 |  Loss: (0.5372) | Acc: (81.42%) (21990/27008)\n",
            "Epoch: 18 | Batch_idx: 220 |  Loss: (0.5377) | Acc: (81.40%) (23026/28288)\n",
            "Epoch: 18 | Batch_idx: 230 |  Loss: (0.5364) | Acc: (81.44%) (24081/29568)\n",
            "Epoch: 18 | Batch_idx: 240 |  Loss: (0.5356) | Acc: (81.47%) (25133/30848)\n",
            "Epoch: 18 | Batch_idx: 250 |  Loss: (0.5383) | Acc: (81.40%) (26152/32128)\n",
            "Epoch: 18 | Batch_idx: 260 |  Loss: (0.5390) | Acc: (81.37%) (27185/33408)\n",
            "Epoch: 18 | Batch_idx: 270 |  Loss: (0.5398) | Acc: (81.35%) (28219/34688)\n",
            "Epoch: 18 | Batch_idx: 280 |  Loss: (0.5400) | Acc: (81.31%) (29245/35968)\n",
            "Epoch: 18 | Batch_idx: 290 |  Loss: (0.5396) | Acc: (81.35%) (30302/37248)\n",
            "Epoch: 18 | Batch_idx: 300 |  Loss: (0.5401) | Acc: (81.34%) (31339/38528)\n",
            "Epoch: 18 | Batch_idx: 310 |  Loss: (0.5399) | Acc: (81.35%) (32385/39808)\n",
            "Epoch: 18 | Batch_idx: 320 |  Loss: (0.5387) | Acc: (81.38%) (33439/41088)\n",
            "Epoch: 18 | Batch_idx: 330 |  Loss: (0.5394) | Acc: (81.35%) (34466/42368)\n",
            "Epoch: 18 | Batch_idx: 340 |  Loss: (0.5392) | Acc: (81.38%) (35519/43648)\n",
            "Epoch: 18 | Batch_idx: 350 |  Loss: (0.5390) | Acc: (81.38%) (36563/44928)\n",
            "Epoch: 18 | Batch_idx: 360 |  Loss: (0.5398) | Acc: (81.34%) (37586/46208)\n",
            "Epoch: 18 | Batch_idx: 370 |  Loss: (0.5386) | Acc: (81.39%) (38652/47488)\n",
            "Epoch: 18 | Batch_idx: 380 |  Loss: (0.5376) | Acc: (81.41%) (39702/48768)\n",
            "Epoch: 18 | Batch_idx: 390 |  Loss: (0.5388) | Acc: (81.36%) (40679/50000)\n",
            "# TEST : Loss: (0.6670) | Acc: (77.38%) (7738/10000)\n",
            "Epoch: 19 | Batch_idx: 0 |  Loss: (0.5007) | Acc: (82.81%) (106/128)\n",
            "Epoch: 19 | Batch_idx: 10 |  Loss: (0.5132) | Acc: (82.60%) (1163/1408)\n",
            "Epoch: 19 | Batch_idx: 20 |  Loss: (0.5204) | Acc: (82.22%) (2210/2688)\n",
            "Epoch: 19 | Batch_idx: 30 |  Loss: (0.5172) | Acc: (82.26%) (3264/3968)\n",
            "Epoch: 19 | Batch_idx: 40 |  Loss: (0.5128) | Acc: (82.39%) (4324/5248)\n",
            "Epoch: 19 | Batch_idx: 50 |  Loss: (0.5229) | Acc: (81.85%) (5343/6528)\n",
            "Epoch: 19 | Batch_idx: 60 |  Loss: (0.5220) | Acc: (81.97%) (6400/7808)\n",
            "Epoch: 19 | Batch_idx: 70 |  Loss: (0.5159) | Acc: (82.16%) (7467/9088)\n",
            "Epoch: 19 | Batch_idx: 80 |  Loss: (0.5151) | Acc: (82.21%) (8524/10368)\n",
            "Epoch: 19 | Batch_idx: 90 |  Loss: (0.5142) | Acc: (82.30%) (9586/11648)\n",
            "Epoch: 19 | Batch_idx: 100 |  Loss: (0.5152) | Acc: (82.14%) (10619/12928)\n",
            "Epoch: 19 | Batch_idx: 110 |  Loss: (0.5149) | Acc: (82.17%) (11675/14208)\n",
            "Epoch: 19 | Batch_idx: 120 |  Loss: (0.5134) | Acc: (82.26%) (12741/15488)\n",
            "Epoch: 19 | Batch_idx: 130 |  Loss: (0.5127) | Acc: (82.30%) (13800/16768)\n",
            "Epoch: 19 | Batch_idx: 140 |  Loss: (0.5110) | Acc: (82.37%) (14866/18048)\n",
            "Epoch: 19 | Batch_idx: 150 |  Loss: (0.5107) | Acc: (82.39%) (15924/19328)\n",
            "Epoch: 19 | Batch_idx: 160 |  Loss: (0.5108) | Acc: (82.36%) (16973/20608)\n",
            "Epoch: 19 | Batch_idx: 170 |  Loss: (0.5086) | Acc: (82.40%) (18035/21888)\n",
            "Epoch: 19 | Batch_idx: 180 |  Loss: (0.5093) | Acc: (82.39%) (19088/23168)\n",
            "Epoch: 19 | Batch_idx: 190 |  Loss: (0.5078) | Acc: (82.49%) (20167/24448)\n",
            "Epoch: 19 | Batch_idx: 200 |  Loss: (0.5073) | Acc: (82.48%) (21221/25728)\n",
            "Epoch: 19 | Batch_idx: 210 |  Loss: (0.5073) | Acc: (82.48%) (22277/27008)\n",
            "Epoch: 19 | Batch_idx: 220 |  Loss: (0.5083) | Acc: (82.48%) (23331/28288)\n",
            "Epoch: 19 | Batch_idx: 230 |  Loss: (0.5074) | Acc: (82.52%) (24399/29568)\n",
            "Epoch: 19 | Batch_idx: 240 |  Loss: (0.5070) | Acc: (82.49%) (25448/30848)\n",
            "Epoch: 19 | Batch_idx: 250 |  Loss: (0.5070) | Acc: (82.47%) (26496/32128)\n",
            "Epoch: 19 | Batch_idx: 260 |  Loss: (0.5067) | Acc: (82.48%) (27555/33408)\n",
            "Epoch: 19 | Batch_idx: 270 |  Loss: (0.5054) | Acc: (82.48%) (28609/34688)\n",
            "Epoch: 19 | Batch_idx: 280 |  Loss: (0.5063) | Acc: (82.48%) (29665/35968)\n",
            "Epoch: 19 | Batch_idx: 290 |  Loss: (0.5053) | Acc: (82.47%) (30719/37248)\n",
            "Epoch: 19 | Batch_idx: 300 |  Loss: (0.5036) | Acc: (82.55%) (31804/38528)\n",
            "Epoch: 19 | Batch_idx: 310 |  Loss: (0.5021) | Acc: (82.63%) (32892/39808)\n",
            "Epoch: 19 | Batch_idx: 320 |  Loss: (0.5019) | Acc: (82.61%) (33943/41088)\n",
            "Epoch: 19 | Batch_idx: 330 |  Loss: (0.5028) | Acc: (82.59%) (34991/42368)\n",
            "Epoch: 19 | Batch_idx: 340 |  Loss: (0.5033) | Acc: (82.54%) (36025/43648)\n",
            "Epoch: 19 | Batch_idx: 350 |  Loss: (0.5031) | Acc: (82.54%) (37082/44928)\n",
            "Epoch: 19 | Batch_idx: 360 |  Loss: (0.5031) | Acc: (82.56%) (38148/46208)\n",
            "Epoch: 19 | Batch_idx: 370 |  Loss: (0.5038) | Acc: (82.56%) (39207/47488)\n",
            "Epoch: 19 | Batch_idx: 380 |  Loss: (0.5044) | Acc: (82.53%) (40247/48768)\n",
            "Epoch: 19 | Batch_idx: 390 |  Loss: (0.5061) | Acc: (82.46%) (41232/50000)\n",
            "# TEST : Loss: (0.6464) | Acc: (78.31%) (7831/10000)\n",
            "0 hours 6 mins 57 secs for training\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torchvision import datasets, transforms\n",
        "import time\n",
        "import os\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
        "start_time = time.time()\n",
        "batch_size = 128\n",
        "learning_rate = 0.1\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding = 4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.4914, 0.4824, 0.4467),\n",
        "                          std=(0.2471, 0.2436, 0.2616))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.4914, 0.4824, 0.4467),\n",
        "                          std=(0.2471, 0.2436, 0.2616))\n",
        "])\n",
        "\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='/content/pytorch/data/cifar10/',\n",
        "                                 train=True,\n",
        "                                 transform=transform_train,\n",
        "                                 download=True)\n",
        "\n",
        "test_dataset = datasets.CIFAR10(root='/content/pytorch/data/cifar10/',\n",
        "                                train=False,\n",
        "                                transform=transform_test)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True,\n",
        "                                           num_workers=2)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False,\n",
        "                                          num_workers=2)\n",
        "\n",
        "\n",
        "class Vgg(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(Vgg, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.classifier = nn.Linear(2048, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        # x.size()=[batch_size, channel, width, height] \n",
        "        #          [128, 512, 2, 2] \n",
        "        # flatten 결과 => [128, 512x2x2] \n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "model = Vgg()\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-4)\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "if torch.cuda.device_count() > 0:\n",
        "    print(\"USE\", torch.cuda.device_count(), \"GPUs!\")\n",
        "    model = nn.DataParallel(model)\n",
        "    cudnn.benchmark = True\n",
        "else:\n",
        "    print(\"USE ONLY CPU!\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "\n",
        "writer = SummaryWriter()\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        if torch.cuda.is_available():\n",
        "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
        "        else:\n",
        "            data, target = Variable(data), Variable(target)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        # torch.max() : (maximum value, index of maximum value) return. \n",
        "        # 1 :  row마다 max계산 (즉, row는 10개의 class를 의미) \n",
        "        # 0 : column마다 max 계산 \n",
        "        total += target.size(0)\n",
        "        correct += predicted.eq(target.data).cpu().sum()\n",
        "        if batch_idx % 10 == 0:        \n",
        "            print('Epoch: {} | Batch_idx: {} |  Loss: ({:.4f}) | Acc: ({:.2f}%) ({}/{})'\n",
        "                  .format(epoch, batch_idx, train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "            if writer:\n",
        "                writer.add_scalar('Loss/train', train_loss/(batch_idx+1), epoch*40 + batch_idx//10)\n",
        "                writer.add_scalar('Accuracy/train', 100.*correct/total, epoch*40 + batch_idx//10)\n",
        "def test():\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (data, target) in enumerate(test_loader):\n",
        "        if torch.cuda.is_available():\n",
        "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
        "        else:\n",
        "            data, target = Variable(data), Variable(target)\n",
        "\n",
        "        outputs = model(data)\n",
        "        loss = criterion(outputs, target)\n",
        "\n",
        "        test_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += target.size(0)\n",
        "        correct += predicted.eq(target.data).cpu().sum()\n",
        "    print('# TEST : Loss: ({:.4f}) | Acc: ({:.2f}%) ({}/{})'\n",
        "      .format(test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "    if writer:\n",
        "        writer.add_scalar('Loss/test', test_loss/(batch_idx+1), epoch)\n",
        "        writer.add_scalar('Accuracy/test', 100.*correct/total, epoch)\n",
        "\n",
        "for epoch in range(0, 20): #165):\n",
        "    if epoch < 80:\n",
        "        learning_rate = learning_rate\n",
        "    elif epoch < 120:\n",
        "        learning_rate = learning_rate * 0.1\n",
        "    else:\n",
        "        learning_rate = learning_rate * 0.01\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['learning_rate'] = learning_rate\n",
        "\n",
        "    train(epoch)\n",
        "    test()\n",
        "\n",
        "writer.close()\n",
        "\n",
        "now = time.gmtime(time.time() - start_time)\n",
        "print('{} hours {} mins {} secs for training'.format(now.tm_hour, now.tm_min, now.tm_sec))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "[2022] [학번] CVassignment3.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "bd9e381d39ab9633cca76cd3e15ee02b9664d8008f1b7c104e61dafb323a5b70"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 ('ML')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
