{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEvKl8H0YNtf",
        "outputId": "49b642bf-c36e-4ca0-b8d7-c3114014c875"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thu Jun  9 22:23:21 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   67C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nrd4rdBvX46W",
        "outputId": "8f864390-d0d1-4f6b-ca1e-1e40c5d34648"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.11.0+cu113\n",
            "Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJO4AenrYK7N"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torchvision import datasets, transforms\n",
        "import time\n",
        "import os\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torchsummary import summary\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgB281DPYNt0"
      },
      "outputs": [],
      "source": [
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
        "start_time = time.time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hw3Z8kqyYNt6"
      },
      "source": [
        "### Data\n",
        "#### Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXPuW60DYNt_"
      },
      "outputs": [],
      "source": [
        "high_transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    # transforms.RandomVerticalFlip(0.1),\n",
        "    # transforms.ColorJitter(contrast=0.1, saturation=0.1, hue=0.1),\n",
        "    # transforms.RandomAffine(30, translate=(0.2,0.2), scale=(0.8,1.6), shear=20),\n",
        "    transforms.RandomAffine(0, translate=(0.1,0.1), scale=(0.9,1.5)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.4914, 0.4824, 0.4467),\n",
        "                          std=(0.2471, 0.2436, 0.2616))\n",
        "])\n",
        "\n",
        "low_transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.4914, 0.4824, 0.4467),\n",
        "                          std=(0.2471, 0.2436, 0.2616))\n",
        "])\n",
        "\n",
        "transform_train = transforms.RandomChoice([high_transform_train,low_transform_train],p=[1,0])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.4914, 0.4824, 0.4467),\n",
        "                          std=(0.2471, 0.2436, 0.2616))\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQxXpAxYYNuB"
      },
      "source": [
        "#### Data loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7lOQFHbYNuE",
        "outputId": "48ad9976-f59d-46f8-bc85-09c4c7015e85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "image_size = (3,32,32)\n",
        "train_dataset = datasets.CIFAR10(root='/content/pytorch/data/cifar10/',\n",
        "                                 train=True,\n",
        "                                 transform=transform_train,\n",
        "                                 download=True)\n",
        "\n",
        "\n",
        "test_dataset = datasets.CIFAR10(root='/content/pytorch/data/cifar10/',\n",
        "                                train=False,\n",
        "                                transform=transform_test)\n",
        "\n",
        "batch_size = 128\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True,\n",
        "                                           num_workers=2)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False,\n",
        "                                          num_workers=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YEk84dGYNuH"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "labels_map= {\n",
        "    0 : 'airplane',\n",
        "    1 : 'automobile',\n",
        "    2 : 'bird',\n",
        "    3 : 'cat',\n",
        "    4 : 'deer',\n",
        "    5 : 'dog',\n",
        "    6 : 'frog',\n",
        "    7 : 'horse',\n",
        "    8 : 'ship',\n",
        "    9 : 'truck',\n",
        "}\n",
        "\n",
        "def imshow():\n",
        "    figure = plt.figure(figsize=(8, 8))\n",
        "    cols, rows = 3, 3\n",
        "    for i in range(1, cols * rows + 1):\n",
        "        sample_idx = torch.randint(len(train_dataset), size=(1,)).item()\n",
        "        img, label = train_dataset[sample_idx]\n",
        "        figure.add_subplot(rows, cols, i)\n",
        "        plt.title(labels_map[label])\n",
        "        plt.axis(\"off\")\n",
        "        plt.imshow(np.transpose(img, (1, 2, 0)))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        },
        "id": "5WfajyEjYNuK",
        "outputId": "4eaf0344-0707-4de6-e27b-1308bd323e7f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 576x576 with 9 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "imshow()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OCfyrHOYNuN"
      },
      "source": [
        "### Design Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZKBMeohYNuO"
      },
      "outputs": [],
      "source": [
        "class Vgg(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(Vgg, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.classifier = nn.Linear(2048, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        # x.size()=[batch_size, channel, width, height] \n",
        "        #          [128, 512, 2, 2] \n",
        "        # flatten 결과 => [128, 512x2x2] \n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkf3nLwFYNuR",
        "outputId": "47b058db-398c-4762-932d-b9bb46e5c919"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 32, 32]           1,792\n",
            "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
            "              ReLU-3           [-1, 64, 32, 32]               0\n",
            "            Conv2d-4           [-1, 64, 32, 32]          36,928\n",
            "       BatchNorm2d-5           [-1, 64, 32, 32]             128\n",
            "              ReLU-6           [-1, 64, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 64, 16, 16]               0\n",
            "            Conv2d-8          [-1, 128, 16, 16]          73,856\n",
            "       BatchNorm2d-9          [-1, 128, 16, 16]             256\n",
            "             ReLU-10          [-1, 128, 16, 16]               0\n",
            "           Conv2d-11          [-1, 128, 16, 16]         147,584\n",
            "      BatchNorm2d-12          [-1, 128, 16, 16]             256\n",
            "             ReLU-13          [-1, 128, 16, 16]               0\n",
            "        MaxPool2d-14            [-1, 128, 8, 8]               0\n",
            "           Conv2d-15            [-1, 256, 8, 8]         295,168\n",
            "      BatchNorm2d-16            [-1, 256, 8, 8]             512\n",
            "             ReLU-17            [-1, 256, 8, 8]               0\n",
            "           Conv2d-18            [-1, 256, 8, 8]         590,080\n",
            "      BatchNorm2d-19            [-1, 256, 8, 8]             512\n",
            "             ReLU-20            [-1, 256, 8, 8]               0\n",
            "        MaxPool2d-21            [-1, 256, 4, 4]               0\n",
            "           Conv2d-22            [-1, 512, 4, 4]       1,180,160\n",
            "      BatchNorm2d-23            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-24            [-1, 512, 4, 4]               0\n",
            "           Conv2d-25            [-1, 512, 4, 4]       2,359,808\n",
            "      BatchNorm2d-26            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-27            [-1, 512, 4, 4]               0\n",
            "        MaxPool2d-28            [-1, 512, 2, 2]               0\n",
            "           Linear-29                   [-1, 10]          20,490\n",
            "================================================================\n",
            "Total params: 4,709,706\n",
            "Trainable params: 4,709,706\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 5.86\n",
            "Params size (MB): 17.97\n",
            "Estimated Total Size (MB): 23.84\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "summary(Vgg().cuda(), image_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x49Lg1F3YNuS"
      },
      "outputs": [],
      "source": [
        "# 참조: https://deep-learning-study.tistory.com/534 \n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "\n",
        "        # BatchNorm에 bias가 포함되어 있으므로, conv2d는 bias=False로 설정합니다.\n",
        "        self.residual_function = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(out_channels, out_channels * BasicBlock.expansion, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels * BasicBlock.expansion),\n",
        "        )\n",
        "\n",
        "        # identity mapping, input과 output의 feature map size, filter 수가 동일한 경우 사용.\n",
        "        self.shortcut = nn.Sequential()\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # projection mapping using 1x1conv\n",
        "        if stride != 1 or in_channels != BasicBlock.expansion * out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels * BasicBlock.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.residual_function(x) + self.shortcut(x)\n",
        "        x = self.relu(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMmpm-Z-YNuU"
      },
      "outputs": [],
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_block, num_classes=10, init_weights=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.in_channels=64\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        )\n",
        "\n",
        "        self.conv2_x = self._make_layer(block, 64, num_block[0], 1)\n",
        "        self.conv3_x = self._make_layer(block, 128, num_block[1], 2)\n",
        "        self.conv4_x = self._make_layer(block, 256, num_block[2], 2)\n",
        "        # self.conv5_x = self._make_layer(block, 512, num_block[3], 2)\n",
        "\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.fc = nn.Linear(256 * block.expansion, num_classes)\n",
        "\n",
        "        # weights inittialization\n",
        "        if init_weights:\n",
        "            self._initialize_weights()\n",
        "\n",
        "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_channels, out_channels, stride))\n",
        "            self.in_channels = out_channels * block.expansion\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self,x):\n",
        "        output = self.conv1(x)\n",
        "        output = self.conv2_x(output)\n",
        "        x = self.conv3_x(output)\n",
        "        x = self.conv4_x(x)\n",
        "        # x = self.conv5_x(x)\n",
        "        x = self.avg_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    # define weight initialization function\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrfryY-pYNuV",
        "outputId": "b59ac8a5-c1b0-481a-f32f-194156950c00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
            "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
            "              ReLU-3           [-1, 64, 16, 16]               0\n",
            "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
            "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
            "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
            "              ReLU-7             [-1, 64, 8, 8]               0\n",
            "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
            "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
            "             ReLU-10             [-1, 64, 8, 8]               0\n",
            "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
            "           Conv2d-12            [-1, 128, 4, 4]          73,728\n",
            "      BatchNorm2d-13            [-1, 128, 4, 4]             256\n",
            "             ReLU-14            [-1, 128, 4, 4]               0\n",
            "           Conv2d-15            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-16            [-1, 128, 4, 4]             256\n",
            "           Conv2d-17            [-1, 128, 4, 4]           8,192\n",
            "      BatchNorm2d-18            [-1, 128, 4, 4]             256\n",
            "             ReLU-19            [-1, 128, 4, 4]               0\n",
            "       BasicBlock-20            [-1, 128, 4, 4]               0\n",
            "           Conv2d-21            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-22            [-1, 128, 4, 4]             256\n",
            "             ReLU-23            [-1, 128, 4, 4]               0\n",
            "           Conv2d-24            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
            "             ReLU-26            [-1, 128, 4, 4]               0\n",
            "       BasicBlock-27            [-1, 128, 4, 4]               0\n",
            "           Conv2d-28            [-1, 256, 2, 2]         294,912\n",
            "      BatchNorm2d-29            [-1, 256, 2, 2]             512\n",
            "             ReLU-30            [-1, 256, 2, 2]               0\n",
            "           Conv2d-31            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-32            [-1, 256, 2, 2]             512\n",
            "           Conv2d-33            [-1, 256, 2, 2]          32,768\n",
            "      BatchNorm2d-34            [-1, 256, 2, 2]             512\n",
            "             ReLU-35            [-1, 256, 2, 2]               0\n",
            "       BasicBlock-36            [-1, 256, 2, 2]               0\n",
            "AdaptiveAvgPool2d-37            [-1, 256, 1, 1]               0\n",
            "           Linear-38                   [-1, 10]           2,570\n",
            "================================================================\n",
            "Total params: 1,530,698\n",
            "Trainable params: 1,530,698\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 0.95\n",
            "Params size (MB): 5.84\n",
            "Estimated Total Size (MB): 6.80\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "model = ResNet(BasicBlock, [1,2,1]).cuda()\n",
        "summary(model, image_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5E54LUpYNuW"
      },
      "outputs": [],
      "source": [
        "class OneShortcut(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.features_1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.GELU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.GELU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.GELU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.features_2 = nn.Sequential(\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(512, 256, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            \n",
        "        )\n",
        "        self.shortcut = nn.Sequential(\n",
        "            # nn.Conv2d(128, 256, kernel_size=1,stride=2, bias=False),\n",
        "            # nn.BatchNorm2d(256)\n",
        "        )\n",
        "\n",
        "        self.merge = nn.Sequential(\n",
        "            nn.GELU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.features_3 = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=3 , padding=1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(256, 128, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.GELU(),\n",
        "            nn.AdaptiveAvgPool2d((1,1)),\n",
        "        )\n",
        "        self.classifier = nn.Linear(128, num_classes)\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features_1(x)\n",
        "        x = self.features_2(x) + self.shortcut(x)\n",
        "        x = self.merge(x)\n",
        "        x = self.features_3(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "    \n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pOnLEl8YNuY",
        "outputId": "6a173bce-fdef-436f-a408-ef3a0cb19e1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 32, 32]           1,728\n",
            "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
            "              GELU-3           [-1, 64, 32, 32]               0\n",
            "            Conv2d-4           [-1, 64, 32, 32]          36,864\n",
            "       BatchNorm2d-5           [-1, 64, 32, 32]             128\n",
            "              GELU-6           [-1, 64, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 64, 16, 16]               0\n",
            "            Conv2d-8          [-1, 128, 16, 16]          73,728\n",
            "       BatchNorm2d-9          [-1, 128, 16, 16]             256\n",
            "             GELU-10          [-1, 128, 16, 16]               0\n",
            "           Conv2d-11          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-12          [-1, 128, 16, 16]             256\n",
            "             GELU-13          [-1, 128, 16, 16]               0\n",
            "        MaxPool2d-14            [-1, 128, 8, 8]               0\n",
            "           Conv2d-15            [-1, 256, 8, 8]         294,912\n",
            "      BatchNorm2d-16            [-1, 256, 8, 8]             512\n",
            "             GELU-17            [-1, 256, 8, 8]               0\n",
            "           Conv2d-18            [-1, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-19            [-1, 256, 8, 8]             512\n",
            "             GELU-20            [-1, 256, 8, 8]               0\n",
            "        MaxPool2d-21            [-1, 256, 4, 4]               0\n",
            "           Conv2d-22            [-1, 512, 4, 4]       1,179,648\n",
            "      BatchNorm2d-23            [-1, 512, 4, 4]           1,024\n",
            "             GELU-24            [-1, 512, 4, 4]               0\n",
            "           Conv2d-25            [-1, 256, 4, 4]       1,179,648\n",
            "      BatchNorm2d-26            [-1, 256, 4, 4]             512\n",
            "             GELU-27            [-1, 256, 4, 4]               0\n",
            "        MaxPool2d-28            [-1, 256, 2, 2]               0\n",
            "           Conv2d-29            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-30            [-1, 256, 2, 2]             512\n",
            "             GELU-31            [-1, 256, 2, 2]               0\n",
            "           Conv2d-32            [-1, 128, 2, 2]         294,912\n",
            "      BatchNorm2d-33            [-1, 128, 2, 2]             256\n",
            "             GELU-34            [-1, 128, 2, 2]               0\n",
            "AdaptiveAvgPool2d-35            [-1, 128, 1, 1]               0\n",
            "           Linear-36                   [-1, 10]           1,290\n",
            "================================================================\n",
            "Total params: 4,393,930\n",
            "Trainable params: 4,393,930\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 5.79\n",
            "Params size (MB): 16.76\n",
            "Estimated Total Size (MB): 22.57\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "summary(OneShortcut().cuda(), image_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEddwQT0YNuZ"
      },
      "outputs": [],
      "source": [
        "def train(epoch, writer:SummaryWriter = None):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        if torch.cuda.is_available():\n",
        "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
        "        else:\n",
        "            data, target = Variable(data), Variable(target)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        # torch.max() : (maximum value, index of maximum value) return. \n",
        "        # 1 :  row마다 max계산 (즉, row는 10개의 class를 의미) \n",
        "        # 0 : column마다 max 계산 \n",
        "        total += target.size(0)\n",
        "        correct += predicted.eq(target.data).cpu().sum()\n",
        "        if batch_idx % 10 == 0:        \n",
        "            print('Epoch: {} | Batch_idx: {} |  Loss: ({:.4f}) | Acc: ({:.2f}%) ({}/{})'\n",
        "                  .format(epoch, batch_idx, train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "            if writer:\n",
        "                writer.add_scalar('Loss/train', train_loss/(batch_idx+1), epoch*40 + batch_idx//10)\n",
        "                writer.add_scalar('Accuracy/train', 100.*correct/total, epoch*40 + batch_idx//10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJuaf8mKYNua"
      },
      "outputs": [],
      "source": [
        "def test(epoch, writer:SummaryWriter = None):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (data, target) in enumerate(test_loader):\n",
        "        if torch.cuda.is_available():\n",
        "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
        "        else:\n",
        "            data, target = Variable(data), Variable(target)\n",
        "\n",
        "        outputs = model(data)\n",
        "        loss = criterion(outputs, target)\n",
        "\n",
        "        test_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += target.size(0)\n",
        "        correct += predicted.eq(target.data).cpu().sum()\n",
        "    print('# TEST : Loss: ({:.4f}) | Acc: ({:.2f}%) ({}/{})'\n",
        "      .format(test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "    if writer:\n",
        "        writer.add_scalar('Loss/test', test_loss/(batch_idx+1), epoch)\n",
        "        writer.add_scalar('Accuracy/test', 100.*correct/total, epoch)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WD3znACsYNud"
      },
      "source": [
        "### train & test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kLHaW9pYNue",
        "outputId": "6ccf9da0-f26f-4580-c202-caa420c4cdfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "USE 1 GPUs!\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 0.01\n",
        "\n",
        "# model = Vgg()\n",
        "# model = ResNet(BasicBlock, [1,2,1])\n",
        "model = OneShortcut()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), weight_decay=1e-2)\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.2).cuda()\n",
        "\n",
        "if torch.cuda.device_count() > 0:\n",
        "    print(\"USE\", torch.cuda.device_count(), \"GPUs!\")\n",
        "    model = nn.DataParallel(model)\n",
        "    cudnn.benchmark = True\n",
        "else:\n",
        "    print(\"USE ONLY CPU!\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3jPkbxwYNug"
      },
      "outputs": [],
      "source": [
        "def run(writer=None):\n",
        "    for epoch in range(0, 50):\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['learning_rate'] = learning_rate / 2**(epoch/5)\n",
        "\n",
        "        train(epoch, writer)\n",
        "        test(epoch, writer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAYJmy6PYNuj",
        "outputId": "d5b35938-d017-4f91-c17e-e8629b8e5277"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0 | Batch_idx: 0 |  Loss: (2.3075) | Acc: (7.81%) (10/128)\n",
            "Epoch: 0 | Batch_idx: 10 |  Loss: (2.2523) | Acc: (14.77%) (208/1408)\n",
            "Epoch: 0 | Batch_idx: 20 |  Loss: (2.2061) | Acc: (17.60%) (473/2688)\n",
            "Epoch: 0 | Batch_idx: 30 |  Loss: (2.1756) | Acc: (19.05%) (756/3968)\n",
            "Epoch: 0 | Batch_idx: 40 |  Loss: (2.1568) | Acc: (20.12%) (1056/5248)\n",
            "Epoch: 0 | Batch_idx: 50 |  Loss: (2.1435) | Acc: (21.12%) (1379/6528)\n",
            "Epoch: 0 | Batch_idx: 60 |  Loss: (2.1284) | Acc: (21.61%) (1687/7808)\n",
            "Epoch: 0 | Batch_idx: 70 |  Loss: (2.1169) | Acc: (22.55%) (2049/9088)\n",
            "Epoch: 0 | Batch_idx: 80 |  Loss: (2.1052) | Acc: (23.45%) (2431/10368)\n",
            "Epoch: 0 | Batch_idx: 90 |  Loss: (2.0956) | Acc: (23.94%) (2788/11648)\n",
            "Epoch: 0 | Batch_idx: 100 |  Loss: (2.0862) | Acc: (24.83%) (3210/12928)\n",
            "Epoch: 0 | Batch_idx: 110 |  Loss: (2.0732) | Acc: (25.72%) (3654/14208)\n",
            "Epoch: 0 | Batch_idx: 120 |  Loss: (2.0590) | Acc: (26.60%) (4120/15488)\n",
            "Epoch: 0 | Batch_idx: 130 |  Loss: (2.0504) | Acc: (27.11%) (4546/16768)\n",
            "Epoch: 0 | Batch_idx: 140 |  Loss: (2.0413) | Acc: (27.75%) (5009/18048)\n",
            "Epoch: 0 | Batch_idx: 150 |  Loss: (2.0326) | Acc: (28.36%) (5482/19328)\n",
            "Epoch: 0 | Batch_idx: 160 |  Loss: (2.0248) | Acc: (28.92%) (5960/20608)\n",
            "Epoch: 0 | Batch_idx: 170 |  Loss: (2.0170) | Acc: (29.53%) (6464/21888)\n",
            "Epoch: 0 | Batch_idx: 180 |  Loss: (2.0093) | Acc: (30.11%) (6977/23168)\n",
            "Epoch: 0 | Batch_idx: 190 |  Loss: (1.9999) | Acc: (30.82%) (7535/24448)\n",
            "Epoch: 0 | Batch_idx: 200 |  Loss: (1.9894) | Acc: (31.48%) (8098/25728)\n",
            "Epoch: 0 | Batch_idx: 210 |  Loss: (1.9820) | Acc: (31.96%) (8631/27008)\n",
            "Epoch: 0 | Batch_idx: 220 |  Loss: (1.9740) | Acc: (32.56%) (9211/28288)\n",
            "Epoch: 0 | Batch_idx: 230 |  Loss: (1.9668) | Acc: (33.06%) (9775/29568)\n",
            "Epoch: 0 | Batch_idx: 240 |  Loss: (1.9588) | Acc: (33.63%) (10375/30848)\n",
            "Epoch: 0 | Batch_idx: 250 |  Loss: (1.9500) | Acc: (34.25%) (11004/32128)\n",
            "Epoch: 0 | Batch_idx: 260 |  Loss: (1.9424) | Acc: (34.80%) (11627/33408)\n",
            "Epoch: 0 | Batch_idx: 270 |  Loss: (1.9356) | Acc: (35.30%) (12244/34688)\n",
            "Epoch: 0 | Batch_idx: 280 |  Loss: (1.9287) | Acc: (35.80%) (12878/35968)\n",
            "Epoch: 0 | Batch_idx: 290 |  Loss: (1.9220) | Acc: (36.31%) (13523/37248)\n",
            "Epoch: 0 | Batch_idx: 300 |  Loss: (1.9142) | Acc: (36.85%) (14198/38528)\n",
            "Epoch: 0 | Batch_idx: 310 |  Loss: (1.9078) | Acc: (37.30%) (14850/39808)\n",
            "Epoch: 0 | Batch_idx: 320 |  Loss: (1.9015) | Acc: (37.76%) (15514/41088)\n",
            "Epoch: 0 | Batch_idx: 330 |  Loss: (1.8956) | Acc: (38.14%) (16159/42368)\n",
            "Epoch: 0 | Batch_idx: 340 |  Loss: (1.8897) | Acc: (38.61%) (16851/43648)\n",
            "Epoch: 0 | Batch_idx: 350 |  Loss: (1.8833) | Acc: (39.08%) (17557/44928)\n",
            "Epoch: 0 | Batch_idx: 360 |  Loss: (1.8768) | Acc: (39.51%) (18259/46208)\n",
            "Epoch: 0 | Batch_idx: 370 |  Loss: (1.8713) | Acc: (39.86%) (18928/47488)\n",
            "Epoch: 0 | Batch_idx: 380 |  Loss: (1.8654) | Acc: (40.26%) (19634/48768)\n",
            "Epoch: 0 | Batch_idx: 390 |  Loss: (1.8597) | Acc: (40.66%) (20328/50000)\n",
            "# TEST : Loss: (1.7127) | Acc: (52.12%) (5212/10000)\n",
            "Epoch: 1 | Batch_idx: 0 |  Loss: (1.5466) | Acc: (59.38%) (76/128)\n",
            "Epoch: 1 | Batch_idx: 10 |  Loss: (1.6556) | Acc: (54.62%) (769/1408)\n",
            "Epoch: 1 | Batch_idx: 20 |  Loss: (1.6282) | Acc: (56.51%) (1519/2688)\n",
            "Epoch: 1 | Batch_idx: 30 |  Loss: (1.6184) | Acc: (57.21%) (2270/3968)\n",
            "Epoch: 1 | Batch_idx: 40 |  Loss: (1.6123) | Acc: (57.28%) (3006/5248)\n",
            "Epoch: 1 | Batch_idx: 50 |  Loss: (1.6020) | Acc: (57.98%) (3785/6528)\n",
            "Epoch: 1 | Batch_idx: 60 |  Loss: (1.5971) | Acc: (58.22%) (4546/7808)\n",
            "Epoch: 1 | Batch_idx: 70 |  Loss: (1.5952) | Acc: (58.44%) (5311/9088)\n",
            "Epoch: 1 | Batch_idx: 80 |  Loss: (1.5947) | Acc: (58.65%) (6081/10368)\n",
            "Epoch: 1 | Batch_idx: 90 |  Loss: (1.5935) | Acc: (58.71%) (6839/11648)\n",
            "Epoch: 1 | Batch_idx: 100 |  Loss: (1.5936) | Acc: (58.84%) (7607/12928)\n",
            "Epoch: 1 | Batch_idx: 110 |  Loss: (1.5942) | Acc: (58.79%) (8353/14208)\n",
            "Epoch: 1 | Batch_idx: 120 |  Loss: (1.5927) | Acc: (58.97%) (9134/15488)\n",
            "Epoch: 1 | Batch_idx: 130 |  Loss: (1.5897) | Acc: (59.12%) (9913/16768)\n",
            "Epoch: 1 | Batch_idx: 140 |  Loss: (1.5886) | Acc: (59.28%) (10698/18048)\n",
            "Epoch: 1 | Batch_idx: 150 |  Loss: (1.5868) | Acc: (59.38%) (11476/19328)\n",
            "Epoch: 1 | Batch_idx: 160 |  Loss: (1.5819) | Acc: (59.71%) (12305/20608)\n",
            "Epoch: 1 | Batch_idx: 170 |  Loss: (1.5792) | Acc: (59.84%) (13097/21888)\n",
            "Epoch: 1 | Batch_idx: 180 |  Loss: (1.5754) | Acc: (60.03%) (13907/23168)\n",
            "Epoch: 1 | Batch_idx: 190 |  Loss: (1.5738) | Acc: (60.14%) (14703/24448)\n",
            "Epoch: 1 | Batch_idx: 200 |  Loss: (1.5722) | Acc: (60.30%) (15513/25728)\n",
            "Epoch: 1 | Batch_idx: 210 |  Loss: (1.5705) | Acc: (60.37%) (16306/27008)\n",
            "Epoch: 1 | Batch_idx: 220 |  Loss: (1.5686) | Acc: (60.43%) (17094/28288)\n",
            "Epoch: 1 | Batch_idx: 230 |  Loss: (1.5670) | Acc: (60.55%) (17903/29568)\n",
            "Epoch: 1 | Batch_idx: 240 |  Loss: (1.5649) | Acc: (60.64%) (18706/30848)\n",
            "Epoch: 1 | Batch_idx: 250 |  Loss: (1.5621) | Acc: (60.80%) (19534/32128)\n",
            "Epoch: 1 | Batch_idx: 260 |  Loss: (1.5591) | Acc: (60.93%) (20357/33408)\n",
            "Epoch: 1 | Batch_idx: 270 |  Loss: (1.5574) | Acc: (61.06%) (21182/34688)\n",
            "Epoch: 1 | Batch_idx: 280 |  Loss: (1.5554) | Acc: (61.18%) (22005/35968)\n",
            "Epoch: 1 | Batch_idx: 290 |  Loss: (1.5532) | Acc: (61.33%) (22844/37248)\n",
            "Epoch: 1 | Batch_idx: 300 |  Loss: (1.5506) | Acc: (61.51%) (23697/38528)\n",
            "Epoch: 1 | Batch_idx: 310 |  Loss: (1.5485) | Acc: (61.61%) (24527/39808)\n",
            "Epoch: 1 | Batch_idx: 320 |  Loss: (1.5458) | Acc: (61.78%) (25385/41088)\n",
            "Epoch: 1 | Batch_idx: 330 |  Loss: (1.5434) | Acc: (61.95%) (26247/42368)\n",
            "Epoch: 1 | Batch_idx: 340 |  Loss: (1.5403) | Acc: (62.13%) (27120/43648)\n",
            "Epoch: 1 | Batch_idx: 350 |  Loss: (1.5378) | Acc: (62.30%) (27991/44928)\n",
            "Epoch: 1 | Batch_idx: 360 |  Loss: (1.5354) | Acc: (62.43%) (28847/46208)\n",
            "Epoch: 1 | Batch_idx: 370 |  Loss: (1.5328) | Acc: (62.59%) (29724/47488)\n",
            "Epoch: 1 | Batch_idx: 380 |  Loss: (1.5299) | Acc: (62.77%) (30613/48768)\n",
            "Epoch: 1 | Batch_idx: 390 |  Loss: (1.5272) | Acc: (62.88%) (31438/50000)\n",
            "# TEST : Loss: (1.4997) | Acc: (64.21%) (6421/10000)\n",
            "Epoch: 2 | Batch_idx: 0 |  Loss: (1.3889) | Acc: (74.22%) (95/128)\n",
            "Epoch: 2 | Batch_idx: 10 |  Loss: (1.4067) | Acc: (69.82%) (983/1408)\n",
            "Epoch: 2 | Batch_idx: 20 |  Loss: (1.4129) | Acc: (69.75%) (1875/2688)\n",
            "Epoch: 2 | Batch_idx: 30 |  Loss: (1.4046) | Acc: (70.16%) (2784/3968)\n",
            "Epoch: 2 | Batch_idx: 40 |  Loss: (1.4034) | Acc: (70.50%) (3700/5248)\n",
            "Epoch: 2 | Batch_idx: 50 |  Loss: (1.4105) | Acc: (69.90%) (4563/6528)\n",
            "Epoch: 2 | Batch_idx: 60 |  Loss: (1.4078) | Acc: (70.06%) (5470/7808)\n",
            "Epoch: 2 | Batch_idx: 70 |  Loss: (1.4047) | Acc: (70.26%) (6385/9088)\n",
            "Epoch: 2 | Batch_idx: 80 |  Loss: (1.4051) | Acc: (70.11%) (7269/10368)\n",
            "Epoch: 2 | Batch_idx: 90 |  Loss: (1.4049) | Acc: (70.21%) (8178/11648)\n",
            "Epoch: 2 | Batch_idx: 100 |  Loss: (1.4061) | Acc: (70.28%) (9086/12928)\n",
            "Epoch: 2 | Batch_idx: 110 |  Loss: (1.4047) | Acc: (70.37%) (9998/14208)\n",
            "Epoch: 2 | Batch_idx: 120 |  Loss: (1.4044) | Acc: (70.27%) (10884/15488)\n",
            "Epoch: 2 | Batch_idx: 130 |  Loss: (1.4060) | Acc: (70.22%) (11774/16768)\n",
            "Epoch: 2 | Batch_idx: 140 |  Loss: (1.4034) | Acc: (70.40%) (12706/18048)\n",
            "Epoch: 2 | Batch_idx: 150 |  Loss: (1.4033) | Acc: (70.37%) (13602/19328)\n",
            "Epoch: 2 | Batch_idx: 160 |  Loss: (1.4028) | Acc: (70.43%) (14514/20608)\n",
            "Epoch: 2 | Batch_idx: 170 |  Loss: (1.4009) | Acc: (70.60%) (15454/21888)\n",
            "Epoch: 2 | Batch_idx: 180 |  Loss: (1.3994) | Acc: (70.75%) (16391/23168)\n",
            "Epoch: 2 | Batch_idx: 190 |  Loss: (1.3968) | Acc: (70.93%) (17342/24448)\n",
            "Epoch: 2 | Batch_idx: 200 |  Loss: (1.3945) | Acc: (71.04%) (18278/25728)\n",
            "Epoch: 2 | Batch_idx: 210 |  Loss: (1.3930) | Acc: (71.08%) (19198/27008)\n",
            "Epoch: 2 | Batch_idx: 220 |  Loss: (1.3917) | Acc: (71.16%) (20129/28288)\n",
            "Epoch: 2 | Batch_idx: 230 |  Loss: (1.3903) | Acc: (71.26%) (21071/29568)\n",
            "Epoch: 2 | Batch_idx: 240 |  Loss: (1.3912) | Acc: (71.23%) (21973/30848)\n",
            "Epoch: 2 | Batch_idx: 250 |  Loss: (1.3902) | Acc: (71.31%) (22909/32128)\n",
            "Epoch: 2 | Batch_idx: 260 |  Loss: (1.3879) | Acc: (71.43%) (23864/33408)\n",
            "Epoch: 2 | Batch_idx: 270 |  Loss: (1.3871) | Acc: (71.42%) (24773/34688)\n",
            "Epoch: 2 | Batch_idx: 280 |  Loss: (1.3852) | Acc: (71.54%) (25731/35968)\n",
            "Epoch: 2 | Batch_idx: 290 |  Loss: (1.3838) | Acc: (71.60%) (26669/37248)\n",
            "Epoch: 2 | Batch_idx: 300 |  Loss: (1.3825) | Acc: (71.75%) (27645/38528)\n",
            "Epoch: 2 | Batch_idx: 310 |  Loss: (1.3823) | Acc: (71.76%) (28565/39808)\n",
            "Epoch: 2 | Batch_idx: 320 |  Loss: (1.3813) | Acc: (71.85%) (29523/41088)\n",
            "Epoch: 2 | Batch_idx: 330 |  Loss: (1.3800) | Acc: (71.94%) (30478/42368)\n",
            "Epoch: 2 | Batch_idx: 340 |  Loss: (1.3795) | Acc: (71.96%) (31407/43648)\n",
            "Epoch: 2 | Batch_idx: 350 |  Loss: (1.3784) | Acc: (72.02%) (32355/44928)\n",
            "Epoch: 2 | Batch_idx: 360 |  Loss: (1.3761) | Acc: (72.15%) (33339/46208)\n",
            "Epoch: 2 | Batch_idx: 370 |  Loss: (1.3754) | Acc: (72.20%) (34287/47488)\n",
            "Epoch: 2 | Batch_idx: 380 |  Loss: (1.3739) | Acc: (72.30%) (35259/48768)\n",
            "Epoch: 2 | Batch_idx: 390 |  Loss: (1.3734) | Acc: (72.33%) (36164/50000)\n",
            "# TEST : Loss: (1.3073) | Acc: (76.04%) (7604/10000)\n",
            "Epoch: 3 | Batch_idx: 0 |  Loss: (1.3652) | Acc: (71.09%) (91/128)\n",
            "Epoch: 3 | Batch_idx: 10 |  Loss: (1.3237) | Acc: (75.71%) (1066/1408)\n",
            "Epoch: 3 | Batch_idx: 20 |  Loss: (1.3236) | Acc: (74.93%) (2014/2688)\n",
            "Epoch: 3 | Batch_idx: 30 |  Loss: (1.3136) | Acc: (75.91%) (3012/3968)\n",
            "Epoch: 3 | Batch_idx: 40 |  Loss: (1.3140) | Acc: (75.84%) (3980/5248)\n",
            "Epoch: 3 | Batch_idx: 50 |  Loss: (1.3161) | Acc: (75.44%) (4925/6528)\n",
            "Epoch: 3 | Batch_idx: 60 |  Loss: (1.3159) | Acc: (75.55%) (5899/7808)\n",
            "Epoch: 3 | Batch_idx: 70 |  Loss: (1.3113) | Acc: (75.78%) (6887/9088)\n",
            "Epoch: 3 | Batch_idx: 80 |  Loss: (1.3087) | Acc: (75.95%) (7875/10368)\n",
            "Epoch: 3 | Batch_idx: 90 |  Loss: (1.3133) | Acc: (75.61%) (8807/11648)\n",
            "Epoch: 3 | Batch_idx: 100 |  Loss: (1.3120) | Acc: (75.70%) (9787/12928)\n",
            "Epoch: 3 | Batch_idx: 110 |  Loss: (1.3128) | Acc: (75.63%) (10746/14208)\n",
            "Epoch: 3 | Batch_idx: 120 |  Loss: (1.3112) | Acc: (75.77%) (11735/15488)\n",
            "Epoch: 3 | Batch_idx: 130 |  Loss: (1.3141) | Acc: (75.55%) (12668/16768)\n",
            "Epoch: 3 | Batch_idx: 140 |  Loss: (1.3131) | Acc: (75.63%) (13650/18048)\n",
            "Epoch: 3 | Batch_idx: 150 |  Loss: (1.3112) | Acc: (75.75%) (14640/19328)\n",
            "Epoch: 3 | Batch_idx: 160 |  Loss: (1.3105) | Acc: (75.82%) (15626/20608)\n",
            "Epoch: 3 | Batch_idx: 170 |  Loss: (1.3106) | Acc: (75.83%) (16598/21888)\n",
            "Epoch: 3 | Batch_idx: 180 |  Loss: (1.3112) | Acc: (75.82%) (17567/23168)\n",
            "Epoch: 3 | Batch_idx: 190 |  Loss: (1.3106) | Acc: (75.80%) (18532/24448)\n",
            "Epoch: 3 | Batch_idx: 200 |  Loss: (1.3093) | Acc: (75.87%) (19521/25728)\n",
            "Epoch: 3 | Batch_idx: 210 |  Loss: (1.3078) | Acc: (75.97%) (20518/27008)\n",
            "Epoch: 3 | Batch_idx: 220 |  Loss: (1.3080) | Acc: (75.99%) (21495/28288)\n",
            "Epoch: 3 | Batch_idx: 230 |  Loss: (1.3075) | Acc: (76.09%) (22497/29568)\n",
            "Epoch: 3 | Batch_idx: 240 |  Loss: (1.3068) | Acc: (76.12%) (23480/30848)\n",
            "Epoch: 3 | Batch_idx: 250 |  Loss: (1.3057) | Acc: (76.20%) (24481/32128)\n",
            "Epoch: 3 | Batch_idx: 260 |  Loss: (1.3054) | Acc: (76.19%) (25452/33408)\n",
            "Epoch: 3 | Batch_idx: 270 |  Loss: (1.3043) | Acc: (76.21%) (26436/34688)\n",
            "Epoch: 3 | Batch_idx: 280 |  Loss: (1.3038) | Acc: (76.26%) (27429/35968)\n",
            "Epoch: 3 | Batch_idx: 290 |  Loss: (1.3039) | Acc: (76.21%) (28386/37248)\n",
            "Epoch: 3 | Batch_idx: 300 |  Loss: (1.3043) | Acc: (76.18%) (29351/38528)\n",
            "Epoch: 3 | Batch_idx: 310 |  Loss: (1.3026) | Acc: (76.29%) (30371/39808)\n",
            "Epoch: 3 | Batch_idx: 320 |  Loss: (1.3029) | Acc: (76.29%) (31344/41088)\n",
            "Epoch: 3 | Batch_idx: 330 |  Loss: (1.3023) | Acc: (76.33%) (32338/42368)\n",
            "Epoch: 3 | Batch_idx: 340 |  Loss: (1.3008) | Acc: (76.40%) (33347/43648)\n",
            "Epoch: 3 | Batch_idx: 350 |  Loss: (1.3000) | Acc: (76.44%) (34341/44928)\n",
            "Epoch: 3 | Batch_idx: 360 |  Loss: (1.2993) | Acc: (76.47%) (35335/46208)\n",
            "Epoch: 3 | Batch_idx: 370 |  Loss: (1.2983) | Acc: (76.54%) (36349/47488)\n",
            "Epoch: 3 | Batch_idx: 380 |  Loss: (1.2982) | Acc: (76.56%) (37336/48768)\n",
            "Epoch: 3 | Batch_idx: 390 |  Loss: (1.2973) | Acc: (76.61%) (38304/50000)\n",
            "# TEST : Loss: (1.2803) | Acc: (77.88%) (7788/10000)\n",
            "Epoch: 4 | Batch_idx: 0 |  Loss: (1.3104) | Acc: (73.44%) (94/128)\n",
            "Epoch: 4 | Batch_idx: 10 |  Loss: (1.2645) | Acc: (78.27%) (1102/1408)\n",
            "Epoch: 4 | Batch_idx: 20 |  Loss: (1.2587) | Acc: (78.72%) (2116/2688)\n",
            "Epoch: 4 | Batch_idx: 30 |  Loss: (1.2538) | Acc: (79.01%) (3135/3968)\n",
            "Epoch: 4 | Batch_idx: 40 |  Loss: (1.2518) | Acc: (79.12%) (4152/5248)\n",
            "Epoch: 4 | Batch_idx: 50 |  Loss: (1.2558) | Acc: (78.88%) (5149/6528)\n",
            "Epoch: 4 | Batch_idx: 60 |  Loss: (1.2525) | Acc: (79.10%) (6176/7808)\n",
            "Epoch: 4 | Batch_idx: 70 |  Loss: (1.2498) | Acc: (79.14%) (7192/9088)\n",
            "Epoch: 4 | Batch_idx: 80 |  Loss: (1.2524) | Acc: (78.95%) (8186/10368)\n",
            "Epoch: 4 | Batch_idx: 90 |  Loss: (1.2558) | Acc: (78.80%) (9179/11648)\n",
            "Epoch: 4 | Batch_idx: 100 |  Loss: (1.2540) | Acc: (78.86%) (10195/12928)\n",
            "Epoch: 4 | Batch_idx: 110 |  Loss: (1.2540) | Acc: (78.90%) (11210/14208)\n",
            "Epoch: 4 | Batch_idx: 120 |  Loss: (1.2523) | Acc: (79.01%) (12237/15488)\n",
            "Epoch: 4 | Batch_idx: 130 |  Loss: (1.2518) | Acc: (78.99%) (13245/16768)\n",
            "Epoch: 4 | Batch_idx: 140 |  Loss: (1.2518) | Acc: (78.96%) (14251/18048)\n",
            "Epoch: 4 | Batch_idx: 150 |  Loss: (1.2531) | Acc: (78.93%) (15255/19328)\n",
            "Epoch: 4 | Batch_idx: 160 |  Loss: (1.2547) | Acc: (78.81%) (16241/20608)\n",
            "Epoch: 4 | Batch_idx: 170 |  Loss: (1.2549) | Acc: (78.80%) (17248/21888)\n",
            "Epoch: 4 | Batch_idx: 180 |  Loss: (1.2556) | Acc: (78.82%) (18262/23168)\n",
            "Epoch: 4 | Batch_idx: 190 |  Loss: (1.2540) | Acc: (78.91%) (19293/24448)\n",
            "Epoch: 4 | Batch_idx: 200 |  Loss: (1.2535) | Acc: (78.97%) (20317/25728)\n",
            "Epoch: 4 | Batch_idx: 210 |  Loss: (1.2528) | Acc: (79.01%) (21340/27008)\n",
            "Epoch: 4 | Batch_idx: 220 |  Loss: (1.2529) | Acc: (79.04%) (22360/28288)\n",
            "Epoch: 4 | Batch_idx: 230 |  Loss: (1.2530) | Acc: (79.04%) (23371/29568)\n",
            "Epoch: 4 | Batch_idx: 240 |  Loss: (1.2526) | Acc: (79.09%) (24397/30848)\n",
            "Epoch: 4 | Batch_idx: 250 |  Loss: (1.2528) | Acc: (79.07%) (25405/32128)\n",
            "Epoch: 4 | Batch_idx: 260 |  Loss: (1.2527) | Acc: (79.14%) (26440/33408)\n",
            "Epoch: 4 | Batch_idx: 270 |  Loss: (1.2525) | Acc: (79.15%) (27457/34688)\n",
            "Epoch: 4 | Batch_idx: 280 |  Loss: (1.2523) | Acc: (79.20%) (28488/35968)\n",
            "Epoch: 4 | Batch_idx: 290 |  Loss: (1.2518) | Acc: (79.24%) (29517/37248)\n",
            "Epoch: 4 | Batch_idx: 300 |  Loss: (1.2516) | Acc: (79.26%) (30536/38528)\n",
            "Epoch: 4 | Batch_idx: 310 |  Loss: (1.2517) | Acc: (79.24%) (31542/39808)\n",
            "Epoch: 4 | Batch_idx: 320 |  Loss: (1.2520) | Acc: (79.22%) (32551/41088)\n",
            "Epoch: 4 | Batch_idx: 330 |  Loss: (1.2523) | Acc: (79.19%) (33552/42368)\n",
            "Epoch: 4 | Batch_idx: 340 |  Loss: (1.2516) | Acc: (79.24%) (34586/43648)\n",
            "Epoch: 4 | Batch_idx: 350 |  Loss: (1.2498) | Acc: (79.36%) (35654/44928)\n",
            "Epoch: 4 | Batch_idx: 360 |  Loss: (1.2500) | Acc: (79.35%) (36667/46208)\n",
            "Epoch: 4 | Batch_idx: 370 |  Loss: (1.2496) | Acc: (79.38%) (37694/47488)\n",
            "Epoch: 4 | Batch_idx: 380 |  Loss: (1.2498) | Acc: (79.36%) (38700/48768)\n",
            "Epoch: 4 | Batch_idx: 390 |  Loss: (1.2496) | Acc: (79.36%) (39678/50000)\n",
            "# TEST : Loss: (1.2168) | Acc: (81.21%) (8121/10000)\n",
            "Epoch: 5 | Batch_idx: 0 |  Loss: (1.2052) | Acc: (82.03%) (105/128)\n",
            "Epoch: 5 | Batch_idx: 10 |  Loss: (1.2129) | Acc: (82.10%) (1156/1408)\n",
            "Epoch: 5 | Batch_idx: 20 |  Loss: (1.2141) | Acc: (81.77%) (2198/2688)\n",
            "Epoch: 5 | Batch_idx: 30 |  Loss: (1.2100) | Acc: (81.98%) (3253/3968)\n",
            "Epoch: 5 | Batch_idx: 40 |  Loss: (1.2024) | Acc: (82.26%) (4317/5248)\n",
            "Epoch: 5 | Batch_idx: 50 |  Loss: (1.2070) | Acc: (81.91%) (5347/6528)\n",
            "Epoch: 5 | Batch_idx: 60 |  Loss: (1.2092) | Acc: (81.72%) (6381/7808)\n",
            "Epoch: 5 | Batch_idx: 70 |  Loss: (1.2078) | Acc: (81.86%) (7439/9088)\n",
            "Epoch: 5 | Batch_idx: 80 |  Loss: (1.2091) | Acc: (81.69%) (8470/10368)\n",
            "Epoch: 5 | Batch_idx: 90 |  Loss: (1.2098) | Acc: (81.69%) (9515/11648)\n",
            "Epoch: 5 | Batch_idx: 100 |  Loss: (1.2103) | Acc: (81.61%) (10550/12928)\n",
            "Epoch: 5 | Batch_idx: 110 |  Loss: (1.2107) | Acc: (81.59%) (11592/14208)\n",
            "Epoch: 5 | Batch_idx: 120 |  Loss: (1.2105) | Acc: (81.61%) (12640/15488)\n",
            "Epoch: 5 | Batch_idx: 130 |  Loss: (1.2088) | Acc: (81.63%) (13688/16768)\n",
            "Epoch: 5 | Batch_idx: 140 |  Loss: (1.2071) | Acc: (81.75%) (14755/18048)\n",
            "Epoch: 5 | Batch_idx: 150 |  Loss: (1.2082) | Acc: (81.65%) (15782/19328)\n",
            "Epoch: 5 | Batch_idx: 160 |  Loss: (1.2061) | Acc: (81.74%) (16845/20608)\n",
            "Epoch: 5 | Batch_idx: 170 |  Loss: (1.2069) | Acc: (81.67%) (17877/21888)\n",
            "Epoch: 5 | Batch_idx: 180 |  Loss: (1.2066) | Acc: (81.69%) (18927/23168)\n",
            "Epoch: 5 | Batch_idx: 190 |  Loss: (1.2072) | Acc: (81.68%) (19970/24448)\n",
            "Epoch: 5 | Batch_idx: 200 |  Loss: (1.2069) | Acc: (81.70%) (21021/25728)\n",
            "Epoch: 5 | Batch_idx: 210 |  Loss: (1.2076) | Acc: (81.62%) (22043/27008)\n",
            "Epoch: 5 | Batch_idx: 220 |  Loss: (1.2080) | Acc: (81.61%) (23086/28288)\n",
            "Epoch: 5 | Batch_idx: 230 |  Loss: (1.2086) | Acc: (81.60%) (24127/29568)\n",
            "Epoch: 5 | Batch_idx: 240 |  Loss: (1.2080) | Acc: (81.66%) (25191/30848)\n",
            "Epoch: 5 | Batch_idx: 250 |  Loss: (1.2076) | Acc: (81.66%) (26237/32128)\n",
            "Epoch: 5 | Batch_idx: 260 |  Loss: (1.2080) | Acc: (81.63%) (27272/33408)\n",
            "Epoch: 5 | Batch_idx: 270 |  Loss: (1.2086) | Acc: (81.60%) (28306/34688)\n",
            "Epoch: 5 | Batch_idx: 280 |  Loss: (1.2089) | Acc: (81.58%) (29344/35968)\n",
            "Epoch: 5 | Batch_idx: 290 |  Loss: (1.2091) | Acc: (81.56%) (30380/37248)\n",
            "Epoch: 5 | Batch_idx: 300 |  Loss: (1.2103) | Acc: (81.51%) (31405/38528)\n",
            "Epoch: 5 | Batch_idx: 310 |  Loss: (1.2097) | Acc: (81.52%) (32450/39808)\n",
            "Epoch: 5 | Batch_idx: 320 |  Loss: (1.2100) | Acc: (81.50%) (33486/41088)\n",
            "Epoch: 5 | Batch_idx: 330 |  Loss: (1.2104) | Acc: (81.47%) (34516/42368)\n",
            "Epoch: 5 | Batch_idx: 340 |  Loss: (1.2105) | Acc: (81.45%) (35553/43648)\n",
            "Epoch: 5 | Batch_idx: 350 |  Loss: (1.2099) | Acc: (81.48%) (36609/44928)\n",
            "Epoch: 5 | Batch_idx: 360 |  Loss: (1.2100) | Acc: (81.51%) (37663/46208)\n",
            "Epoch: 5 | Batch_idx: 370 |  Loss: (1.2099) | Acc: (81.50%) (38705/47488)\n",
            "Epoch: 5 | Batch_idx: 380 |  Loss: (1.2104) | Acc: (81.50%) (39746/48768)\n",
            "Epoch: 5 | Batch_idx: 390 |  Loss: (1.2097) | Acc: (81.56%) (40778/50000)\n",
            "# TEST : Loss: (1.2249) | Acc: (80.46%) (8046/10000)\n",
            "Epoch: 6 | Batch_idx: 0 |  Loss: (1.2459) | Acc: (80.47%) (103/128)\n",
            "Epoch: 6 | Batch_idx: 10 |  Loss: (1.1987) | Acc: (82.67%) (1164/1408)\n",
            "Epoch: 6 | Batch_idx: 20 |  Loss: (1.1876) | Acc: (83.07%) (2233/2688)\n",
            "Epoch: 6 | Batch_idx: 30 |  Loss: (1.1800) | Acc: (83.52%) (3314/3968)\n",
            "Epoch: 6 | Batch_idx: 40 |  Loss: (1.1821) | Acc: (83.31%) (4372/5248)\n",
            "Epoch: 6 | Batch_idx: 50 |  Loss: (1.1848) | Acc: (82.97%) (5416/6528)\n",
            "Epoch: 6 | Batch_idx: 60 |  Loss: (1.1837) | Acc: (82.95%) (6477/7808)\n",
            "Epoch: 6 | Batch_idx: 70 |  Loss: (1.1822) | Acc: (83.07%) (7549/9088)\n",
            "Epoch: 6 | Batch_idx: 80 |  Loss: (1.1807) | Acc: (83.17%) (8623/10368)\n",
            "Epoch: 6 | Batch_idx: 90 |  Loss: (1.1782) | Acc: (83.26%) (9698/11648)\n",
            "Epoch: 6 | Batch_idx: 100 |  Loss: (1.1782) | Acc: (83.27%) (10765/12928)\n",
            "Epoch: 6 | Batch_idx: 110 |  Loss: (1.1813) | Acc: (83.12%) (11809/14208)\n",
            "Epoch: 6 | Batch_idx: 120 |  Loss: (1.1836) | Acc: (83.00%) (12855/15488)\n",
            "Epoch: 6 | Batch_idx: 130 |  Loss: (1.1840) | Acc: (83.07%) (13930/16768)\n",
            "Epoch: 6 | Batch_idx: 140 |  Loss: (1.1843) | Acc: (83.08%) (14994/18048)\n",
            "Epoch: 6 | Batch_idx: 150 |  Loss: (1.1825) | Acc: (83.21%) (16082/19328)\n",
            "Epoch: 6 | Batch_idx: 160 |  Loss: (1.1802) | Acc: (83.25%) (17157/20608)\n",
            "Epoch: 6 | Batch_idx: 170 |  Loss: (1.1800) | Acc: (83.26%) (18225/21888)\n",
            "Epoch: 6 | Batch_idx: 180 |  Loss: (1.1802) | Acc: (83.25%) (19288/23168)\n",
            "Epoch: 6 | Batch_idx: 190 |  Loss: (1.1798) | Acc: (83.29%) (20362/24448)\n",
            "Epoch: 6 | Batch_idx: 200 |  Loss: (1.1790) | Acc: (83.30%) (21431/25728)\n",
            "Epoch: 6 | Batch_idx: 210 |  Loss: (1.1780) | Acc: (83.34%) (22509/27008)\n",
            "Epoch: 6 | Batch_idx: 220 |  Loss: (1.1784) | Acc: (83.31%) (23566/28288)\n",
            "Epoch: 6 | Batch_idx: 230 |  Loss: (1.1790) | Acc: (83.25%) (24616/29568)\n",
            "Epoch: 6 | Batch_idx: 240 |  Loss: (1.1785) | Acc: (83.30%) (25696/30848)\n",
            "Epoch: 6 | Batch_idx: 250 |  Loss: (1.1790) | Acc: (83.26%) (26750/32128)\n",
            "Epoch: 6 | Batch_idx: 260 |  Loss: (1.1787) | Acc: (83.25%) (27813/33408)\n",
            "Epoch: 6 | Batch_idx: 270 |  Loss: (1.1777) | Acc: (83.29%) (28891/34688)\n",
            "Epoch: 6 | Batch_idx: 280 |  Loss: (1.1786) | Acc: (83.24%) (29939/35968)\n",
            "Epoch: 6 | Batch_idx: 290 |  Loss: (1.1790) | Acc: (83.21%) (30995/37248)\n",
            "Epoch: 6 | Batch_idx: 300 |  Loss: (1.1796) | Acc: (83.18%) (32048/38528)\n",
            "Epoch: 6 | Batch_idx: 310 |  Loss: (1.1798) | Acc: (83.18%) (33113/39808)\n",
            "Epoch: 6 | Batch_idx: 320 |  Loss: (1.1793) | Acc: (83.19%) (34182/41088)\n",
            "Epoch: 6 | Batch_idx: 330 |  Loss: (1.1792) | Acc: (83.20%) (35251/42368)\n",
            "Epoch: 6 | Batch_idx: 340 |  Loss: (1.1791) | Acc: (83.17%) (36303/43648)\n",
            "Epoch: 6 | Batch_idx: 350 |  Loss: (1.1792) | Acc: (83.14%) (37353/44928)\n",
            "Epoch: 6 | Batch_idx: 360 |  Loss: (1.1793) | Acc: (83.14%) (38419/46208)\n",
            "Epoch: 6 | Batch_idx: 370 |  Loss: (1.1794) | Acc: (83.14%) (39483/47488)\n",
            "Epoch: 6 | Batch_idx: 380 |  Loss: (1.1794) | Acc: (83.15%) (40551/48768)\n",
            "Epoch: 6 | Batch_idx: 390 |  Loss: (1.1796) | Acc: (83.14%) (41569/50000)\n",
            "# TEST : Loss: (1.1581) | Acc: (84.72%) (8472/10000)\n",
            "Epoch: 7 | Batch_idx: 0 |  Loss: (1.2223) | Acc: (80.47%) (103/128)\n",
            "Epoch: 7 | Batch_idx: 10 |  Loss: (1.1777) | Acc: (83.59%) (1177/1408)\n",
            "Epoch: 7 | Batch_idx: 20 |  Loss: (1.1686) | Acc: (84.08%) (2260/2688)\n",
            "Epoch: 7 | Batch_idx: 30 |  Loss: (1.1662) | Acc: (84.20%) (3341/3968)\n",
            "Epoch: 7 | Batch_idx: 40 |  Loss: (1.1677) | Acc: (84.34%) (4426/5248)\n",
            "Epoch: 7 | Batch_idx: 50 |  Loss: (1.1691) | Acc: (83.98%) (5482/6528)\n",
            "Epoch: 7 | Batch_idx: 60 |  Loss: (1.1687) | Acc: (83.99%) (6558/7808)\n",
            "Epoch: 7 | Batch_idx: 70 |  Loss: (1.1646) | Acc: (84.23%) (7655/9088)\n",
            "Epoch: 7 | Batch_idx: 80 |  Loss: (1.1637) | Acc: (84.40%) (8751/10368)\n",
            "Epoch: 7 | Batch_idx: 90 |  Loss: (1.1655) | Acc: (84.25%) (9814/11648)\n",
            "Epoch: 7 | Batch_idx: 100 |  Loss: (1.1653) | Acc: (84.30%) (10898/12928)\n",
            "Epoch: 7 | Batch_idx: 110 |  Loss: (1.1647) | Acc: (84.28%) (11974/14208)\n",
            "Epoch: 7 | Batch_idx: 120 |  Loss: (1.1650) | Acc: (84.28%) (13053/15488)\n",
            "Epoch: 7 | Batch_idx: 130 |  Loss: (1.1645) | Acc: (84.28%) (14132/16768)\n",
            "Epoch: 7 | Batch_idx: 140 |  Loss: (1.1633) | Acc: (84.33%) (15220/18048)\n",
            "Epoch: 7 | Batch_idx: 150 |  Loss: (1.1639) | Acc: (84.24%) (16281/19328)\n",
            "Epoch: 7 | Batch_idx: 160 |  Loss: (1.1635) | Acc: (84.27%) (17367/20608)\n",
            "Epoch: 7 | Batch_idx: 170 |  Loss: (1.1615) | Acc: (84.38%) (18470/21888)\n",
            "Epoch: 7 | Batch_idx: 180 |  Loss: (1.1614) | Acc: (84.38%) (19550/23168)\n",
            "Epoch: 7 | Batch_idx: 190 |  Loss: (1.1612) | Acc: (84.40%) (20634/24448)\n",
            "Epoch: 7 | Batch_idx: 200 |  Loss: (1.1612) | Acc: (84.42%) (21720/25728)\n",
            "Epoch: 7 | Batch_idx: 210 |  Loss: (1.1614) | Acc: (84.39%) (22793/27008)\n",
            "Epoch: 7 | Batch_idx: 220 |  Loss: (1.1601) | Acc: (84.45%) (23888/28288)\n",
            "Epoch: 7 | Batch_idx: 230 |  Loss: (1.1601) | Acc: (84.45%) (24970/29568)\n",
            "Epoch: 7 | Batch_idx: 240 |  Loss: (1.1586) | Acc: (84.55%) (26083/30848)\n",
            "Epoch: 7 | Batch_idx: 250 |  Loss: (1.1585) | Acc: (84.55%) (27165/32128)\n",
            "Epoch: 7 | Batch_idx: 260 |  Loss: (1.1577) | Acc: (84.63%) (28274/33408)\n",
            "Epoch: 7 | Batch_idx: 270 |  Loss: (1.1568) | Acc: (84.69%) (29376/34688)\n",
            "Epoch: 7 | Batch_idx: 280 |  Loss: (1.1570) | Acc: (84.66%) (30451/35968)\n",
            "Epoch: 7 | Batch_idx: 290 |  Loss: (1.1571) | Acc: (84.64%) (31527/37248)\n",
            "Epoch: 7 | Batch_idx: 300 |  Loss: (1.1587) | Acc: (84.55%) (32574/38528)\n",
            "Epoch: 7 | Batch_idx: 310 |  Loss: (1.1593) | Acc: (84.51%) (33641/39808)\n",
            "Epoch: 7 | Batch_idx: 320 |  Loss: (1.1594) | Acc: (84.49%) (34717/41088)\n",
            "Epoch: 7 | Batch_idx: 330 |  Loss: (1.1591) | Acc: (84.53%) (35812/42368)\n",
            "Epoch: 7 | Batch_idx: 340 |  Loss: (1.1597) | Acc: (84.49%) (36880/43648)\n",
            "Epoch: 7 | Batch_idx: 350 |  Loss: (1.1590) | Acc: (84.52%) (37972/44928)\n",
            "Epoch: 7 | Batch_idx: 360 |  Loss: (1.1585) | Acc: (84.56%) (39072/46208)\n",
            "Epoch: 7 | Batch_idx: 370 |  Loss: (1.1585) | Acc: (84.55%) (40153/47488)\n",
            "Epoch: 7 | Batch_idx: 380 |  Loss: (1.1581) | Acc: (84.58%) (41250/48768)\n",
            "Epoch: 7 | Batch_idx: 390 |  Loss: (1.1581) | Acc: (84.59%) (42295/50000)\n",
            "# TEST : Loss: (1.2027) | Acc: (81.91%) (8191/10000)\n",
            "Epoch: 8 | Batch_idx: 0 |  Loss: (1.1655) | Acc: (80.47%) (103/128)\n",
            "Epoch: 8 | Batch_idx: 10 |  Loss: (1.1582) | Acc: (83.24%) (1172/1408)\n",
            "Epoch: 8 | Batch_idx: 20 |  Loss: (1.1504) | Acc: (84.97%) (2284/2688)\n",
            "Epoch: 8 | Batch_idx: 30 |  Loss: (1.1473) | Acc: (85.33%) (3386/3968)\n",
            "Epoch: 8 | Batch_idx: 40 |  Loss: (1.1438) | Acc: (85.42%) (4483/5248)\n",
            "Epoch: 8 | Batch_idx: 50 |  Loss: (1.1484) | Acc: (85.19%) (5561/6528)\n",
            "Epoch: 8 | Batch_idx: 60 |  Loss: (1.1471) | Acc: (85.25%) (6656/7808)\n",
            "Epoch: 8 | Batch_idx: 70 |  Loss: (1.1448) | Acc: (85.32%) (7754/9088)\n",
            "Epoch: 8 | Batch_idx: 80 |  Loss: (1.1421) | Acc: (85.40%) (8854/10368)\n",
            "Epoch: 8 | Batch_idx: 90 |  Loss: (1.1409) | Acc: (85.45%) (9953/11648)\n",
            "Epoch: 8 | Batch_idx: 100 |  Loss: (1.1383) | Acc: (85.63%) (11070/12928)\n",
            "Epoch: 8 | Batch_idx: 110 |  Loss: (1.1376) | Acc: (85.65%) (12169/14208)\n",
            "Epoch: 8 | Batch_idx: 120 |  Loss: (1.1370) | Acc: (85.64%) (13264/15488)\n",
            "Epoch: 8 | Batch_idx: 130 |  Loss: (1.1353) | Acc: (85.68%) (14366/16768)\n",
            "Epoch: 8 | Batch_idx: 140 |  Loss: (1.1361) | Acc: (85.64%) (15457/18048)\n",
            "Epoch: 8 | Batch_idx: 150 |  Loss: (1.1371) | Acc: (85.60%) (16545/19328)\n",
            "Epoch: 8 | Batch_idx: 160 |  Loss: (1.1364) | Acc: (85.64%) (17649/20608)\n",
            "Epoch: 8 | Batch_idx: 170 |  Loss: (1.1363) | Acc: (85.68%) (18754/21888)\n",
            "Epoch: 8 | Batch_idx: 180 |  Loss: (1.1367) | Acc: (85.67%) (19848/23168)\n",
            "Epoch: 8 | Batch_idx: 190 |  Loss: (1.1364) | Acc: (85.70%) (20953/24448)\n",
            "Epoch: 8 | Batch_idx: 200 |  Loss: (1.1367) | Acc: (85.72%) (22053/25728)\n",
            "Epoch: 8 | Batch_idx: 210 |  Loss: (1.1369) | Acc: (85.73%) (23154/27008)\n",
            "Epoch: 8 | Batch_idx: 220 |  Loss: (1.1370) | Acc: (85.72%) (24249/28288)\n",
            "Epoch: 8 | Batch_idx: 230 |  Loss: (1.1358) | Acc: (85.73%) (25349/29568)\n",
            "Epoch: 8 | Batch_idx: 240 |  Loss: (1.1353) | Acc: (85.78%) (26460/30848)\n",
            "Epoch: 8 | Batch_idx: 250 |  Loss: (1.1343) | Acc: (85.83%) (27576/32128)\n",
            "Epoch: 8 | Batch_idx: 260 |  Loss: (1.1350) | Acc: (85.78%) (28659/33408)\n",
            "Epoch: 8 | Batch_idx: 270 |  Loss: (1.1346) | Acc: (85.85%) (29780/34688)\n",
            "Epoch: 8 | Batch_idx: 280 |  Loss: (1.1342) | Acc: (85.87%) (30884/35968)\n",
            "Epoch: 8 | Batch_idx: 290 |  Loss: (1.1342) | Acc: (85.85%) (31976/37248)\n",
            "Epoch: 8 | Batch_idx: 300 |  Loss: (1.1341) | Acc: (85.83%) (33067/38528)\n",
            "Epoch: 8 | Batch_idx: 310 |  Loss: (1.1339) | Acc: (85.82%) (34164/39808)\n",
            "Epoch: 8 | Batch_idx: 320 |  Loss: (1.1341) | Acc: (85.82%) (35262/41088)\n",
            "Epoch: 8 | Batch_idx: 330 |  Loss: (1.1355) | Acc: (85.77%) (36339/42368)\n",
            "Epoch: 8 | Batch_idx: 340 |  Loss: (1.1353) | Acc: (85.80%) (37448/43648)\n",
            "Epoch: 8 | Batch_idx: 350 |  Loss: (1.1362) | Acc: (85.75%) (38524/44928)\n",
            "Epoch: 8 | Batch_idx: 360 |  Loss: (1.1369) | Acc: (85.71%) (39603/46208)\n",
            "Epoch: 8 | Batch_idx: 370 |  Loss: (1.1372) | Acc: (85.67%) (40684/47488)\n",
            "Epoch: 8 | Batch_idx: 380 |  Loss: (1.1372) | Acc: (85.66%) (41776/48768)\n",
            "Epoch: 8 | Batch_idx: 390 |  Loss: (1.1366) | Acc: (85.71%) (42855/50000)\n",
            "# TEST : Loss: (1.1432) | Acc: (85.32%) (8532/10000)\n",
            "Epoch: 9 | Batch_idx: 0 |  Loss: (1.0946) | Acc: (87.50%) (112/128)\n",
            "Epoch: 9 | Batch_idx: 10 |  Loss: (1.1100) | Acc: (87.50%) (1232/1408)\n",
            "Epoch: 9 | Batch_idx: 20 |  Loss: (1.1103) | Acc: (87.13%) (2342/2688)\n",
            "Epoch: 9 | Batch_idx: 30 |  Loss: (1.1101) | Acc: (87.37%) (3467/3968)\n",
            "Epoch: 9 | Batch_idx: 40 |  Loss: (1.1082) | Acc: (87.42%) (4588/5248)\n",
            "Epoch: 9 | Batch_idx: 50 |  Loss: (1.1063) | Acc: (87.79%) (5731/6528)\n",
            "Epoch: 9 | Batch_idx: 60 |  Loss: (1.1061) | Acc: (87.72%) (6849/7808)\n",
            "Epoch: 9 | Batch_idx: 70 |  Loss: (1.1069) | Acc: (87.54%) (7956/9088)\n",
            "Epoch: 9 | Batch_idx: 80 |  Loss: (1.1073) | Acc: (87.46%) (9068/10368)\n",
            "Epoch: 9 | Batch_idx: 90 |  Loss: (1.1085) | Acc: (87.31%) (10170/11648)\n",
            "Epoch: 9 | Batch_idx: 100 |  Loss: (1.1086) | Acc: (87.31%) (11287/12928)\n",
            "Epoch: 9 | Batch_idx: 110 |  Loss: (1.1092) | Acc: (87.20%) (12389/14208)\n",
            "Epoch: 9 | Batch_idx: 120 |  Loss: (1.1123) | Acc: (87.00%) (13474/15488)\n",
            "Epoch: 9 | Batch_idx: 130 |  Loss: (1.1135) | Acc: (86.92%) (14575/16768)\n",
            "Epoch: 9 | Batch_idx: 140 |  Loss: (1.1142) | Acc: (86.86%) (15677/18048)\n",
            "Epoch: 9 | Batch_idx: 150 |  Loss: (1.1158) | Acc: (86.70%) (16757/19328)\n",
            "Epoch: 9 | Batch_idx: 160 |  Loss: (1.1158) | Acc: (86.68%) (17862/20608)\n",
            "Epoch: 9 | Batch_idx: 170 |  Loss: (1.1162) | Acc: (86.64%) (18963/21888)\n",
            "Epoch: 9 | Batch_idx: 180 |  Loss: (1.1153) | Acc: (86.74%) (20096/23168)\n",
            "Epoch: 9 | Batch_idx: 190 |  Loss: (1.1145) | Acc: (86.80%) (21220/24448)\n",
            "Epoch: 9 | Batch_idx: 200 |  Loss: (1.1153) | Acc: (86.77%) (22323/25728)\n",
            "Epoch: 9 | Batch_idx: 210 |  Loss: (1.1146) | Acc: (86.79%) (23440/27008)\n",
            "Epoch: 9 | Batch_idx: 220 |  Loss: (1.1146) | Acc: (86.82%) (24559/28288)\n",
            "Epoch: 9 | Batch_idx: 230 |  Loss: (1.1158) | Acc: (86.78%) (25659/29568)\n",
            "Epoch: 9 | Batch_idx: 240 |  Loss: (1.1153) | Acc: (86.82%) (26781/30848)\n",
            "Epoch: 9 | Batch_idx: 250 |  Loss: (1.1148) | Acc: (86.85%) (27904/32128)\n",
            "Epoch: 9 | Batch_idx: 260 |  Loss: (1.1154) | Acc: (86.85%) (29015/33408)\n",
            "Epoch: 9 | Batch_idx: 270 |  Loss: (1.1165) | Acc: (86.76%) (30096/34688)\n",
            "Epoch: 9 | Batch_idx: 280 |  Loss: (1.1173) | Acc: (86.72%) (31191/35968)\n",
            "Epoch: 9 | Batch_idx: 290 |  Loss: (1.1171) | Acc: (86.72%) (32301/37248)\n",
            "Epoch: 9 | Batch_idx: 300 |  Loss: (1.1170) | Acc: (86.75%) (33422/38528)\n",
            "Epoch: 9 | Batch_idx: 310 |  Loss: (1.1167) | Acc: (86.78%) (34545/39808)\n",
            "Epoch: 9 | Batch_idx: 320 |  Loss: (1.1164) | Acc: (86.81%) (35668/41088)\n",
            "Epoch: 9 | Batch_idx: 330 |  Loss: (1.1166) | Acc: (86.81%) (36779/42368)\n",
            "Epoch: 9 | Batch_idx: 340 |  Loss: (1.1168) | Acc: (86.81%) (37891/43648)\n",
            "Epoch: 9 | Batch_idx: 350 |  Loss: (1.1164) | Acc: (86.82%) (39008/44928)\n",
            "Epoch: 9 | Batch_idx: 360 |  Loss: (1.1164) | Acc: (86.81%) (40114/46208)\n",
            "Epoch: 9 | Batch_idx: 370 |  Loss: (1.1163) | Acc: (86.81%) (41226/47488)\n",
            "Epoch: 9 | Batch_idx: 380 |  Loss: (1.1165) | Acc: (86.80%) (42333/48768)\n",
            "Epoch: 9 | Batch_idx: 390 |  Loss: (1.1171) | Acc: (86.75%) (43377/50000)\n",
            "# TEST : Loss: (1.1550) | Acc: (84.58%) (8458/10000)\n",
            "Epoch: 10 | Batch_idx: 0 |  Loss: (1.0776) | Acc: (89.84%) (115/128)\n",
            "Epoch: 10 | Batch_idx: 10 |  Loss: (1.0896) | Acc: (88.42%) (1245/1408)\n",
            "Epoch: 10 | Batch_idx: 20 |  Loss: (1.0922) | Acc: (88.24%) (2372/2688)\n",
            "Epoch: 10 | Batch_idx: 30 |  Loss: (1.1005) | Acc: (87.70%) (3480/3968)\n",
            "Epoch: 10 | Batch_idx: 40 |  Loss: (1.0970) | Acc: (88.07%) (4622/5248)\n",
            "Epoch: 10 | Batch_idx: 50 |  Loss: (1.0927) | Acc: (88.31%) (5765/6528)\n",
            "Epoch: 10 | Batch_idx: 60 |  Loss: (1.0946) | Acc: (88.17%) (6884/7808)\n",
            "Epoch: 10 | Batch_idx: 70 |  Loss: (1.0953) | Acc: (88.02%) (7999/9088)\n",
            "Epoch: 10 | Batch_idx: 80 |  Loss: (1.0959) | Acc: (88.02%) (9126/10368)\n",
            "Epoch: 10 | Batch_idx: 90 |  Loss: (1.0955) | Acc: (87.99%) (10249/11648)\n",
            "Epoch: 10 | Batch_idx: 100 |  Loss: (1.0965) | Acc: (87.90%) (11364/12928)\n",
            "Epoch: 10 | Batch_idx: 110 |  Loss: (1.0954) | Acc: (87.98%) (12500/14208)\n",
            "Epoch: 10 | Batch_idx: 120 |  Loss: (1.0935) | Acc: (88.11%) (13646/15488)\n",
            "Epoch: 10 | Batch_idx: 130 |  Loss: (1.0931) | Acc: (88.13%) (14777/16768)\n",
            "Epoch: 10 | Batch_idx: 140 |  Loss: (1.0943) | Acc: (88.07%) (15894/18048)\n",
            "Epoch: 10 | Batch_idx: 150 |  Loss: (1.0963) | Acc: (87.93%) (16996/19328)\n",
            "Epoch: 10 | Batch_idx: 160 |  Loss: (1.0956) | Acc: (87.91%) (18117/20608)\n",
            "Epoch: 10 | Batch_idx: 170 |  Loss: (1.0955) | Acc: (87.91%) (19241/21888)\n",
            "Epoch: 10 | Batch_idx: 180 |  Loss: (1.0977) | Acc: (87.79%) (20340/23168)\n",
            "Epoch: 10 | Batch_idx: 190 |  Loss: (1.0973) | Acc: (87.83%) (21473/24448)\n",
            "Epoch: 10 | Batch_idx: 200 |  Loss: (1.0966) | Acc: (87.89%) (22613/25728)\n",
            "Epoch: 10 | Batch_idx: 210 |  Loss: (1.0957) | Acc: (87.96%) (23757/27008)\n",
            "Epoch: 10 | Batch_idx: 220 |  Loss: (1.0949) | Acc: (88.02%) (24900/28288)\n",
            "Epoch: 10 | Batch_idx: 230 |  Loss: (1.0961) | Acc: (87.96%) (26007/29568)\n",
            "Epoch: 10 | Batch_idx: 240 |  Loss: (1.0953) | Acc: (88.03%) (27156/30848)\n",
            "Epoch: 10 | Batch_idx: 250 |  Loss: (1.0962) | Acc: (88.00%) (28274/32128)\n",
            "Epoch: 10 | Batch_idx: 260 |  Loss: (1.0953) | Acc: (88.09%) (29428/33408)\n",
            "Epoch: 10 | Batch_idx: 270 |  Loss: (1.0950) | Acc: (88.09%) (30556/34688)\n",
            "Epoch: 10 | Batch_idx: 280 |  Loss: (1.0955) | Acc: (88.04%) (31668/35968)\n",
            "Epoch: 10 | Batch_idx: 290 |  Loss: (1.0956) | Acc: (88.00%) (32779/37248)\n",
            "Epoch: 10 | Batch_idx: 300 |  Loss: (1.0961) | Acc: (87.97%) (33894/38528)\n",
            "Epoch: 10 | Batch_idx: 310 |  Loss: (1.0967) | Acc: (87.95%) (35013/39808)\n",
            "Epoch: 10 | Batch_idx: 320 |  Loss: (1.0968) | Acc: (87.93%) (36130/41088)\n",
            "Epoch: 10 | Batch_idx: 330 |  Loss: (1.0966) | Acc: (87.95%) (37261/42368)\n",
            "Epoch: 10 | Batch_idx: 340 |  Loss: (1.0965) | Acc: (87.94%) (38386/43648)\n",
            "Epoch: 10 | Batch_idx: 350 |  Loss: (1.0969) | Acc: (87.91%) (39498/44928)\n",
            "Epoch: 10 | Batch_idx: 360 |  Loss: (1.0978) | Acc: (87.86%) (40599/46208)\n",
            "Epoch: 10 | Batch_idx: 370 |  Loss: (1.0980) | Acc: (87.88%) (41732/47488)\n",
            "Epoch: 10 | Batch_idx: 380 |  Loss: (1.0977) | Acc: (87.88%) (42857/48768)\n",
            "Epoch: 10 | Batch_idx: 390 |  Loss: (1.0981) | Acc: (87.85%) (43927/50000)\n",
            "# TEST : Loss: (1.1411) | Acc: (85.34%) (8534/10000)\n",
            "Epoch: 11 | Batch_idx: 0 |  Loss: (1.0031) | Acc: (93.75%) (120/128)\n",
            "Epoch: 11 | Batch_idx: 10 |  Loss: (1.0546) | Acc: (89.91%) (1266/1408)\n",
            "Epoch: 11 | Batch_idx: 20 |  Loss: (1.0659) | Acc: (89.69%) (2411/2688)\n",
            "Epoch: 11 | Batch_idx: 30 |  Loss: (1.0708) | Acc: (89.26%) (3542/3968)\n",
            "Epoch: 11 | Batch_idx: 40 |  Loss: (1.0718) | Acc: (89.12%) (4677/5248)\n",
            "Epoch: 11 | Batch_idx: 50 |  Loss: (1.0802) | Acc: (88.56%) (5781/6528)\n",
            "Epoch: 11 | Batch_idx: 60 |  Loss: (1.0803) | Acc: (88.55%) (6914/7808)\n",
            "Epoch: 11 | Batch_idx: 70 |  Loss: (1.0781) | Acc: (88.80%) (8070/9088)\n",
            "Epoch: 11 | Batch_idx: 80 |  Loss: (1.0783) | Acc: (88.76%) (9203/10368)\n",
            "Epoch: 11 | Batch_idx: 90 |  Loss: (1.0786) | Acc: (88.81%) (10345/11648)\n",
            "Epoch: 11 | Batch_idx: 100 |  Loss: (1.0768) | Acc: (88.86%) (11488/12928)\n",
            "Epoch: 11 | Batch_idx: 110 |  Loss: (1.0780) | Acc: (88.75%) (12610/14208)\n",
            "Epoch: 11 | Batch_idx: 120 |  Loss: (1.0778) | Acc: (88.79%) (13752/15488)\n",
            "Epoch: 11 | Batch_idx: 130 |  Loss: (1.0779) | Acc: (88.80%) (14890/16768)\n",
            "Epoch: 11 | Batch_idx: 140 |  Loss: (1.0784) | Acc: (88.80%) (16026/18048)\n",
            "Epoch: 11 | Batch_idx: 150 |  Loss: (1.0784) | Acc: (88.86%) (17175/19328)\n",
            "Epoch: 11 | Batch_idx: 160 |  Loss: (1.0784) | Acc: (88.82%) (18303/20608)\n",
            "Epoch: 11 | Batch_idx: 170 |  Loss: (1.0790) | Acc: (88.79%) (19435/21888)\n",
            "Epoch: 11 | Batch_idx: 180 |  Loss: (1.0796) | Acc: (88.79%) (20570/23168)\n",
            "Epoch: 11 | Batch_idx: 190 |  Loss: (1.0811) | Acc: (88.68%) (21681/24448)\n",
            "Epoch: 11 | Batch_idx: 200 |  Loss: (1.0813) | Acc: (88.69%) (22817/25728)\n",
            "Epoch: 11 | Batch_idx: 210 |  Loss: (1.0819) | Acc: (88.65%) (23943/27008)\n",
            "Epoch: 11 | Batch_idx: 220 |  Loss: (1.0830) | Acc: (88.59%) (25060/28288)\n",
            "Epoch: 11 | Batch_idx: 230 |  Loss: (1.0835) | Acc: (88.58%) (26190/29568)\n",
            "Epoch: 11 | Batch_idx: 240 |  Loss: (1.0830) | Acc: (88.61%) (27335/30848)\n",
            "Epoch: 11 | Batch_idx: 250 |  Loss: (1.0838) | Acc: (88.57%) (28456/32128)\n",
            "Epoch: 11 | Batch_idx: 260 |  Loss: (1.0834) | Acc: (88.60%) (29599/33408)\n",
            "Epoch: 11 | Batch_idx: 270 |  Loss: (1.0833) | Acc: (88.62%) (30739/34688)\n",
            "Epoch: 11 | Batch_idx: 280 |  Loss: (1.0835) | Acc: (88.61%) (31870/35968)\n",
            "Epoch: 11 | Batch_idx: 290 |  Loss: (1.0834) | Acc: (88.60%) (33003/37248)\n",
            "Epoch: 11 | Batch_idx: 300 |  Loss: (1.0834) | Acc: (88.60%) (34135/38528)\n",
            "Epoch: 11 | Batch_idx: 310 |  Loss: (1.0842) | Acc: (88.57%) (35259/39808)\n",
            "Epoch: 11 | Batch_idx: 320 |  Loss: (1.0849) | Acc: (88.51%) (36366/41088)\n",
            "Epoch: 11 | Batch_idx: 330 |  Loss: (1.0844) | Acc: (88.56%) (37523/42368)\n",
            "Epoch: 11 | Batch_idx: 340 |  Loss: (1.0848) | Acc: (88.54%) (38645/43648)\n",
            "Epoch: 11 | Batch_idx: 350 |  Loss: (1.0849) | Acc: (88.54%) (39778/44928)\n",
            "Epoch: 11 | Batch_idx: 360 |  Loss: (1.0853) | Acc: (88.49%) (40890/46208)\n",
            "Epoch: 11 | Batch_idx: 370 |  Loss: (1.0858) | Acc: (88.46%) (42008/47488)\n",
            "Epoch: 11 | Batch_idx: 380 |  Loss: (1.0862) | Acc: (88.45%) (43133/48768)\n",
            "Epoch: 11 | Batch_idx: 390 |  Loss: (1.0857) | Acc: (88.47%) (44236/50000)\n",
            "# TEST : Loss: (1.1142) | Acc: (87.00%) (8700/10000)\n",
            "Epoch: 12 | Batch_idx: 0 |  Loss: (1.1083) | Acc: (86.72%) (111/128)\n",
            "Epoch: 12 | Batch_idx: 10 |  Loss: (1.0680) | Acc: (89.84%) (1265/1408)\n",
            "Epoch: 12 | Batch_idx: 20 |  Loss: (1.0593) | Acc: (90.22%) (2425/2688)\n",
            "Epoch: 12 | Batch_idx: 30 |  Loss: (1.0630) | Acc: (89.99%) (3571/3968)\n",
            "Epoch: 12 | Batch_idx: 40 |  Loss: (1.0596) | Acc: (90.24%) (4736/5248)\n",
            "Epoch: 12 | Batch_idx: 50 |  Loss: (1.0631) | Acc: (89.95%) (5872/6528)\n",
            "Epoch: 12 | Batch_idx: 60 |  Loss: (1.0610) | Acc: (89.98%) (7026/7808)\n",
            "Epoch: 12 | Batch_idx: 70 |  Loss: (1.0625) | Acc: (89.82%) (8163/9088)\n",
            "Epoch: 12 | Batch_idx: 80 |  Loss: (1.0604) | Acc: (89.93%) (9324/10368)\n",
            "Epoch: 12 | Batch_idx: 90 |  Loss: (1.0635) | Acc: (89.78%) (10457/11648)\n",
            "Epoch: 12 | Batch_idx: 100 |  Loss: (1.0639) | Acc: (89.80%) (11609/12928)\n",
            "Epoch: 12 | Batch_idx: 110 |  Loss: (1.0667) | Acc: (89.59%) (12729/14208)\n",
            "Epoch: 12 | Batch_idx: 120 |  Loss: (1.0661) | Acc: (89.66%) (13887/15488)\n",
            "Epoch: 12 | Batch_idx: 130 |  Loss: (1.0673) | Acc: (89.58%) (15021/16768)\n",
            "Epoch: 12 | Batch_idx: 140 |  Loss: (1.0684) | Acc: (89.49%) (16152/18048)\n",
            "Epoch: 12 | Batch_idx: 150 |  Loss: (1.0693) | Acc: (89.42%) (17284/19328)\n",
            "Epoch: 12 | Batch_idx: 160 |  Loss: (1.0705) | Acc: (89.38%) (18419/20608)\n",
            "Epoch: 12 | Batch_idx: 170 |  Loss: (1.0698) | Acc: (89.35%) (19558/21888)\n",
            "Epoch: 12 | Batch_idx: 180 |  Loss: (1.0709) | Acc: (89.25%) (20678/23168)\n",
            "Epoch: 12 | Batch_idx: 190 |  Loss: (1.0710) | Acc: (89.25%) (21820/24448)\n",
            "Epoch: 12 | Batch_idx: 200 |  Loss: (1.0707) | Acc: (89.24%) (22959/25728)\n",
            "Epoch: 12 | Batch_idx: 210 |  Loss: (1.0704) | Acc: (89.26%) (24108/27008)\n",
            "Epoch: 12 | Batch_idx: 220 |  Loss: (1.0713) | Acc: (89.22%) (25239/28288)\n",
            "Epoch: 12 | Batch_idx: 230 |  Loss: (1.0713) | Acc: (89.26%) (26393/29568)\n",
            "Epoch: 12 | Batch_idx: 240 |  Loss: (1.0707) | Acc: (89.29%) (27545/30848)\n",
            "Epoch: 12 | Batch_idx: 250 |  Loss: (1.0710) | Acc: (89.31%) (28692/32128)\n",
            "Epoch: 12 | Batch_idx: 260 |  Loss: (1.0706) | Acc: (89.31%) (29836/33408)\n",
            "Epoch: 12 | Batch_idx: 270 |  Loss: (1.0704) | Acc: (89.30%) (30975/34688)\n",
            "Epoch: 12 | Batch_idx: 280 |  Loss: (1.0717) | Acc: (89.21%) (32088/35968)\n",
            "Epoch: 12 | Batch_idx: 290 |  Loss: (1.0726) | Acc: (89.17%) (33213/37248)\n",
            "Epoch: 12 | Batch_idx: 300 |  Loss: (1.0733) | Acc: (89.14%) (34343/38528)\n",
            "Epoch: 12 | Batch_idx: 310 |  Loss: (1.0736) | Acc: (89.12%) (35478/39808)\n",
            "Epoch: 12 | Batch_idx: 320 |  Loss: (1.0742) | Acc: (89.10%) (36609/41088)\n",
            "Epoch: 12 | Batch_idx: 330 |  Loss: (1.0739) | Acc: (89.13%) (37764/42368)\n",
            "Epoch: 12 | Batch_idx: 340 |  Loss: (1.0741) | Acc: (89.14%) (38906/43648)\n",
            "Epoch: 12 | Batch_idx: 350 |  Loss: (1.0740) | Acc: (89.15%) (40054/44928)\n",
            "Epoch: 12 | Batch_idx: 360 |  Loss: (1.0743) | Acc: (89.13%) (41187/46208)\n",
            "Epoch: 12 | Batch_idx: 370 |  Loss: (1.0748) | Acc: (89.13%) (42328/47488)\n",
            "Epoch: 12 | Batch_idx: 380 |  Loss: (1.0748) | Acc: (89.14%) (43471/48768)\n",
            "Epoch: 12 | Batch_idx: 390 |  Loss: (1.0748) | Acc: (89.14%) (44570/50000)\n",
            "# TEST : Loss: (1.1300) | Acc: (86.42%) (8642/10000)\n",
            "Epoch: 13 | Batch_idx: 0 |  Loss: (1.0547) | Acc: (89.06%) (114/128)\n",
            "Epoch: 13 | Batch_idx: 10 |  Loss: (1.0653) | Acc: (89.28%) (1257/1408)\n",
            "Epoch: 13 | Batch_idx: 20 |  Loss: (1.0558) | Acc: (89.96%) (2418/2688)\n",
            "Epoch: 13 | Batch_idx: 30 |  Loss: (1.0563) | Acc: (90.15%) (3577/3968)\n",
            "Epoch: 13 | Batch_idx: 40 |  Loss: (1.0556) | Acc: (90.11%) (4729/5248)\n",
            "Epoch: 13 | Batch_idx: 50 |  Loss: (1.0531) | Acc: (90.30%) (5895/6528)\n",
            "Epoch: 13 | Batch_idx: 60 |  Loss: (1.0532) | Acc: (90.28%) (7049/7808)\n",
            "Epoch: 13 | Batch_idx: 70 |  Loss: (1.0518) | Acc: (90.34%) (8210/9088)\n",
            "Epoch: 13 | Batch_idx: 80 |  Loss: (1.0517) | Acc: (90.38%) (9371/10368)\n",
            "Epoch: 13 | Batch_idx: 90 |  Loss: (1.0513) | Acc: (90.44%) (10535/11648)\n",
            "Epoch: 13 | Batch_idx: 100 |  Loss: (1.0509) | Acc: (90.44%) (11692/12928)\n",
            "Epoch: 13 | Batch_idx: 110 |  Loss: (1.0504) | Acc: (90.50%) (12858/14208)\n",
            "Epoch: 13 | Batch_idx: 120 |  Loss: (1.0516) | Acc: (90.39%) (14000/15488)\n",
            "Epoch: 13 | Batch_idx: 130 |  Loss: (1.0511) | Acc: (90.44%) (15165/16768)\n",
            "Epoch: 13 | Batch_idx: 140 |  Loss: (1.0512) | Acc: (90.43%) (16321/18048)\n",
            "Epoch: 13 | Batch_idx: 150 |  Loss: (1.0519) | Acc: (90.37%) (17467/19328)\n",
            "Epoch: 13 | Batch_idx: 160 |  Loss: (1.0519) | Acc: (90.35%) (18620/20608)\n",
            "Epoch: 13 | Batch_idx: 170 |  Loss: (1.0534) | Acc: (90.27%) (19758/21888)\n",
            "Epoch: 13 | Batch_idx: 180 |  Loss: (1.0535) | Acc: (90.23%) (20905/23168)\n",
            "Epoch: 13 | Batch_idx: 190 |  Loss: (1.0544) | Acc: (90.15%) (22041/24448)\n",
            "Epoch: 13 | Batch_idx: 200 |  Loss: (1.0550) | Acc: (90.10%) (23182/25728)\n",
            "Epoch: 13 | Batch_idx: 210 |  Loss: (1.0544) | Acc: (90.15%) (24347/27008)\n",
            "Epoch: 13 | Batch_idx: 220 |  Loss: (1.0548) | Acc: (90.13%) (25495/28288)\n",
            "Epoch: 13 | Batch_idx: 230 |  Loss: (1.0543) | Acc: (90.14%) (26654/29568)\n",
            "Epoch: 13 | Batch_idx: 240 |  Loss: (1.0555) | Acc: (90.06%) (27782/30848)\n",
            "Epoch: 13 | Batch_idx: 250 |  Loss: (1.0569) | Acc: (89.97%) (28906/32128)\n",
            "Epoch: 13 | Batch_idx: 260 |  Loss: (1.0575) | Acc: (89.97%) (30056/33408)\n",
            "Epoch: 13 | Batch_idx: 270 |  Loss: (1.0573) | Acc: (89.96%) (31204/34688)\n",
            "Epoch: 13 | Batch_idx: 280 |  Loss: (1.0573) | Acc: (89.98%) (32364/35968)\n",
            "Epoch: 13 | Batch_idx: 290 |  Loss: (1.0573) | Acc: (89.96%) (33508/37248)\n",
            "Epoch: 13 | Batch_idx: 300 |  Loss: (1.0580) | Acc: (89.92%) (34645/38528)\n",
            "Epoch: 13 | Batch_idx: 310 |  Loss: (1.0588) | Acc: (89.89%) (35783/39808)\n",
            "Epoch: 13 | Batch_idx: 320 |  Loss: (1.0591) | Acc: (89.87%) (36927/41088)\n",
            "Epoch: 13 | Batch_idx: 330 |  Loss: (1.0598) | Acc: (89.82%) (38057/42368)\n",
            "Epoch: 13 | Batch_idx: 340 |  Loss: (1.0596) | Acc: (89.83%) (39208/43648)\n",
            "Epoch: 13 | Batch_idx: 350 |  Loss: (1.0602) | Acc: (89.81%) (40348/44928)\n",
            "Epoch: 13 | Batch_idx: 360 |  Loss: (1.0604) | Acc: (89.81%) (41501/46208)\n",
            "Epoch: 13 | Batch_idx: 370 |  Loss: (1.0604) | Acc: (89.82%) (42656/47488)\n",
            "Epoch: 13 | Batch_idx: 380 |  Loss: (1.0608) | Acc: (89.80%) (43795/48768)\n",
            "Epoch: 13 | Batch_idx: 390 |  Loss: (1.0605) | Acc: (89.81%) (44903/50000)\n",
            "# TEST : Loss: (1.1124) | Acc: (87.31%) (8731/10000)\n",
            "Epoch: 14 | Batch_idx: 0 |  Loss: (1.0707) | Acc: (90.62%) (116/128)\n",
            "Epoch: 14 | Batch_idx: 10 |  Loss: (1.0593) | Acc: (89.70%) (1263/1408)\n",
            "Epoch: 14 | Batch_idx: 20 |  Loss: (1.0485) | Acc: (90.48%) (2432/2688)\n",
            "Epoch: 14 | Batch_idx: 30 |  Loss: (1.0475) | Acc: (90.45%) (3589/3968)\n",
            "Epoch: 14 | Batch_idx: 40 |  Loss: (1.0478) | Acc: (90.34%) (4741/5248)\n",
            "Epoch: 14 | Batch_idx: 50 |  Loss: (1.0444) | Acc: (90.61%) (5915/6528)\n",
            "Epoch: 14 | Batch_idx: 60 |  Loss: (1.0437) | Acc: (90.71%) (7083/7808)\n",
            "Epoch: 14 | Batch_idx: 70 |  Loss: (1.0430) | Acc: (90.65%) (8238/9088)\n",
            "Epoch: 14 | Batch_idx: 80 |  Loss: (1.0450) | Acc: (90.59%) (9392/10368)\n",
            "Epoch: 14 | Batch_idx: 90 |  Loss: (1.0456) | Acc: (90.61%) (10554/11648)\n",
            "Epoch: 14 | Batch_idx: 100 |  Loss: (1.0468) | Acc: (90.62%) (11715/12928)\n",
            "Epoch: 14 | Batch_idx: 110 |  Loss: (1.0451) | Acc: (90.73%) (12891/14208)\n",
            "Epoch: 14 | Batch_idx: 120 |  Loss: (1.0456) | Acc: (90.70%) (14047/15488)\n",
            "Epoch: 14 | Batch_idx: 130 |  Loss: (1.0452) | Acc: (90.80%) (15225/16768)\n",
            "Epoch: 14 | Batch_idx: 140 |  Loss: (1.0471) | Acc: (90.69%) (16367/18048)\n",
            "Epoch: 14 | Batch_idx: 150 |  Loss: (1.0488) | Acc: (90.57%) (17505/19328)\n",
            "Epoch: 14 | Batch_idx: 160 |  Loss: (1.0492) | Acc: (90.53%) (18656/20608)\n",
            "Epoch: 14 | Batch_idx: 170 |  Loss: (1.0493) | Acc: (90.55%) (19820/21888)\n",
            "Epoch: 14 | Batch_idx: 180 |  Loss: (1.0491) | Acc: (90.57%) (20984/23168)\n",
            "Epoch: 14 | Batch_idx: 190 |  Loss: (1.0486) | Acc: (90.58%) (22146/24448)\n",
            "Epoch: 14 | Batch_idx: 200 |  Loss: (1.0488) | Acc: (90.55%) (23297/25728)\n",
            "Epoch: 14 | Batch_idx: 210 |  Loss: (1.0504) | Acc: (90.47%) (24435/27008)\n",
            "Epoch: 14 | Batch_idx: 220 |  Loss: (1.0513) | Acc: (90.40%) (25571/28288)\n",
            "Epoch: 14 | Batch_idx: 230 |  Loss: (1.0509) | Acc: (90.41%) (26731/29568)\n",
            "Epoch: 14 | Batch_idx: 240 |  Loss: (1.0511) | Acc: (90.40%) (27887/30848)\n",
            "Epoch: 14 | Batch_idx: 250 |  Loss: (1.0520) | Acc: (90.36%) (29031/32128)\n",
            "Epoch: 14 | Batch_idx: 260 |  Loss: (1.0528) | Acc: (90.31%) (30170/33408)\n",
            "Epoch: 14 | Batch_idx: 270 |  Loss: (1.0528) | Acc: (90.31%) (31328/34688)\n",
            "Epoch: 14 | Batch_idx: 280 |  Loss: (1.0534) | Acc: (90.27%) (32467/35968)\n",
            "Epoch: 14 | Batch_idx: 290 |  Loss: (1.0537) | Acc: (90.25%) (33616/37248)\n",
            "Epoch: 14 | Batch_idx: 300 |  Loss: (1.0540) | Acc: (90.24%) (34769/38528)\n",
            "Epoch: 14 | Batch_idx: 310 |  Loss: (1.0537) | Acc: (90.26%) (35932/39808)\n",
            "Epoch: 14 | Batch_idx: 320 |  Loss: (1.0539) | Acc: (90.24%) (37077/41088)\n",
            "Epoch: 14 | Batch_idx: 330 |  Loss: (1.0549) | Acc: (90.20%) (38215/42368)\n",
            "Epoch: 14 | Batch_idx: 340 |  Loss: (1.0552) | Acc: (90.16%) (39351/43648)\n",
            "Epoch: 14 | Batch_idx: 350 |  Loss: (1.0552) | Acc: (90.16%) (40507/44928)\n",
            "Epoch: 14 | Batch_idx: 360 |  Loss: (1.0559) | Acc: (90.12%) (41641/46208)\n",
            "Epoch: 14 | Batch_idx: 370 |  Loss: (1.0561) | Acc: (90.09%) (42783/47488)\n",
            "Epoch: 14 | Batch_idx: 380 |  Loss: (1.0558) | Acc: (90.09%) (43937/48768)\n",
            "Epoch: 14 | Batch_idx: 390 |  Loss: (1.0556) | Acc: (90.10%) (45048/50000)\n",
            "# TEST : Loss: (1.1088) | Acc: (87.24%) (8724/10000)\n",
            "Epoch: 15 | Batch_idx: 0 |  Loss: (1.0479) | Acc: (89.84%) (115/128)\n",
            "Epoch: 15 | Batch_idx: 10 |  Loss: (1.0306) | Acc: (91.62%) (1290/1408)\n",
            "Epoch: 15 | Batch_idx: 20 |  Loss: (1.0301) | Acc: (91.67%) (2464/2688)\n",
            "Epoch: 15 | Batch_idx: 30 |  Loss: (1.0292) | Acc: (91.78%) (3642/3968)\n",
            "Epoch: 15 | Batch_idx: 40 |  Loss: (1.0274) | Acc: (91.75%) (4815/5248)\n",
            "Epoch: 15 | Batch_idx: 50 |  Loss: (1.0268) | Acc: (91.84%) (5995/6528)\n",
            "Epoch: 15 | Batch_idx: 60 |  Loss: (1.0261) | Acc: (91.93%) (7178/7808)\n",
            "Epoch: 15 | Batch_idx: 70 |  Loss: (1.0284) | Acc: (91.78%) (8341/9088)\n",
            "Epoch: 15 | Batch_idx: 80 |  Loss: (1.0292) | Acc: (91.61%) (9498/10368)\n",
            "Epoch: 15 | Batch_idx: 90 |  Loss: (1.0292) | Acc: (91.66%) (10677/11648)\n",
            "Epoch: 15 | Batch_idx: 100 |  Loss: (1.0310) | Acc: (91.58%) (11839/12928)\n",
            "Epoch: 15 | Batch_idx: 110 |  Loss: (1.0305) | Acc: (91.56%) (13009/14208)\n",
            "Epoch: 15 | Batch_idx: 120 |  Loss: (1.0324) | Acc: (91.41%) (14158/15488)\n",
            "Epoch: 15 | Batch_idx: 130 |  Loss: (1.0313) | Acc: (91.48%) (15340/16768)\n",
            "Epoch: 15 | Batch_idx: 140 |  Loss: (1.0331) | Acc: (91.33%) (16484/18048)\n",
            "Epoch: 15 | Batch_idx: 150 |  Loss: (1.0340) | Acc: (91.31%) (17649/19328)\n",
            "Epoch: 15 | Batch_idx: 160 |  Loss: (1.0344) | Acc: (91.31%) (18817/20608)\n",
            "Epoch: 15 | Batch_idx: 170 |  Loss: (1.0342) | Acc: (91.33%) (19990/21888)\n",
            "Epoch: 15 | Batch_idx: 180 |  Loss: (1.0346) | Acc: (91.34%) (21161/23168)\n",
            "Epoch: 15 | Batch_idx: 190 |  Loss: (1.0365) | Acc: (91.23%) (22304/24448)\n",
            "Epoch: 15 | Batch_idx: 200 |  Loss: (1.0367) | Acc: (91.21%) (23467/25728)\n",
            "Epoch: 15 | Batch_idx: 210 |  Loss: (1.0364) | Acc: (91.27%) (24651/27008)\n",
            "Epoch: 15 | Batch_idx: 220 |  Loss: (1.0372) | Acc: (91.21%) (25802/28288)\n",
            "Epoch: 15 | Batch_idx: 230 |  Loss: (1.0377) | Acc: (91.19%) (26963/29568)\n",
            "Epoch: 15 | Batch_idx: 240 |  Loss: (1.0376) | Acc: (91.19%) (28129/30848)\n",
            "Epoch: 15 | Batch_idx: 250 |  Loss: (1.0383) | Acc: (91.16%) (29287/32128)\n",
            "Epoch: 15 | Batch_idx: 260 |  Loss: (1.0379) | Acc: (91.16%) (30455/33408)\n",
            "Epoch: 15 | Batch_idx: 270 |  Loss: (1.0386) | Acc: (91.10%) (31602/34688)\n",
            "Epoch: 15 | Batch_idx: 280 |  Loss: (1.0392) | Acc: (91.06%) (32752/35968)\n",
            "Epoch: 15 | Batch_idx: 290 |  Loss: (1.0398) | Acc: (91.03%) (33908/37248)\n",
            "Epoch: 15 | Batch_idx: 300 |  Loss: (1.0398) | Acc: (91.04%) (35074/38528)\n",
            "Epoch: 15 | Batch_idx: 310 |  Loss: (1.0399) | Acc: (91.02%) (36234/39808)\n",
            "Epoch: 15 | Batch_idx: 320 |  Loss: (1.0398) | Acc: (91.04%) (37407/41088)\n",
            "Epoch: 15 | Batch_idx: 330 |  Loss: (1.0403) | Acc: (91.03%) (38566/42368)\n",
            "Epoch: 15 | Batch_idx: 340 |  Loss: (1.0403) | Acc: (91.01%) (39725/43648)\n",
            "Epoch: 15 | Batch_idx: 350 |  Loss: (1.0400) | Acc: (91.04%) (40901/44928)\n",
            "Epoch: 15 | Batch_idx: 360 |  Loss: (1.0406) | Acc: (91.00%) (42047/46208)\n",
            "Epoch: 15 | Batch_idx: 370 |  Loss: (1.0416) | Acc: (90.92%) (43178/47488)\n",
            "Epoch: 15 | Batch_idx: 380 |  Loss: (1.0417) | Acc: (90.93%) (44344/48768)\n",
            "Epoch: 15 | Batch_idx: 390 |  Loss: (1.0416) | Acc: (90.94%) (45470/50000)\n",
            "# TEST : Loss: (1.1086) | Acc: (87.32%) (8732/10000)\n",
            "Epoch: 16 | Batch_idx: 0 |  Loss: (0.9984) | Acc: (92.19%) (118/128)\n",
            "Epoch: 16 | Batch_idx: 10 |  Loss: (1.0239) | Acc: (91.83%) (1293/1408)\n",
            "Epoch: 16 | Batch_idx: 20 |  Loss: (1.0204) | Acc: (91.89%) (2470/2688)\n",
            "Epoch: 16 | Batch_idx: 30 |  Loss: (1.0215) | Acc: (91.91%) (3647/3968)\n",
            "Epoch: 16 | Batch_idx: 40 |  Loss: (1.0251) | Acc: (91.77%) (4816/5248)\n",
            "Epoch: 16 | Batch_idx: 50 |  Loss: (1.0241) | Acc: (91.88%) (5998/6528)\n",
            "Epoch: 16 | Batch_idx: 60 |  Loss: (1.0248) | Acc: (91.88%) (7174/7808)\n",
            "Epoch: 16 | Batch_idx: 70 |  Loss: (1.0207) | Acc: (92.19%) (8378/9088)\n",
            "Epoch: 16 | Batch_idx: 80 |  Loss: (1.0227) | Acc: (91.97%) (9535/10368)\n",
            "Epoch: 16 | Batch_idx: 90 |  Loss: (1.0250) | Acc: (91.82%) (10695/11648)\n",
            "Epoch: 16 | Batch_idx: 100 |  Loss: (1.0271) | Acc: (91.68%) (11853/12928)\n",
            "Epoch: 16 | Batch_idx: 110 |  Loss: (1.0264) | Acc: (91.75%) (13036/14208)\n",
            "Epoch: 16 | Batch_idx: 120 |  Loss: (1.0285) | Acc: (91.65%) (14195/15488)\n",
            "Epoch: 16 | Batch_idx: 130 |  Loss: (1.0280) | Acc: (91.67%) (15372/16768)\n",
            "Epoch: 16 | Batch_idx: 140 |  Loss: (1.0265) | Acc: (91.82%) (16572/18048)\n",
            "Epoch: 16 | Batch_idx: 150 |  Loss: (1.0261) | Acc: (91.85%) (17752/19328)\n",
            "Epoch: 16 | Batch_idx: 160 |  Loss: (1.0253) | Acc: (91.94%) (18946/20608)\n",
            "Epoch: 16 | Batch_idx: 170 |  Loss: (1.0248) | Acc: (91.94%) (20123/21888)\n",
            "Epoch: 16 | Batch_idx: 180 |  Loss: (1.0253) | Acc: (91.91%) (21293/23168)\n",
            "Epoch: 16 | Batch_idx: 190 |  Loss: (1.0268) | Acc: (91.81%) (22445/24448)\n",
            "Epoch: 16 | Batch_idx: 200 |  Loss: (1.0270) | Acc: (91.75%) (23606/25728)\n",
            "Epoch: 16 | Batch_idx: 210 |  Loss: (1.0269) | Acc: (91.77%) (24785/27008)\n",
            "Epoch: 16 | Batch_idx: 220 |  Loss: (1.0274) | Acc: (91.74%) (25951/28288)\n",
            "Epoch: 16 | Batch_idx: 230 |  Loss: (1.0280) | Acc: (91.67%) (27106/29568)\n",
            "Epoch: 16 | Batch_idx: 240 |  Loss: (1.0286) | Acc: (91.63%) (28266/30848)\n",
            "Epoch: 16 | Batch_idx: 250 |  Loss: (1.0296) | Acc: (91.57%) (29421/32128)\n",
            "Epoch: 16 | Batch_idx: 260 |  Loss: (1.0300) | Acc: (91.53%) (30579/33408)\n",
            "Epoch: 16 | Batch_idx: 270 |  Loss: (1.0306) | Acc: (91.50%) (31741/34688)\n",
            "Epoch: 16 | Batch_idx: 280 |  Loss: (1.0308) | Acc: (91.51%) (32914/35968)\n",
            "Epoch: 16 | Batch_idx: 290 |  Loss: (1.0302) | Acc: (91.55%) (34100/37248)\n",
            "Epoch: 16 | Batch_idx: 300 |  Loss: (1.0318) | Acc: (91.46%) (35236/38528)\n",
            "Epoch: 16 | Batch_idx: 310 |  Loss: (1.0322) | Acc: (91.43%) (36397/39808)\n",
            "Epoch: 16 | Batch_idx: 320 |  Loss: (1.0324) | Acc: (91.42%) (37561/41088)\n",
            "Epoch: 16 | Batch_idx: 330 |  Loss: (1.0327) | Acc: (91.39%) (38721/42368)\n",
            "Epoch: 16 | Batch_idx: 340 |  Loss: (1.0330) | Acc: (91.39%) (39888/43648)\n",
            "Epoch: 16 | Batch_idx: 350 |  Loss: (1.0327) | Acc: (91.41%) (41067/44928)\n",
            "Epoch: 16 | Batch_idx: 360 |  Loss: (1.0326) | Acc: (91.39%) (42230/46208)\n",
            "Epoch: 16 | Batch_idx: 370 |  Loss: (1.0332) | Acc: (91.37%) (43389/47488)\n",
            "Epoch: 16 | Batch_idx: 380 |  Loss: (1.0334) | Acc: (91.35%) (44552/48768)\n",
            "Epoch: 16 | Batch_idx: 390 |  Loss: (1.0342) | Acc: (91.31%) (45656/50000)\n",
            "# TEST : Loss: (1.1185) | Acc: (87.18%) (8718/10000)\n",
            "Epoch: 17 | Batch_idx: 0 |  Loss: (1.0134) | Acc: (92.19%) (118/128)\n",
            "Epoch: 17 | Batch_idx: 10 |  Loss: (1.0081) | Acc: (93.39%) (1315/1408)\n",
            "Epoch: 17 | Batch_idx: 20 |  Loss: (1.0117) | Acc: (92.71%) (2492/2688)\n",
            "Epoch: 17 | Batch_idx: 30 |  Loss: (1.0182) | Acc: (92.41%) (3667/3968)\n",
            "Epoch: 17 | Batch_idx: 40 |  Loss: (1.0147) | Acc: (92.72%) (4866/5248)\n",
            "Epoch: 17 | Batch_idx: 50 |  Loss: (1.0160) | Acc: (92.60%) (6045/6528)\n",
            "Epoch: 17 | Batch_idx: 60 |  Loss: (1.0186) | Acc: (92.47%) (7220/7808)\n",
            "Epoch: 17 | Batch_idx: 70 |  Loss: (1.0188) | Acc: (92.46%) (8403/9088)\n",
            "Epoch: 17 | Batch_idx: 80 |  Loss: (1.0171) | Acc: (92.54%) (9595/10368)\n",
            "Epoch: 17 | Batch_idx: 90 |  Loss: (1.0155) | Acc: (92.64%) (10791/11648)\n",
            "Epoch: 17 | Batch_idx: 100 |  Loss: (1.0160) | Acc: (92.57%) (11968/12928)\n",
            "Epoch: 17 | Batch_idx: 110 |  Loss: (1.0185) | Acc: (92.41%) (13130/14208)\n",
            "Epoch: 17 | Batch_idx: 120 |  Loss: (1.0200) | Acc: (92.32%) (14299/15488)\n",
            "Epoch: 17 | Batch_idx: 130 |  Loss: (1.0205) | Acc: (92.27%) (15472/16768)\n",
            "Epoch: 17 | Batch_idx: 140 |  Loss: (1.0218) | Acc: (92.14%) (16630/18048)\n",
            "Epoch: 17 | Batch_idx: 150 |  Loss: (1.0224) | Acc: (92.07%) (17796/19328)\n",
            "Epoch: 17 | Batch_idx: 160 |  Loss: (1.0225) | Acc: (92.11%) (18981/20608)\n",
            "Epoch: 17 | Batch_idx: 170 |  Loss: (1.0225) | Acc: (92.09%) (20156/21888)\n",
            "Epoch: 17 | Batch_idx: 180 |  Loss: (1.0219) | Acc: (92.11%) (21341/23168)\n",
            "Epoch: 17 | Batch_idx: 190 |  Loss: (1.0220) | Acc: (92.12%) (22522/24448)\n",
            "Epoch: 17 | Batch_idx: 200 |  Loss: (1.0225) | Acc: (92.07%) (23689/25728)\n",
            "Epoch: 17 | Batch_idx: 210 |  Loss: (1.0229) | Acc: (92.05%) (24862/27008)\n",
            "Epoch: 17 | Batch_idx: 220 |  Loss: (1.0228) | Acc: (92.06%) (26043/28288)\n",
            "Epoch: 17 | Batch_idx: 230 |  Loss: (1.0232) | Acc: (92.03%) (27211/29568)\n",
            "Epoch: 17 | Batch_idx: 240 |  Loss: (1.0236) | Acc: (91.97%) (28370/30848)\n",
            "Epoch: 17 | Batch_idx: 250 |  Loss: (1.0239) | Acc: (91.93%) (29536/32128)\n",
            "Epoch: 17 | Batch_idx: 260 |  Loss: (1.0243) | Acc: (91.92%) (30710/33408)\n",
            "Epoch: 17 | Batch_idx: 270 |  Loss: (1.0243) | Acc: (91.93%) (31890/34688)\n",
            "Epoch: 17 | Batch_idx: 280 |  Loss: (1.0243) | Acc: (91.92%) (33060/35968)\n",
            "Epoch: 17 | Batch_idx: 290 |  Loss: (1.0249) | Acc: (91.88%) (34224/37248)\n",
            "Epoch: 17 | Batch_idx: 300 |  Loss: (1.0254) | Acc: (91.86%) (35392/38528)\n",
            "Epoch: 17 | Batch_idx: 310 |  Loss: (1.0261) | Acc: (91.81%) (36549/39808)\n",
            "Epoch: 17 | Batch_idx: 320 |  Loss: (1.0263) | Acc: (91.78%) (37712/41088)\n",
            "Epoch: 17 | Batch_idx: 330 |  Loss: (1.0261) | Acc: (91.79%) (38890/42368)\n",
            "Epoch: 17 | Batch_idx: 340 |  Loss: (1.0260) | Acc: (91.78%) (40060/43648)\n",
            "Epoch: 17 | Batch_idx: 350 |  Loss: (1.0254) | Acc: (91.80%) (41244/44928)\n",
            "Epoch: 17 | Batch_idx: 360 |  Loss: (1.0254) | Acc: (91.82%) (42428/46208)\n",
            "Epoch: 17 | Batch_idx: 370 |  Loss: (1.0255) | Acc: (91.82%) (43604/47488)\n",
            "Epoch: 17 | Batch_idx: 380 |  Loss: (1.0263) | Acc: (91.78%) (44757/48768)\n",
            "Epoch: 17 | Batch_idx: 390 |  Loss: (1.0268) | Acc: (91.75%) (45873/50000)\n",
            "# TEST : Loss: (1.0957) | Acc: (87.97%) (8797/10000)\n",
            "Epoch: 18 | Batch_idx: 0 |  Loss: (1.0087) | Acc: (92.97%) (119/128)\n",
            "Epoch: 18 | Batch_idx: 10 |  Loss: (1.0095) | Acc: (92.68%) (1305/1408)\n",
            "Epoch: 18 | Batch_idx: 20 |  Loss: (1.0073) | Acc: (93.08%) (2502/2688)\n",
            "Epoch: 18 | Batch_idx: 30 |  Loss: (1.0074) | Acc: (93.20%) (3698/3968)\n",
            "Epoch: 18 | Batch_idx: 40 |  Loss: (1.0092) | Acc: (92.99%) (4880/5248)\n",
            "Epoch: 18 | Batch_idx: 50 |  Loss: (1.0112) | Acc: (92.83%) (6060/6528)\n",
            "Epoch: 18 | Batch_idx: 60 |  Loss: (1.0125) | Acc: (92.80%) (7246/7808)\n",
            "Epoch: 18 | Batch_idx: 70 |  Loss: (1.0149) | Acc: (92.64%) (8419/9088)\n",
            "Epoch: 18 | Batch_idx: 80 |  Loss: (1.0159) | Acc: (92.53%) (9594/10368)\n",
            "Epoch: 18 | Batch_idx: 90 |  Loss: (1.0163) | Acc: (92.45%) (10769/11648)\n",
            "Epoch: 18 | Batch_idx: 100 |  Loss: (1.0160) | Acc: (92.45%) (11952/12928)\n",
            "Epoch: 18 | Batch_idx: 110 |  Loss: (1.0144) | Acc: (92.53%) (13147/14208)\n",
            "Epoch: 18 | Batch_idx: 120 |  Loss: (1.0151) | Acc: (92.43%) (14315/15488)\n",
            "Epoch: 18 | Batch_idx: 130 |  Loss: (1.0155) | Acc: (92.43%) (15499/16768)\n",
            "Epoch: 18 | Batch_idx: 140 |  Loss: (1.0155) | Acc: (92.46%) (16687/18048)\n",
            "Epoch: 18 | Batch_idx: 150 |  Loss: (1.0145) | Acc: (92.50%) (17879/19328)\n",
            "Epoch: 18 | Batch_idx: 160 |  Loss: (1.0145) | Acc: (92.52%) (19066/20608)\n",
            "Epoch: 18 | Batch_idx: 170 |  Loss: (1.0153) | Acc: (92.49%) (20245/21888)\n",
            "Epoch: 18 | Batch_idx: 180 |  Loss: (1.0155) | Acc: (92.46%) (21420/23168)\n",
            "Epoch: 18 | Batch_idx: 190 |  Loss: (1.0153) | Acc: (92.45%) (22601/24448)\n",
            "Epoch: 18 | Batch_idx: 200 |  Loss: (1.0159) | Acc: (92.41%) (23776/25728)\n",
            "Epoch: 18 | Batch_idx: 210 |  Loss: (1.0162) | Acc: (92.39%) (24953/27008)\n",
            "Epoch: 18 | Batch_idx: 220 |  Loss: (1.0163) | Acc: (92.39%) (26134/28288)\n",
            "Epoch: 18 | Batch_idx: 230 |  Loss: (1.0173) | Acc: (92.34%) (27302/29568)\n",
            "Epoch: 18 | Batch_idx: 240 |  Loss: (1.0167) | Acc: (92.37%) (28494/30848)\n",
            "Epoch: 18 | Batch_idx: 250 |  Loss: (1.0172) | Acc: (92.35%) (29670/32128)\n",
            "Epoch: 18 | Batch_idx: 260 |  Loss: (1.0177) | Acc: (92.29%) (30833/33408)\n",
            "Epoch: 18 | Batch_idx: 270 |  Loss: (1.0177) | Acc: (92.29%) (32012/34688)\n",
            "Epoch: 18 | Batch_idx: 280 |  Loss: (1.0180) | Acc: (92.26%) (33183/35968)\n",
            "Epoch: 18 | Batch_idx: 290 |  Loss: (1.0181) | Acc: (92.25%) (34363/37248)\n",
            "Epoch: 18 | Batch_idx: 300 |  Loss: (1.0180) | Acc: (92.26%) (35545/38528)\n",
            "Epoch: 18 | Batch_idx: 310 |  Loss: (1.0179) | Acc: (92.28%) (36735/39808)\n",
            "Epoch: 18 | Batch_idx: 320 |  Loss: (1.0176) | Acc: (92.28%) (37915/41088)\n",
            "Epoch: 18 | Batch_idx: 330 |  Loss: (1.0180) | Acc: (92.26%) (39088/42368)\n",
            "Epoch: 18 | Batch_idx: 340 |  Loss: (1.0188) | Acc: (92.20%) (40244/43648)\n",
            "Epoch: 18 | Batch_idx: 350 |  Loss: (1.0194) | Acc: (92.17%) (41412/44928)\n",
            "Epoch: 18 | Batch_idx: 360 |  Loss: (1.0201) | Acc: (92.14%) (42576/46208)\n",
            "Epoch: 18 | Batch_idx: 370 |  Loss: (1.0203) | Acc: (92.12%) (43747/47488)\n",
            "Epoch: 18 | Batch_idx: 380 |  Loss: (1.0210) | Acc: (92.07%) (44903/48768)\n",
            "Epoch: 18 | Batch_idx: 390 |  Loss: (1.0219) | Acc: (92.02%) (46008/50000)\n",
            "# TEST : Loss: (1.0973) | Acc: (87.67%) (8767/10000)\n",
            "Epoch: 19 | Batch_idx: 0 |  Loss: (0.9927) | Acc: (92.97%) (119/128)\n",
            "Epoch: 19 | Batch_idx: 10 |  Loss: (0.9971) | Acc: (93.47%) (1316/1408)\n",
            "Epoch: 19 | Batch_idx: 20 |  Loss: (1.0022) | Acc: (93.23%) (2506/2688)\n",
            "Epoch: 19 | Batch_idx: 30 |  Loss: (1.0058) | Acc: (93.04%) (3692/3968)\n",
            "Epoch: 19 | Batch_idx: 40 |  Loss: (1.0108) | Acc: (92.80%) (4870/5248)\n",
            "Epoch: 19 | Batch_idx: 50 |  Loss: (1.0118) | Acc: (92.77%) (6056/6528)\n",
            "Epoch: 19 | Batch_idx: 60 |  Loss: (1.0110) | Acc: (92.75%) (7242/7808)\n",
            "Epoch: 19 | Batch_idx: 70 |  Loss: (1.0108) | Acc: (92.76%) (8430/9088)\n",
            "Epoch: 19 | Batch_idx: 80 |  Loss: (1.0139) | Acc: (92.55%) (9596/10368)\n",
            "Epoch: 19 | Batch_idx: 90 |  Loss: (1.0130) | Acc: (92.60%) (10786/11648)\n",
            "Epoch: 19 | Batch_idx: 100 |  Loss: (1.0129) | Acc: (92.56%) (11966/12928)\n",
            "Epoch: 19 | Batch_idx: 110 |  Loss: (1.0127) | Acc: (92.55%) (13150/14208)\n",
            "Epoch: 19 | Batch_idx: 120 |  Loss: (1.0121) | Acc: (92.55%) (14334/15488)\n",
            "Epoch: 19 | Batch_idx: 130 |  Loss: (1.0120) | Acc: (92.58%) (15524/16768)\n",
            "Epoch: 19 | Batch_idx: 140 |  Loss: (1.0124) | Acc: (92.54%) (16702/18048)\n",
            "Epoch: 19 | Batch_idx: 150 |  Loss: (1.0140) | Acc: (92.45%) (17869/19328)\n",
            "Epoch: 19 | Batch_idx: 160 |  Loss: (1.0132) | Acc: (92.49%) (19061/20608)\n",
            "Epoch: 19 | Batch_idx: 170 |  Loss: (1.0131) | Acc: (92.52%) (20250/21888)\n",
            "Epoch: 19 | Batch_idx: 180 |  Loss: (1.0133) | Acc: (92.47%) (21423/23168)\n",
            "Epoch: 19 | Batch_idx: 190 |  Loss: (1.0128) | Acc: (92.50%) (22614/24448)\n",
            "Epoch: 19 | Batch_idx: 200 |  Loss: (1.0117) | Acc: (92.55%) (23811/25728)\n",
            "Epoch: 19 | Batch_idx: 210 |  Loss: (1.0111) | Acc: (92.61%) (25013/27008)\n",
            "Epoch: 19 | Batch_idx: 220 |  Loss: (1.0112) | Acc: (92.58%) (26190/28288)\n",
            "Epoch: 19 | Batch_idx: 230 |  Loss: (1.0117) | Acc: (92.54%) (27362/29568)\n",
            "Epoch: 19 | Batch_idx: 240 |  Loss: (1.0119) | Acc: (92.54%) (28546/30848)\n",
            "Epoch: 19 | Batch_idx: 250 |  Loss: (1.0118) | Acc: (92.53%) (29729/32128)\n",
            "Epoch: 19 | Batch_idx: 260 |  Loss: (1.0126) | Acc: (92.50%) (30901/33408)\n",
            "Epoch: 19 | Batch_idx: 270 |  Loss: (1.0131) | Acc: (92.45%) (32068/34688)\n",
            "Epoch: 19 | Batch_idx: 280 |  Loss: (1.0142) | Acc: (92.36%) (33221/35968)\n",
            "Epoch: 19 | Batch_idx: 290 |  Loss: (1.0151) | Acc: (92.33%) (34392/37248)\n",
            "Epoch: 19 | Batch_idx: 300 |  Loss: (1.0153) | Acc: (92.33%) (35572/38528)\n",
            "Epoch: 19 | Batch_idx: 310 |  Loss: (1.0154) | Acc: (92.32%) (36751/39808)\n",
            "Epoch: 19 | Batch_idx: 320 |  Loss: (1.0156) | Acc: (92.34%) (37939/41088)\n",
            "Epoch: 19 | Batch_idx: 330 |  Loss: (1.0162) | Acc: (92.31%) (39111/42368)\n",
            "Epoch: 19 | Batch_idx: 340 |  Loss: (1.0166) | Acc: (92.30%) (40288/43648)\n",
            "Epoch: 19 | Batch_idx: 350 |  Loss: (1.0164) | Acc: (92.33%) (41482/44928)\n",
            "Epoch: 19 | Batch_idx: 360 |  Loss: (1.0167) | Acc: (92.32%) (42658/46208)\n",
            "Epoch: 19 | Batch_idx: 370 |  Loss: (1.0172) | Acc: (92.28%) (43821/47488)\n",
            "Epoch: 19 | Batch_idx: 380 |  Loss: (1.0178) | Acc: (92.26%) (44994/48768)\n",
            "Epoch: 19 | Batch_idx: 390 |  Loss: (1.0179) | Acc: (92.26%) (46130/50000)\n",
            "# TEST : Loss: (1.1086) | Acc: (87.23%) (8723/10000)\n",
            "Epoch: 20 | Batch_idx: 0 |  Loss: (0.9818) | Acc: (93.75%) (120/128)\n",
            "Epoch: 20 | Batch_idx: 10 |  Loss: (0.9833) | Acc: (93.75%) (1320/1408)\n",
            "Epoch: 20 | Batch_idx: 20 |  Loss: (0.9774) | Acc: (94.42%) (2538/2688)\n",
            "Epoch: 20 | Batch_idx: 30 |  Loss: (0.9869) | Acc: (93.88%) (3725/3968)\n",
            "Epoch: 20 | Batch_idx: 40 |  Loss: (0.9903) | Acc: (93.43%) (4903/5248)\n",
            "Epoch: 20 | Batch_idx: 50 |  Loss: (0.9954) | Acc: (93.18%) (6083/6528)\n",
            "Epoch: 20 | Batch_idx: 60 |  Loss: (0.9996) | Acc: (93.12%) (7271/7808)\n",
            "Epoch: 20 | Batch_idx: 70 |  Loss: (1.0000) | Acc: (93.19%) (8469/9088)\n",
            "Epoch: 20 | Batch_idx: 80 |  Loss: (1.0027) | Acc: (93.08%) (9651/10368)\n",
            "Epoch: 20 | Batch_idx: 90 |  Loss: (1.0041) | Acc: (92.98%) (10830/11648)\n",
            "Epoch: 20 | Batch_idx: 100 |  Loss: (1.0056) | Acc: (92.95%) (12017/12928)\n",
            "Epoch: 20 | Batch_idx: 110 |  Loss: (1.0091) | Acc: (92.76%) (13180/14208)\n",
            "Epoch: 20 | Batch_idx: 120 |  Loss: (1.0078) | Acc: (92.84%) (14379/15488)\n",
            "Epoch: 20 | Batch_idx: 130 |  Loss: (1.0089) | Acc: (92.78%) (15558/16768)\n",
            "Epoch: 20 | Batch_idx: 140 |  Loss: (1.0094) | Acc: (92.79%) (16747/18048)\n",
            "Epoch: 20 | Batch_idx: 150 |  Loss: (1.0114) | Acc: (92.65%) (17908/19328)\n",
            "Epoch: 20 | Batch_idx: 160 |  Loss: (1.0125) | Acc: (92.62%) (19087/20608)\n",
            "Epoch: 20 | Batch_idx: 170 |  Loss: (1.0129) | Acc: (92.60%) (20269/21888)\n",
            "Epoch: 20 | Batch_idx: 180 |  Loss: (1.0126) | Acc: (92.63%) (21460/23168)\n",
            "Epoch: 20 | Batch_idx: 190 |  Loss: (1.0127) | Acc: (92.60%) (22640/24448)\n",
            "Epoch: 20 | Batch_idx: 200 |  Loss: (1.0128) | Acc: (92.60%) (23824/25728)\n",
            "Epoch: 20 | Batch_idx: 210 |  Loss: (1.0123) | Acc: (92.65%) (25022/27008)\n",
            "Epoch: 20 | Batch_idx: 220 |  Loss: (1.0119) | Acc: (92.68%) (26218/28288)\n",
            "Epoch: 20 | Batch_idx: 230 |  Loss: (1.0119) | Acc: (92.67%) (27400/29568)\n",
            "Epoch: 20 | Batch_idx: 240 |  Loss: (1.0120) | Acc: (92.65%) (28582/30848)\n",
            "Epoch: 20 | Batch_idx: 250 |  Loss: (1.0118) | Acc: (92.65%) (29767/32128)\n",
            "Epoch: 20 | Batch_idx: 260 |  Loss: (1.0119) | Acc: (92.64%) (30948/33408)\n",
            "Epoch: 20 | Batch_idx: 270 |  Loss: (1.0122) | Acc: (92.62%) (32127/34688)\n",
            "Epoch: 20 | Batch_idx: 280 |  Loss: (1.0124) | Acc: (92.61%) (33310/35968)\n",
            "Epoch: 20 | Batch_idx: 290 |  Loss: (1.0125) | Acc: (92.60%) (34491/37248)\n",
            "Epoch: 20 | Batch_idx: 300 |  Loss: (1.0125) | Acc: (92.59%) (35675/38528)\n",
            "Epoch: 20 | Batch_idx: 310 |  Loss: (1.0126) | Acc: (92.60%) (36862/39808)\n",
            "Epoch: 20 | Batch_idx: 320 |  Loss: (1.0128) | Acc: (92.58%) (38040/41088)\n",
            "Epoch: 20 | Batch_idx: 330 |  Loss: (1.0124) | Acc: (92.61%) (39235/42368)\n",
            "Epoch: 20 | Batch_idx: 340 |  Loss: (1.0130) | Acc: (92.59%) (40414/43648)\n",
            "Epoch: 20 | Batch_idx: 350 |  Loss: (1.0127) | Acc: (92.59%) (41597/44928)\n",
            "Epoch: 20 | Batch_idx: 360 |  Loss: (1.0134) | Acc: (92.54%) (42761/46208)\n",
            "Epoch: 20 | Batch_idx: 370 |  Loss: (1.0134) | Acc: (92.53%) (43942/47488)\n",
            "Epoch: 20 | Batch_idx: 380 |  Loss: (1.0132) | Acc: (92.54%) (45128/48768)\n",
            "Epoch: 20 | Batch_idx: 390 |  Loss: (1.0136) | Acc: (92.51%) (46255/50000)\n",
            "# TEST : Loss: (1.0898) | Acc: (88.47%) (8847/10000)\n",
            "Epoch: 21 | Batch_idx: 0 |  Loss: (1.0593) | Acc: (88.28%) (113/128)\n",
            "Epoch: 21 | Batch_idx: 10 |  Loss: (1.0167) | Acc: (92.33%) (1300/1408)\n",
            "Epoch: 21 | Batch_idx: 20 |  Loss: (1.0087) | Acc: (92.49%) (2486/2688)\n",
            "Epoch: 21 | Batch_idx: 30 |  Loss: (1.0042) | Acc: (92.84%) (3684/3968)\n",
            "Epoch: 21 | Batch_idx: 40 |  Loss: (1.0016) | Acc: (93.20%) (4891/5248)\n",
            "Epoch: 21 | Batch_idx: 50 |  Loss: (1.0068) | Acc: (92.85%) (6061/6528)\n",
            "Epoch: 21 | Batch_idx: 60 |  Loss: (1.0045) | Acc: (92.88%) (7252/7808)\n",
            "Epoch: 21 | Batch_idx: 70 |  Loss: (1.0055) | Acc: (92.90%) (8443/9088)\n",
            "Epoch: 21 | Batch_idx: 80 |  Loss: (1.0054) | Acc: (92.87%) (9629/10368)\n",
            "Epoch: 21 | Batch_idx: 90 |  Loss: (1.0052) | Acc: (92.87%) (10818/11648)\n",
            "Epoch: 21 | Batch_idx: 100 |  Loss: (1.0044) | Acc: (92.92%) (12013/12928)\n",
            "Epoch: 21 | Batch_idx: 110 |  Loss: (1.0026) | Acc: (93.04%) (13219/14208)\n",
            "Epoch: 21 | Batch_idx: 120 |  Loss: (1.0015) | Acc: (93.10%) (14419/15488)\n",
            "Epoch: 21 | Batch_idx: 130 |  Loss: (1.0010) | Acc: (93.14%) (15618/16768)\n",
            "Epoch: 21 | Batch_idx: 140 |  Loss: (1.0008) | Acc: (93.17%) (16816/18048)\n",
            "Epoch: 21 | Batch_idx: 150 |  Loss: (1.0010) | Acc: (93.18%) (18009/19328)\n",
            "Epoch: 21 | Batch_idx: 160 |  Loss: (1.0008) | Acc: (93.20%) (19207/20608)\n",
            "Epoch: 21 | Batch_idx: 170 |  Loss: (1.0007) | Acc: (93.22%) (20403/21888)\n",
            "Epoch: 21 | Batch_idx: 180 |  Loss: (1.0024) | Acc: (93.11%) (21571/23168)\n",
            "Epoch: 21 | Batch_idx: 190 |  Loss: (1.0022) | Acc: (93.14%) (22771/24448)\n",
            "Epoch: 21 | Batch_idx: 200 |  Loss: (1.0025) | Acc: (93.16%) (23968/25728)\n",
            "Epoch: 21 | Batch_idx: 210 |  Loss: (1.0028) | Acc: (93.15%) (25158/27008)\n",
            "Epoch: 21 | Batch_idx: 220 |  Loss: (1.0033) | Acc: (93.10%) (26336/28288)\n",
            "Epoch: 21 | Batch_idx: 230 |  Loss: (1.0041) | Acc: (93.04%) (27510/29568)\n",
            "Epoch: 21 | Batch_idx: 240 |  Loss: (1.0044) | Acc: (93.02%) (28696/30848)\n",
            "Epoch: 21 | Batch_idx: 250 |  Loss: (1.0044) | Acc: (93.01%) (29881/32128)\n",
            "Epoch: 21 | Batch_idx: 260 |  Loss: (1.0046) | Acc: (93.01%) (31073/33408)\n",
            "Epoch: 21 | Batch_idx: 270 |  Loss: (1.0054) | Acc: (92.96%) (32247/34688)\n",
            "Epoch: 21 | Batch_idx: 280 |  Loss: (1.0053) | Acc: (92.97%) (33440/35968)\n",
            "Epoch: 21 | Batch_idx: 290 |  Loss: (1.0048) | Acc: (93.01%) (34644/37248)\n",
            "Epoch: 21 | Batch_idx: 300 |  Loss: (1.0045) | Acc: (93.02%) (35838/38528)\n",
            "Epoch: 21 | Batch_idx: 310 |  Loss: (1.0045) | Acc: (93.00%) (37020/39808)\n",
            "Epoch: 21 | Batch_idx: 320 |  Loss: (1.0044) | Acc: (93.02%) (38220/41088)\n",
            "Epoch: 21 | Batch_idx: 330 |  Loss: (1.0044) | Acc: (93.01%) (39408/42368)\n",
            "Epoch: 21 | Batch_idx: 340 |  Loss: (1.0045) | Acc: (92.99%) (40590/43648)\n",
            "Epoch: 21 | Batch_idx: 350 |  Loss: (1.0051) | Acc: (92.95%) (41760/44928)\n",
            "Epoch: 21 | Batch_idx: 360 |  Loss: (1.0057) | Acc: (92.92%) (42938/46208)\n",
            "Epoch: 21 | Batch_idx: 370 |  Loss: (1.0051) | Acc: (92.96%) (44147/47488)\n",
            "Epoch: 21 | Batch_idx: 380 |  Loss: (1.0049) | Acc: (92.97%) (45340/48768)\n",
            "Epoch: 21 | Batch_idx: 390 |  Loss: (1.0052) | Acc: (92.96%) (46478/50000)\n",
            "# TEST : Loss: (1.0985) | Acc: (87.88%) (8788/10000)\n",
            "Epoch: 22 | Batch_idx: 0 |  Loss: (1.0777) | Acc: (88.28%) (113/128)\n",
            "Epoch: 22 | Batch_idx: 10 |  Loss: (1.0211) | Acc: (91.97%) (1295/1408)\n",
            "Epoch: 22 | Batch_idx: 20 |  Loss: (1.0033) | Acc: (93.27%) (2507/2688)\n",
            "Epoch: 22 | Batch_idx: 30 |  Loss: (1.0065) | Acc: (93.04%) (3692/3968)\n",
            "Epoch: 22 | Batch_idx: 40 |  Loss: (1.0045) | Acc: (93.06%) (4884/5248)\n",
            "Epoch: 22 | Batch_idx: 50 |  Loss: (1.0047) | Acc: (92.98%) (6070/6528)\n",
            "Epoch: 22 | Batch_idx: 60 |  Loss: (1.0015) | Acc: (93.11%) (7270/7808)\n",
            "Epoch: 22 | Batch_idx: 70 |  Loss: (0.9988) | Acc: (93.25%) (8475/9088)\n",
            "Epoch: 22 | Batch_idx: 80 |  Loss: (0.9983) | Acc: (93.24%) (9667/10368)\n",
            "Epoch: 22 | Batch_idx: 90 |  Loss: (0.9999) | Acc: (93.16%) (10851/11648)\n",
            "Epoch: 22 | Batch_idx: 100 |  Loss: (0.9992) | Acc: (93.17%) (12045/12928)\n",
            "Epoch: 22 | Batch_idx: 110 |  Loss: (0.9999) | Acc: (93.08%) (13225/14208)\n",
            "Epoch: 22 | Batch_idx: 120 |  Loss: (1.0013) | Acc: (93.01%) (14405/15488)\n",
            "Epoch: 22 | Batch_idx: 130 |  Loss: (1.0012) | Acc: (92.96%) (15588/16768)\n",
            "Epoch: 22 | Batch_idx: 140 |  Loss: (1.0004) | Acc: (93.00%) (16785/18048)\n",
            "Epoch: 22 | Batch_idx: 150 |  Loss: (1.0004) | Acc: (93.01%) (17977/19328)\n",
            "Epoch: 22 | Batch_idx: 160 |  Loss: (0.9999) | Acc: (93.04%) (19173/20608)\n",
            "Epoch: 22 | Batch_idx: 170 |  Loss: (1.0002) | Acc: (93.02%) (20360/21888)\n",
            "Epoch: 22 | Batch_idx: 180 |  Loss: (1.0009) | Acc: (92.98%) (21542/23168)\n",
            "Epoch: 22 | Batch_idx: 190 |  Loss: (1.0016) | Acc: (93.01%) (22739/24448)\n",
            "Epoch: 22 | Batch_idx: 200 |  Loss: (1.0014) | Acc: (93.03%) (23936/25728)\n",
            "Epoch: 22 | Batch_idx: 210 |  Loss: (1.0002) | Acc: (93.11%) (25147/27008)\n",
            "Epoch: 22 | Batch_idx: 220 |  Loss: (1.0005) | Acc: (93.14%) (26348/28288)\n",
            "Epoch: 22 | Batch_idx: 230 |  Loss: (1.0003) | Acc: (93.15%) (27544/29568)\n",
            "Epoch: 22 | Batch_idx: 240 |  Loss: (1.0009) | Acc: (93.10%) (28721/30848)\n",
            "Epoch: 22 | Batch_idx: 250 |  Loss: (1.0015) | Acc: (93.10%) (29910/32128)\n",
            "Epoch: 22 | Batch_idx: 260 |  Loss: (1.0007) | Acc: (93.15%) (31120/33408)\n",
            "Epoch: 22 | Batch_idx: 270 |  Loss: (1.0009) | Acc: (93.14%) (32309/34688)\n",
            "Epoch: 22 | Batch_idx: 280 |  Loss: (1.0019) | Acc: (93.07%) (33475/35968)\n",
            "Epoch: 22 | Batch_idx: 290 |  Loss: (1.0015) | Acc: (93.09%) (34673/37248)\n",
            "Epoch: 22 | Batch_idx: 300 |  Loss: (1.0021) | Acc: (93.05%) (35849/38528)\n",
            "Epoch: 22 | Batch_idx: 310 |  Loss: (1.0023) | Acc: (93.02%) (37031/39808)\n",
            "Epoch: 22 | Batch_idx: 320 |  Loss: (1.0017) | Acc: (93.05%) (38231/41088)\n",
            "Epoch: 22 | Batch_idx: 330 |  Loss: (1.0018) | Acc: (93.04%) (39418/42368)\n",
            "Epoch: 22 | Batch_idx: 340 |  Loss: (1.0013) | Acc: (93.06%) (40619/43648)\n",
            "Epoch: 22 | Batch_idx: 350 |  Loss: (1.0016) | Acc: (93.05%) (41807/44928)\n",
            "Epoch: 22 | Batch_idx: 360 |  Loss: (1.0021) | Acc: (93.03%) (42988/46208)\n",
            "Epoch: 22 | Batch_idx: 370 |  Loss: (1.0020) | Acc: (93.05%) (44188/47488)\n",
            "Epoch: 22 | Batch_idx: 380 |  Loss: (1.0021) | Acc: (93.06%) (45383/48768)\n",
            "Epoch: 22 | Batch_idx: 390 |  Loss: (1.0019) | Acc: (93.07%) (46534/50000)\n",
            "# TEST : Loss: (1.0986) | Acc: (87.89%) (8789/10000)\n",
            "Epoch: 23 | Batch_idx: 0 |  Loss: (1.0077) | Acc: (92.97%) (119/128)\n",
            "Epoch: 23 | Batch_idx: 10 |  Loss: (0.9847) | Acc: (94.03%) (1324/1408)\n",
            "Epoch: 23 | Batch_idx: 20 |  Loss: (0.9926) | Acc: (93.45%) (2512/2688)\n",
            "Epoch: 23 | Batch_idx: 30 |  Loss: (0.9888) | Acc: (93.75%) (3720/3968)\n",
            "Epoch: 23 | Batch_idx: 40 |  Loss: (0.9899) | Acc: (93.69%) (4917/5248)\n",
            "Epoch: 23 | Batch_idx: 50 |  Loss: (0.9868) | Acc: (93.83%) (6125/6528)\n",
            "Epoch: 23 | Batch_idx: 60 |  Loss: (0.9855) | Acc: (93.87%) (7329/7808)\n",
            "Epoch: 23 | Batch_idx: 70 |  Loss: (0.9851) | Acc: (93.95%) (8538/9088)\n",
            "Epoch: 23 | Batch_idx: 80 |  Loss: (0.9856) | Acc: (93.97%) (9743/10368)\n",
            "Epoch: 23 | Batch_idx: 90 |  Loss: (0.9844) | Acc: (94.08%) (10959/11648)\n",
            "Epoch: 23 | Batch_idx: 100 |  Loss: (0.9865) | Acc: (94.00%) (12152/12928)\n",
            "Epoch: 23 | Batch_idx: 110 |  Loss: (0.9878) | Acc: (93.88%) (13339/14208)\n",
            "Epoch: 23 | Batch_idx: 120 |  Loss: (0.9891) | Acc: (93.80%) (14528/15488)\n",
            "Epoch: 23 | Batch_idx: 130 |  Loss: (0.9892) | Acc: (93.76%) (15721/16768)\n",
            "Epoch: 23 | Batch_idx: 140 |  Loss: (0.9890) | Acc: (93.79%) (16927/18048)\n",
            "Epoch: 23 | Batch_idx: 150 |  Loss: (0.9893) | Acc: (93.75%) (18120/19328)\n",
            "Epoch: 23 | Batch_idx: 160 |  Loss: (0.9899) | Acc: (93.71%) (19312/20608)\n",
            "Epoch: 23 | Batch_idx: 170 |  Loss: (0.9905) | Acc: (93.69%) (20506/21888)\n",
            "Epoch: 23 | Batch_idx: 180 |  Loss: (0.9913) | Acc: (93.67%) (21701/23168)\n",
            "Epoch: 23 | Batch_idx: 190 |  Loss: (0.9921) | Acc: (93.66%) (22897/24448)\n",
            "Epoch: 23 | Batch_idx: 200 |  Loss: (0.9927) | Acc: (93.61%) (24084/25728)\n",
            "Epoch: 23 | Batch_idx: 210 |  Loss: (0.9925) | Acc: (93.61%) (25281/27008)\n",
            "Epoch: 23 | Batch_idx: 220 |  Loss: (0.9933) | Acc: (93.57%) (26470/28288)\n",
            "Epoch: 23 | Batch_idx: 230 |  Loss: (0.9935) | Acc: (93.59%) (27674/29568)\n",
            "Epoch: 23 | Batch_idx: 240 |  Loss: (0.9936) | Acc: (93.56%) (28862/30848)\n",
            "Epoch: 23 | Batch_idx: 250 |  Loss: (0.9937) | Acc: (93.55%) (30055/32128)\n",
            "Epoch: 23 | Batch_idx: 260 |  Loss: (0.9938) | Acc: (93.54%) (31249/33408)\n",
            "Epoch: 23 | Batch_idx: 270 |  Loss: (0.9937) | Acc: (93.54%) (32448/34688)\n",
            "Epoch: 23 | Batch_idx: 280 |  Loss: (0.9932) | Acc: (93.57%) (33656/35968)\n",
            "Epoch: 23 | Batch_idx: 290 |  Loss: (0.9937) | Acc: (93.55%) (34846/37248)\n",
            "Epoch: 23 | Batch_idx: 300 |  Loss: (0.9940) | Acc: (93.54%) (36040/38528)\n",
            "Epoch: 23 | Batch_idx: 310 |  Loss: (0.9949) | Acc: (93.50%) (37220/39808)\n",
            "Epoch: 23 | Batch_idx: 320 |  Loss: (0.9956) | Acc: (93.46%) (38402/41088)\n",
            "Epoch: 23 | Batch_idx: 330 |  Loss: (0.9966) | Acc: (93.42%) (39579/42368)\n",
            "Epoch: 23 | Batch_idx: 340 |  Loss: (0.9974) | Acc: (93.37%) (40754/43648)\n",
            "Epoch: 23 | Batch_idx: 350 |  Loss: (0.9981) | Acc: (93.34%) (41934/44928)\n",
            "Epoch: 23 | Batch_idx: 360 |  Loss: (0.9983) | Acc: (93.31%) (43117/46208)\n",
            "Epoch: 23 | Batch_idx: 370 |  Loss: (0.9985) | Acc: (93.29%) (44303/47488)\n",
            "Epoch: 23 | Batch_idx: 380 |  Loss: (0.9985) | Acc: (93.28%) (45493/48768)\n",
            "Epoch: 23 | Batch_idx: 390 |  Loss: (0.9985) | Acc: (93.30%) (46652/50000)\n",
            "# TEST : Loss: (1.0749) | Acc: (89.30%) (8930/10000)\n",
            "Epoch: 24 | Batch_idx: 0 |  Loss: (0.9847) | Acc: (92.19%) (118/128)\n",
            "Epoch: 24 | Batch_idx: 10 |  Loss: (0.9898) | Acc: (93.25%) (1313/1408)\n",
            "Epoch: 24 | Batch_idx: 20 |  Loss: (0.9850) | Acc: (94.01%) (2527/2688)\n",
            "Epoch: 24 | Batch_idx: 30 |  Loss: (0.9851) | Acc: (94.00%) (3730/3968)\n",
            "Epoch: 24 | Batch_idx: 40 |  Loss: (0.9878) | Acc: (93.92%) (4929/5248)\n",
            "Epoch: 24 | Batch_idx: 50 |  Loss: (0.9903) | Acc: (93.77%) (6121/6528)\n",
            "Epoch: 24 | Batch_idx: 60 |  Loss: (0.9911) | Acc: (93.75%) (7320/7808)\n",
            "Epoch: 24 | Batch_idx: 70 |  Loss: (0.9909) | Acc: (93.72%) (8517/9088)\n",
            "Epoch: 24 | Batch_idx: 80 |  Loss: (0.9905) | Acc: (93.66%) (9711/10368)\n",
            "Epoch: 24 | Batch_idx: 90 |  Loss: (0.9903) | Acc: (93.63%) (10906/11648)\n",
            "Epoch: 24 | Batch_idx: 100 |  Loss: (0.9894) | Acc: (93.70%) (12113/12928)\n",
            "Epoch: 24 | Batch_idx: 110 |  Loss: (0.9893) | Acc: (93.73%) (13317/14208)\n",
            "Epoch: 24 | Batch_idx: 120 |  Loss: (0.9888) | Acc: (93.80%) (14527/15488)\n",
            "Epoch: 24 | Batch_idx: 130 |  Loss: (0.9896) | Acc: (93.75%) (15720/16768)\n",
            "Epoch: 24 | Batch_idx: 140 |  Loss: (0.9893) | Acc: (93.77%) (16924/18048)\n",
            "Epoch: 24 | Batch_idx: 150 |  Loss: (0.9891) | Acc: (93.82%) (18133/19328)\n",
            "Epoch: 24 | Batch_idx: 160 |  Loss: (0.9893) | Acc: (93.81%) (19333/20608)\n",
            "Epoch: 24 | Batch_idx: 170 |  Loss: (0.9884) | Acc: (93.85%) (20542/21888)\n",
            "Epoch: 24 | Batch_idx: 180 |  Loss: (0.9885) | Acc: (93.84%) (21740/23168)\n",
            "Epoch: 24 | Batch_idx: 190 |  Loss: (0.9886) | Acc: (93.83%) (22939/24448)\n",
            "Epoch: 24 | Batch_idx: 200 |  Loss: (0.9883) | Acc: (93.87%) (24152/25728)\n",
            "Epoch: 24 | Batch_idx: 210 |  Loss: (0.9889) | Acc: (93.87%) (25352/27008)\n",
            "Epoch: 24 | Batch_idx: 220 |  Loss: (0.9897) | Acc: (93.84%) (26546/28288)\n",
            "Epoch: 24 | Batch_idx: 230 |  Loss: (0.9904) | Acc: (93.79%) (27731/29568)\n",
            "Epoch: 24 | Batch_idx: 240 |  Loss: (0.9906) | Acc: (93.80%) (28935/30848)\n",
            "Epoch: 24 | Batch_idx: 250 |  Loss: (0.9906) | Acc: (93.78%) (30131/32128)\n",
            "Epoch: 24 | Batch_idx: 260 |  Loss: (0.9916) | Acc: (93.73%) (31314/33408)\n",
            "Epoch: 24 | Batch_idx: 270 |  Loss: (0.9915) | Acc: (93.73%) (32512/34688)\n",
            "Epoch: 24 | Batch_idx: 280 |  Loss: (0.9922) | Acc: (93.68%) (33696/35968)\n",
            "Epoch: 24 | Batch_idx: 290 |  Loss: (0.9925) | Acc: (93.67%) (34891/37248)\n",
            "Epoch: 24 | Batch_idx: 300 |  Loss: (0.9929) | Acc: (93.64%) (36079/38528)\n",
            "Epoch: 24 | Batch_idx: 310 |  Loss: (0.9931) | Acc: (93.63%) (37271/39808)\n",
            "Epoch: 24 | Batch_idx: 320 |  Loss: (0.9933) | Acc: (93.63%) (38469/41088)\n",
            "Epoch: 24 | Batch_idx: 330 |  Loss: (0.9935) | Acc: (93.58%) (39649/42368)\n",
            "Epoch: 24 | Batch_idx: 340 |  Loss: (0.9938) | Acc: (93.58%) (40847/43648)\n",
            "Epoch: 24 | Batch_idx: 350 |  Loss: (0.9943) | Acc: (93.54%) (42025/44928)\n",
            "Epoch: 24 | Batch_idx: 360 |  Loss: (0.9944) | Acc: (93.53%) (43220/46208)\n",
            "Epoch: 24 | Batch_idx: 370 |  Loss: (0.9947) | Acc: (93.53%) (44414/47488)\n",
            "Epoch: 24 | Batch_idx: 380 |  Loss: (0.9948) | Acc: (93.51%) (45602/48768)\n",
            "Epoch: 24 | Batch_idx: 390 |  Loss: (0.9948) | Acc: (93.50%) (46751/50000)\n",
            "# TEST : Loss: (1.0963) | Acc: (88.28%) (8828/10000)\n",
            "Epoch: 25 | Batch_idx: 0 |  Loss: (1.0030) | Acc: (93.75%) (120/128)\n",
            "Epoch: 25 | Batch_idx: 10 |  Loss: (0.9824) | Acc: (94.46%) (1330/1408)\n",
            "Epoch: 25 | Batch_idx: 20 |  Loss: (0.9788) | Acc: (94.72%) (2546/2688)\n",
            "Epoch: 25 | Batch_idx: 30 |  Loss: (0.9831) | Acc: (94.25%) (3740/3968)\n",
            "Epoch: 25 | Batch_idx: 40 |  Loss: (0.9854) | Acc: (94.09%) (4938/5248)\n",
            "Epoch: 25 | Batch_idx: 50 |  Loss: (0.9866) | Acc: (94.01%) (6137/6528)\n",
            "Epoch: 25 | Batch_idx: 60 |  Loss: (0.9854) | Acc: (94.16%) (7352/7808)\n",
            "Epoch: 25 | Batch_idx: 70 |  Loss: (0.9861) | Acc: (94.17%) (8558/9088)\n",
            "Epoch: 25 | Batch_idx: 80 |  Loss: (0.9877) | Acc: (93.99%) (9745/10368)\n",
            "Epoch: 25 | Batch_idx: 90 |  Loss: (0.9883) | Acc: (93.96%) (10945/11648)\n",
            "Epoch: 25 | Batch_idx: 100 |  Loss: (0.9887) | Acc: (93.90%) (12139/12928)\n",
            "Epoch: 25 | Batch_idx: 110 |  Loss: (0.9886) | Acc: (93.94%) (13347/14208)\n",
            "Epoch: 25 | Batch_idx: 120 |  Loss: (0.9884) | Acc: (93.96%) (14553/15488)\n",
            "Epoch: 25 | Batch_idx: 130 |  Loss: (0.9885) | Acc: (93.95%) (15753/16768)\n",
            "Epoch: 25 | Batch_idx: 140 |  Loss: (0.9880) | Acc: (93.99%) (16963/18048)\n",
            "Epoch: 25 | Batch_idx: 150 |  Loss: (0.9870) | Acc: (94.03%) (18175/19328)\n",
            "Epoch: 25 | Batch_idx: 160 |  Loss: (0.9855) | Acc: (94.09%) (19391/20608)\n",
            "Epoch: 25 | Batch_idx: 170 |  Loss: (0.9862) | Acc: (94.04%) (20583/21888)\n",
            "Epoch: 25 | Batch_idx: 180 |  Loss: (0.9861) | Acc: (94.02%) (21783/23168)\n",
            "Epoch: 25 | Batch_idx: 190 |  Loss: (0.9861) | Acc: (94.02%) (22986/24448)\n",
            "Epoch: 25 | Batch_idx: 200 |  Loss: (0.9855) | Acc: (94.08%) (24204/25728)\n",
            "Epoch: 25 | Batch_idx: 210 |  Loss: (0.9853) | Acc: (94.07%) (25407/27008)\n",
            "Epoch: 25 | Batch_idx: 220 |  Loss: (0.9850) | Acc: (94.10%) (26618/28288)\n",
            "Epoch: 25 | Batch_idx: 230 |  Loss: (0.9859) | Acc: (94.03%) (27804/29568)\n",
            "Epoch: 25 | Batch_idx: 240 |  Loss: (0.9862) | Acc: (94.04%) (29008/30848)\n",
            "Epoch: 25 | Batch_idx: 250 |  Loss: (0.9862) | Acc: (94.02%) (30207/32128)\n",
            "Epoch: 25 | Batch_idx: 260 |  Loss: (0.9865) | Acc: (94.00%) (31404/33408)\n",
            "Epoch: 25 | Batch_idx: 270 |  Loss: (0.9864) | Acc: (94.00%) (32606/34688)\n",
            "Epoch: 25 | Batch_idx: 280 |  Loss: (0.9868) | Acc: (93.96%) (33796/35968)\n",
            "Epoch: 25 | Batch_idx: 290 |  Loss: (0.9870) | Acc: (93.94%) (34992/37248)\n",
            "Epoch: 25 | Batch_idx: 300 |  Loss: (0.9875) | Acc: (93.92%) (36184/38528)\n",
            "Epoch: 25 | Batch_idx: 310 |  Loss: (0.9882) | Acc: (93.88%) (37372/39808)\n",
            "Epoch: 25 | Batch_idx: 320 |  Loss: (0.9883) | Acc: (93.88%) (38572/41088)\n",
            "Epoch: 25 | Batch_idx: 330 |  Loss: (0.9882) | Acc: (93.88%) (39774/42368)\n",
            "Epoch: 25 | Batch_idx: 340 |  Loss: (0.9886) | Acc: (93.86%) (40969/43648)\n",
            "Epoch: 25 | Batch_idx: 350 |  Loss: (0.9891) | Acc: (93.83%) (42158/44928)\n",
            "Epoch: 25 | Batch_idx: 360 |  Loss: (0.9897) | Acc: (93.79%) (43339/46208)\n",
            "Epoch: 25 | Batch_idx: 370 |  Loss: (0.9906) | Acc: (93.77%) (44528/47488)\n",
            "Epoch: 25 | Batch_idx: 380 |  Loss: (0.9904) | Acc: (93.78%) (45736/48768)\n",
            "Epoch: 25 | Batch_idx: 390 |  Loss: (0.9906) | Acc: (93.79%) (46896/50000)\n",
            "# TEST : Loss: (1.0909) | Acc: (87.89%) (8789/10000)\n",
            "Epoch: 26 | Batch_idx: 0 |  Loss: (0.9988) | Acc: (92.97%) (119/128)\n",
            "Epoch: 26 | Batch_idx: 10 |  Loss: (0.9822) | Acc: (94.03%) (1324/1408)\n",
            "Epoch: 26 | Batch_idx: 20 |  Loss: (0.9743) | Acc: (94.46%) (2539/2688)\n",
            "Epoch: 26 | Batch_idx: 30 |  Loss: (0.9779) | Acc: (94.28%) (3741/3968)\n",
            "Epoch: 26 | Batch_idx: 40 |  Loss: (0.9746) | Acc: (94.65%) (4967/5248)\n",
            "Epoch: 26 | Batch_idx: 50 |  Loss: (0.9769) | Acc: (94.50%) (6169/6528)\n",
            "Epoch: 26 | Batch_idx: 60 |  Loss: (0.9782) | Acc: (94.49%) (7378/7808)\n",
            "Epoch: 26 | Batch_idx: 70 |  Loss: (0.9788) | Acc: (94.41%) (8580/9088)\n",
            "Epoch: 26 | Batch_idx: 80 |  Loss: (0.9804) | Acc: (94.25%) (9772/10368)\n",
            "Epoch: 26 | Batch_idx: 90 |  Loss: (0.9826) | Acc: (94.18%) (10970/11648)\n",
            "Epoch: 26 | Batch_idx: 100 |  Loss: (0.9846) | Acc: (94.07%) (12162/12928)\n",
            "Epoch: 26 | Batch_idx: 110 |  Loss: (0.9841) | Acc: (94.09%) (13369/14208)\n",
            "Epoch: 26 | Batch_idx: 120 |  Loss: (0.9837) | Acc: (94.13%) (14579/15488)\n",
            "Epoch: 26 | Batch_idx: 130 |  Loss: (0.9831) | Acc: (94.22%) (15799/16768)\n",
            "Epoch: 26 | Batch_idx: 140 |  Loss: (0.9834) | Acc: (94.19%) (17000/18048)\n",
            "Epoch: 26 | Batch_idx: 150 |  Loss: (0.9831) | Acc: (94.20%) (18207/19328)\n",
            "Epoch: 26 | Batch_idx: 160 |  Loss: (0.9829) | Acc: (94.22%) (19416/20608)\n",
            "Epoch: 26 | Batch_idx: 170 |  Loss: (0.9826) | Acc: (94.23%) (20625/21888)\n",
            "Epoch: 26 | Batch_idx: 180 |  Loss: (0.9827) | Acc: (94.22%) (21828/23168)\n",
            "Epoch: 26 | Batch_idx: 190 |  Loss: (0.9831) | Acc: (94.21%) (23033/24448)\n",
            "Epoch: 26 | Batch_idx: 200 |  Loss: (0.9829) | Acc: (94.22%) (24241/25728)\n",
            "Epoch: 26 | Batch_idx: 210 |  Loss: (0.9833) | Acc: (94.18%) (25437/27008)\n",
            "Epoch: 26 | Batch_idx: 220 |  Loss: (0.9834) | Acc: (94.20%) (26647/28288)\n",
            "Epoch: 26 | Batch_idx: 230 |  Loss: (0.9840) | Acc: (94.15%) (27837/29568)\n",
            "Epoch: 26 | Batch_idx: 240 |  Loss: (0.9846) | Acc: (94.12%) (29035/30848)\n",
            "Epoch: 26 | Batch_idx: 250 |  Loss: (0.9847) | Acc: (94.09%) (30229/32128)\n",
            "Epoch: 26 | Batch_idx: 260 |  Loss: (0.9849) | Acc: (94.08%) (31429/33408)\n",
            "Epoch: 26 | Batch_idx: 270 |  Loss: (0.9843) | Acc: (94.10%) (32641/34688)\n",
            "Epoch: 26 | Batch_idx: 280 |  Loss: (0.9841) | Acc: (94.11%) (33848/35968)\n",
            "Epoch: 26 | Batch_idx: 290 |  Loss: (0.9846) | Acc: (94.08%) (35042/37248)\n",
            "Epoch: 26 | Batch_idx: 300 |  Loss: (0.9848) | Acc: (94.07%) (36244/38528)\n",
            "Epoch: 26 | Batch_idx: 310 |  Loss: (0.9850) | Acc: (94.07%) (37448/39808)\n",
            "Epoch: 26 | Batch_idx: 320 |  Loss: (0.9865) | Acc: (93.99%) (38617/41088)\n",
            "Epoch: 26 | Batch_idx: 330 |  Loss: (0.9870) | Acc: (93.96%) (39809/42368)\n",
            "Epoch: 26 | Batch_idx: 340 |  Loss: (0.9878) | Acc: (93.90%) (40984/43648)\n",
            "Epoch: 26 | Batch_idx: 350 |  Loss: (0.9880) | Acc: (93.89%) (42182/44928)\n",
            "Epoch: 26 | Batch_idx: 360 |  Loss: (0.9888) | Acc: (93.87%) (43376/46208)\n",
            "Epoch: 26 | Batch_idx: 370 |  Loss: (0.9888) | Acc: (93.89%) (44587/47488)\n",
            "Epoch: 26 | Batch_idx: 380 |  Loss: (0.9891) | Acc: (93.87%) (45778/48768)\n",
            "Epoch: 26 | Batch_idx: 390 |  Loss: (0.9892) | Acc: (93.86%) (46932/50000)\n",
            "# TEST : Loss: (1.0869) | Acc: (88.41%) (8841/10000)\n",
            "Epoch: 27 | Batch_idx: 0 |  Loss: (0.9764) | Acc: (95.31%) (122/128)\n",
            "Epoch: 27 | Batch_idx: 10 |  Loss: (0.9887) | Acc: (93.39%) (1315/1408)\n",
            "Epoch: 27 | Batch_idx: 20 |  Loss: (0.9952) | Acc: (93.30%) (2508/2688)\n",
            "Epoch: 27 | Batch_idx: 30 |  Loss: (0.9911) | Acc: (93.50%) (3710/3968)\n",
            "Epoch: 27 | Batch_idx: 40 |  Loss: (0.9845) | Acc: (93.90%) (4928/5248)\n",
            "Epoch: 27 | Batch_idx: 50 |  Loss: (0.9797) | Acc: (94.24%) (6152/6528)\n",
            "Epoch: 27 | Batch_idx: 60 |  Loss: (0.9792) | Acc: (94.29%) (7362/7808)\n",
            "Epoch: 27 | Batch_idx: 70 |  Loss: (0.9771) | Acc: (94.45%) (8584/9088)\n",
            "Epoch: 27 | Batch_idx: 80 |  Loss: (0.9777) | Acc: (94.46%) (9794/10368)\n",
            "Epoch: 27 | Batch_idx: 90 |  Loss: (0.9776) | Acc: (94.43%) (10999/11648)\n",
            "Epoch: 27 | Batch_idx: 100 |  Loss: (0.9776) | Acc: (94.42%) (12206/12928)\n",
            "Epoch: 27 | Batch_idx: 110 |  Loss: (0.9766) | Acc: (94.50%) (13427/14208)\n",
            "Epoch: 27 | Batch_idx: 120 |  Loss: (0.9772) | Acc: (94.46%) (14630/15488)\n",
            "Epoch: 27 | Batch_idx: 130 |  Loss: (0.9779) | Acc: (94.41%) (15831/16768)\n",
            "Epoch: 27 | Batch_idx: 140 |  Loss: (0.9781) | Acc: (94.41%) (17039/18048)\n",
            "Epoch: 27 | Batch_idx: 150 |  Loss: (0.9782) | Acc: (94.41%) (18248/19328)\n",
            "Epoch: 27 | Batch_idx: 160 |  Loss: (0.9783) | Acc: (94.40%) (19454/20608)\n",
            "Epoch: 27 | Batch_idx: 170 |  Loss: (0.9781) | Acc: (94.45%) (20674/21888)\n",
            "Epoch: 27 | Batch_idx: 180 |  Loss: (0.9776) | Acc: (94.48%) (21889/23168)\n",
            "Epoch: 27 | Batch_idx: 190 |  Loss: (0.9778) | Acc: (94.46%) (23094/24448)\n",
            "Epoch: 27 | Batch_idx: 200 |  Loss: (0.9787) | Acc: (94.43%) (24294/25728)\n",
            "Epoch: 27 | Batch_idx: 210 |  Loss: (0.9786) | Acc: (94.41%) (25499/27008)\n",
            "Epoch: 27 | Batch_idx: 220 |  Loss: (0.9791) | Acc: (94.40%) (26704/28288)\n",
            "Epoch: 27 | Batch_idx: 230 |  Loss: (0.9784) | Acc: (94.43%) (27922/29568)\n",
            "Epoch: 27 | Batch_idx: 240 |  Loss: (0.9789) | Acc: (94.41%) (29123/30848)\n",
            "Epoch: 27 | Batch_idx: 250 |  Loss: (0.9789) | Acc: (94.41%) (30331/32128)\n",
            "Epoch: 27 | Batch_idx: 260 |  Loss: (0.9788) | Acc: (94.42%) (31543/33408)\n",
            "Epoch: 27 | Batch_idx: 270 |  Loss: (0.9789) | Acc: (94.40%) (32747/34688)\n",
            "Epoch: 27 | Batch_idx: 280 |  Loss: (0.9790) | Acc: (94.40%) (33952/35968)\n",
            "Epoch: 27 | Batch_idx: 290 |  Loss: (0.9795) | Acc: (94.36%) (35148/37248)\n",
            "Epoch: 27 | Batch_idx: 300 |  Loss: (0.9799) | Acc: (94.32%) (36340/38528)\n",
            "Epoch: 27 | Batch_idx: 310 |  Loss: (0.9804) | Acc: (94.28%) (37531/39808)\n",
            "Epoch: 27 | Batch_idx: 320 |  Loss: (0.9805) | Acc: (94.28%) (38739/41088)\n",
            "Epoch: 27 | Batch_idx: 330 |  Loss: (0.9806) | Acc: (94.27%) (39942/42368)\n",
            "Epoch: 27 | Batch_idx: 340 |  Loss: (0.9808) | Acc: (94.28%) (41150/43648)\n",
            "Epoch: 27 | Batch_idx: 350 |  Loss: (0.9808) | Acc: (94.27%) (42353/44928)\n",
            "Epoch: 27 | Batch_idx: 360 |  Loss: (0.9811) | Acc: (94.25%) (43549/46208)\n",
            "Epoch: 27 | Batch_idx: 370 |  Loss: (0.9814) | Acc: (94.24%) (44751/47488)\n",
            "Epoch: 27 | Batch_idx: 380 |  Loss: (0.9812) | Acc: (94.26%) (45968/48768)\n",
            "Epoch: 27 | Batch_idx: 390 |  Loss: (0.9817) | Acc: (94.24%) (47118/50000)\n",
            "# TEST : Loss: (1.0903) | Acc: (88.32%) (8832/10000)\n",
            "Epoch: 28 | Batch_idx: 0 |  Loss: (1.0276) | Acc: (89.84%) (115/128)\n",
            "Epoch: 28 | Batch_idx: 10 |  Loss: (0.9799) | Acc: (94.25%) (1327/1408)\n",
            "Epoch: 28 | Batch_idx: 20 |  Loss: (0.9763) | Acc: (94.49%) (2540/2688)\n",
            "Epoch: 28 | Batch_idx: 30 |  Loss: (0.9752) | Acc: (94.58%) (3753/3968)\n",
            "Epoch: 28 | Batch_idx: 40 |  Loss: (0.9757) | Acc: (94.70%) (4970/5248)\n",
            "Epoch: 28 | Batch_idx: 50 |  Loss: (0.9759) | Acc: (94.64%) (6178/6528)\n",
            "Epoch: 28 | Batch_idx: 60 |  Loss: (0.9736) | Acc: (94.72%) (7396/7808)\n",
            "Epoch: 28 | Batch_idx: 70 |  Loss: (0.9720) | Acc: (94.85%) (8620/9088)\n",
            "Epoch: 28 | Batch_idx: 80 |  Loss: (0.9720) | Acc: (94.88%) (9837/10368)\n",
            "Epoch: 28 | Batch_idx: 90 |  Loss: (0.9721) | Acc: (94.88%) (11052/11648)\n",
            "Epoch: 28 | Batch_idx: 100 |  Loss: (0.9723) | Acc: (94.84%) (12261/12928)\n",
            "Epoch: 28 | Batch_idx: 110 |  Loss: (0.9716) | Acc: (94.86%) (13477/14208)\n",
            "Epoch: 28 | Batch_idx: 120 |  Loss: (0.9709) | Acc: (94.92%) (14701/15488)\n",
            "Epoch: 28 | Batch_idx: 130 |  Loss: (0.9725) | Acc: (94.79%) (15895/16768)\n",
            "Epoch: 28 | Batch_idx: 140 |  Loss: (0.9728) | Acc: (94.79%) (17107/18048)\n",
            "Epoch: 28 | Batch_idx: 150 |  Loss: (0.9726) | Acc: (94.80%) (18323/19328)\n",
            "Epoch: 28 | Batch_idx: 160 |  Loss: (0.9734) | Acc: (94.74%) (19524/20608)\n",
            "Epoch: 28 | Batch_idx: 170 |  Loss: (0.9742) | Acc: (94.68%) (20724/21888)\n",
            "Epoch: 28 | Batch_idx: 180 |  Loss: (0.9740) | Acc: (94.66%) (21930/23168)\n",
            "Epoch: 28 | Batch_idx: 190 |  Loss: (0.9742) | Acc: (94.65%) (23140/24448)\n",
            "Epoch: 28 | Batch_idx: 200 |  Loss: (0.9746) | Acc: (94.66%) (24353/25728)\n",
            "Epoch: 28 | Batch_idx: 210 |  Loss: (0.9752) | Acc: (94.63%) (25559/27008)\n",
            "Epoch: 28 | Batch_idx: 220 |  Loss: (0.9759) | Acc: (94.62%) (26766/28288)\n",
            "Epoch: 28 | Batch_idx: 230 |  Loss: (0.9761) | Acc: (94.61%) (27974/29568)\n",
            "Epoch: 28 | Batch_idx: 240 |  Loss: (0.9765) | Acc: (94.60%) (29183/30848)\n",
            "Epoch: 28 | Batch_idx: 250 |  Loss: (0.9771) | Acc: (94.57%) (30383/32128)\n",
            "Epoch: 28 | Batch_idx: 260 |  Loss: (0.9779) | Acc: (94.51%) (31574/33408)\n",
            "Epoch: 28 | Batch_idx: 270 |  Loss: (0.9777) | Acc: (94.51%) (32784/34688)\n",
            "Epoch: 28 | Batch_idx: 280 |  Loss: (0.9779) | Acc: (94.50%) (33988/35968)\n",
            "Epoch: 28 | Batch_idx: 290 |  Loss: (0.9787) | Acc: (94.44%) (35176/37248)\n",
            "Epoch: 28 | Batch_idx: 300 |  Loss: (0.9787) | Acc: (94.44%) (36387/38528)\n",
            "Epoch: 28 | Batch_idx: 310 |  Loss: (0.9795) | Acc: (94.40%) (37577/39808)\n",
            "Epoch: 28 | Batch_idx: 320 |  Loss: (0.9797) | Acc: (94.39%) (38783/41088)\n",
            "Epoch: 28 | Batch_idx: 330 |  Loss: (0.9809) | Acc: (94.33%) (39965/42368)\n",
            "Epoch: 28 | Batch_idx: 340 |  Loss: (0.9812) | Acc: (94.31%) (41165/43648)\n",
            "Epoch: 28 | Batch_idx: 350 |  Loss: (0.9818) | Acc: (94.27%) (42352/44928)\n",
            "Epoch: 28 | Batch_idx: 360 |  Loss: (0.9822) | Acc: (94.26%) (43556/46208)\n",
            "Epoch: 28 | Batch_idx: 370 |  Loss: (0.9827) | Acc: (94.22%) (44744/47488)\n",
            "Epoch: 28 | Batch_idx: 380 |  Loss: (0.9831) | Acc: (94.19%) (45934/48768)\n",
            "Epoch: 28 | Batch_idx: 390 |  Loss: (0.9837) | Acc: (94.16%) (47078/50000)\n",
            "# TEST : Loss: (1.0937) | Acc: (88.14%) (8814/10000)\n",
            "Epoch: 29 | Batch_idx: 0 |  Loss: (1.0220) | Acc: (92.19%) (118/128)\n",
            "Epoch: 29 | Batch_idx: 10 |  Loss: (0.9912) | Acc: (94.18%) (1326/1408)\n",
            "Epoch: 29 | Batch_idx: 20 |  Loss: (0.9822) | Acc: (94.46%) (2539/2688)\n",
            "Epoch: 29 | Batch_idx: 30 |  Loss: (0.9763) | Acc: (94.58%) (3753/3968)\n",
            "Epoch: 29 | Batch_idx: 40 |  Loss: (0.9741) | Acc: (94.72%) (4971/5248)\n",
            "Epoch: 29 | Batch_idx: 50 |  Loss: (0.9739) | Acc: (94.78%) (6187/6528)\n",
            "Epoch: 29 | Batch_idx: 60 |  Loss: (0.9743) | Acc: (94.66%) (7391/7808)\n",
            "Epoch: 29 | Batch_idx: 70 |  Loss: (0.9749) | Acc: (94.65%) (8602/9088)\n",
            "Epoch: 29 | Batch_idx: 80 |  Loss: (0.9746) | Acc: (94.61%) (9809/10368)\n",
            "Epoch: 29 | Batch_idx: 90 |  Loss: (0.9761) | Acc: (94.50%) (11007/11648)\n",
            "Epoch: 29 | Batch_idx: 100 |  Loss: (0.9773) | Acc: (94.47%) (12213/12928)\n",
            "Epoch: 29 | Batch_idx: 110 |  Loss: (0.9773) | Acc: (94.45%) (13420/14208)\n",
            "Epoch: 29 | Batch_idx: 120 |  Loss: (0.9764) | Acc: (94.52%) (14639/15488)\n",
            "Epoch: 29 | Batch_idx: 130 |  Loss: (0.9771) | Acc: (94.47%) (15841/16768)\n",
            "Epoch: 29 | Batch_idx: 140 |  Loss: (0.9768) | Acc: (94.45%) (17047/18048)\n",
            "Epoch: 29 | Batch_idx: 150 |  Loss: (0.9764) | Acc: (94.47%) (18259/19328)\n",
            "Epoch: 29 | Batch_idx: 160 |  Loss: (0.9762) | Acc: (94.50%) (19475/20608)\n",
            "Epoch: 29 | Batch_idx: 170 |  Loss: (0.9766) | Acc: (94.46%) (20675/21888)\n",
            "Epoch: 29 | Batch_idx: 180 |  Loss: (0.9765) | Acc: (94.47%) (21886/23168)\n",
            "Epoch: 29 | Batch_idx: 190 |  Loss: (0.9761) | Acc: (94.48%) (23099/24448)\n",
            "Epoch: 29 | Batch_idx: 200 |  Loss: (0.9771) | Acc: (94.45%) (24301/25728)\n",
            "Epoch: 29 | Batch_idx: 210 |  Loss: (0.9772) | Acc: (94.44%) (25507/27008)\n",
            "Epoch: 29 | Batch_idx: 220 |  Loss: (0.9777) | Acc: (94.41%) (26707/28288)\n",
            "Epoch: 29 | Batch_idx: 230 |  Loss: (0.9782) | Acc: (94.41%) (27914/29568)\n",
            "Epoch: 29 | Batch_idx: 240 |  Loss: (0.9787) | Acc: (94.39%) (29117/30848)\n",
            "Epoch: 29 | Batch_idx: 250 |  Loss: (0.9797) | Acc: (94.35%) (30314/32128)\n",
            "Epoch: 29 | Batch_idx: 260 |  Loss: (0.9801) | Acc: (94.33%) (31514/33408)\n",
            "Epoch: 29 | Batch_idx: 270 |  Loss: (0.9802) | Acc: (94.32%) (32716/34688)\n",
            "Epoch: 29 | Batch_idx: 280 |  Loss: (0.9805) | Acc: (94.31%) (33921/35968)\n",
            "Epoch: 29 | Batch_idx: 290 |  Loss: (0.9802) | Acc: (94.32%) (35131/37248)\n",
            "Epoch: 29 | Batch_idx: 300 |  Loss: (0.9796) | Acc: (94.35%) (36350/38528)\n",
            "Epoch: 29 | Batch_idx: 310 |  Loss: (0.9792) | Acc: (94.34%) (37556/39808)\n",
            "Epoch: 29 | Batch_idx: 320 |  Loss: (0.9799) | Acc: (94.31%) (38751/41088)\n",
            "Epoch: 29 | Batch_idx: 330 |  Loss: (0.9806) | Acc: (94.28%) (39943/42368)\n",
            "Epoch: 29 | Batch_idx: 340 |  Loss: (0.9805) | Acc: (94.27%) (41148/43648)\n",
            "Epoch: 29 | Batch_idx: 350 |  Loss: (0.9806) | Acc: (94.27%) (42354/44928)\n",
            "Epoch: 29 | Batch_idx: 360 |  Loss: (0.9805) | Acc: (94.27%) (43561/46208)\n",
            "Epoch: 29 | Batch_idx: 370 |  Loss: (0.9806) | Acc: (94.27%) (44767/47488)\n",
            "Epoch: 29 | Batch_idx: 380 |  Loss: (0.9811) | Acc: (94.23%) (45955/48768)\n",
            "Epoch: 29 | Batch_idx: 390 |  Loss: (0.9812) | Acc: (94.23%) (47117/50000)\n",
            "# TEST : Loss: (1.1173) | Acc: (87.16%) (8716/10000)\n",
            "Epoch: 30 | Batch_idx: 0 |  Loss: (0.9919) | Acc: (93.75%) (120/128)\n",
            "Epoch: 30 | Batch_idx: 10 |  Loss: (0.9725) | Acc: (95.03%) (1338/1408)\n",
            "Epoch: 30 | Batch_idx: 20 |  Loss: (0.9785) | Acc: (94.42%) (2538/2688)\n",
            "Epoch: 30 | Batch_idx: 30 |  Loss: (0.9718) | Acc: (94.93%) (3767/3968)\n",
            "Epoch: 30 | Batch_idx: 40 |  Loss: (0.9726) | Acc: (94.80%) (4975/5248)\n",
            "Epoch: 30 | Batch_idx: 50 |  Loss: (0.9710) | Acc: (94.82%) (6190/6528)\n",
            "Epoch: 30 | Batch_idx: 60 |  Loss: (0.9702) | Acc: (94.80%) (7402/7808)\n",
            "Epoch: 30 | Batch_idx: 70 |  Loss: (0.9675) | Acc: (94.92%) (8626/9088)\n",
            "Epoch: 30 | Batch_idx: 80 |  Loss: (0.9677) | Acc: (94.97%) (9846/10368)\n",
            "Epoch: 30 | Batch_idx: 90 |  Loss: (0.9678) | Acc: (94.99%) (11064/11648)\n",
            "Epoch: 30 | Batch_idx: 100 |  Loss: (0.9672) | Acc: (94.98%) (12279/12928)\n",
            "Epoch: 30 | Batch_idx: 110 |  Loss: (0.9664) | Acc: (95.02%) (13500/14208)\n",
            "Epoch: 30 | Batch_idx: 120 |  Loss: (0.9670) | Acc: (94.93%) (14703/15488)\n",
            "Epoch: 30 | Batch_idx: 130 |  Loss: (0.9677) | Acc: (94.90%) (15912/16768)\n",
            "Epoch: 30 | Batch_idx: 140 |  Loss: (0.9679) | Acc: (94.92%) (17132/18048)\n",
            "Epoch: 30 | Batch_idx: 150 |  Loss: (0.9672) | Acc: (94.98%) (18357/19328)\n",
            "Epoch: 30 | Batch_idx: 160 |  Loss: (0.9677) | Acc: (94.94%) (19565/20608)\n",
            "Epoch: 30 | Batch_idx: 170 |  Loss: (0.9691) | Acc: (94.86%) (20762/21888)\n",
            "Epoch: 30 | Batch_idx: 180 |  Loss: (0.9692) | Acc: (94.86%) (21978/23168)\n",
            "Epoch: 30 | Batch_idx: 190 |  Loss: (0.9698) | Acc: (94.83%) (23185/24448)\n",
            "Epoch: 30 | Batch_idx: 200 |  Loss: (0.9703) | Acc: (94.80%) (24391/25728)\n",
            "Epoch: 30 | Batch_idx: 210 |  Loss: (0.9702) | Acc: (94.83%) (25612/27008)\n",
            "Epoch: 30 | Batch_idx: 220 |  Loss: (0.9707) | Acc: (94.83%) (26825/28288)\n",
            "Epoch: 30 | Batch_idx: 230 |  Loss: (0.9710) | Acc: (94.81%) (28033/29568)\n",
            "Epoch: 30 | Batch_idx: 240 |  Loss: (0.9717) | Acc: (94.74%) (29226/30848)\n",
            "Epoch: 30 | Batch_idx: 250 |  Loss: (0.9726) | Acc: (94.68%) (30420/32128)\n",
            "Epoch: 30 | Batch_idx: 260 |  Loss: (0.9731) | Acc: (94.66%) (31625/33408)\n",
            "Epoch: 30 | Batch_idx: 270 |  Loss: (0.9737) | Acc: (94.63%) (32826/34688)\n",
            "Epoch: 30 | Batch_idx: 280 |  Loss: (0.9738) | Acc: (94.61%) (34031/35968)\n",
            "Epoch: 30 | Batch_idx: 290 |  Loss: (0.9743) | Acc: (94.59%) (35234/37248)\n",
            "Epoch: 30 | Batch_idx: 300 |  Loss: (0.9748) | Acc: (94.57%) (36435/38528)\n",
            "Epoch: 30 | Batch_idx: 310 |  Loss: (0.9749) | Acc: (94.56%) (37644/39808)\n",
            "Epoch: 30 | Batch_idx: 320 |  Loss: (0.9748) | Acc: (94.56%) (38853/41088)\n",
            "Epoch: 30 | Batch_idx: 330 |  Loss: (0.9745) | Acc: (94.58%) (40070/42368)\n",
            "Epoch: 30 | Batch_idx: 340 |  Loss: (0.9746) | Acc: (94.57%) (41277/43648)\n",
            "Epoch: 30 | Batch_idx: 350 |  Loss: (0.9755) | Acc: (94.52%) (42467/44928)\n",
            "Epoch: 30 | Batch_idx: 360 |  Loss: (0.9758) | Acc: (94.51%) (43671/46208)\n",
            "Epoch: 30 | Batch_idx: 370 |  Loss: (0.9761) | Acc: (94.51%) (44879/47488)\n",
            "Epoch: 30 | Batch_idx: 380 |  Loss: (0.9762) | Acc: (94.48%) (46078/48768)\n",
            "Epoch: 30 | Batch_idx: 390 |  Loss: (0.9763) | Acc: (94.47%) (47237/50000)\n",
            "# TEST : Loss: (1.0752) | Acc: (88.87%) (8887/10000)\n",
            "Epoch: 31 | Batch_idx: 0 |  Loss: (0.9647) | Acc: (96.88%) (124/128)\n",
            "Epoch: 31 | Batch_idx: 10 |  Loss: (0.9631) | Acc: (95.60%) (1346/1408)\n",
            "Epoch: 31 | Batch_idx: 20 |  Loss: (0.9640) | Acc: (95.39%) (2564/2688)\n",
            "Epoch: 31 | Batch_idx: 30 |  Loss: (0.9670) | Acc: (95.26%) (3780/3968)\n",
            "Epoch: 31 | Batch_idx: 40 |  Loss: (0.9683) | Acc: (95.24%) (4998/5248)\n",
            "Epoch: 31 | Batch_idx: 50 |  Loss: (0.9689) | Acc: (95.11%) (6209/6528)\n",
            "Epoch: 31 | Batch_idx: 60 |  Loss: (0.9690) | Acc: (94.97%) (7415/7808)\n",
            "Epoch: 31 | Batch_idx: 70 |  Loss: (0.9691) | Acc: (94.94%) (8628/9088)\n",
            "Epoch: 31 | Batch_idx: 80 |  Loss: (0.9686) | Acc: (94.98%) (9848/10368)\n",
            "Epoch: 31 | Batch_idx: 90 |  Loss: (0.9701) | Acc: (94.87%) (11051/11648)\n",
            "Epoch: 31 | Batch_idx: 100 |  Loss: (0.9723) | Acc: (94.75%) (12249/12928)\n",
            "Epoch: 31 | Batch_idx: 110 |  Loss: (0.9733) | Acc: (94.69%) (13453/14208)\n",
            "Epoch: 31 | Batch_idx: 120 |  Loss: (0.9737) | Acc: (94.63%) (14657/15488)\n",
            "Epoch: 31 | Batch_idx: 130 |  Loss: (0.9740) | Acc: (94.62%) (15866/16768)\n",
            "Epoch: 31 | Batch_idx: 140 |  Loss: (0.9739) | Acc: (94.59%) (17072/18048)\n",
            "Epoch: 31 | Batch_idx: 150 |  Loss: (0.9733) | Acc: (94.68%) (18299/19328)\n",
            "Epoch: 31 | Batch_idx: 160 |  Loss: (0.9741) | Acc: (94.64%) (19503/20608)\n",
            "Epoch: 31 | Batch_idx: 170 |  Loss: (0.9742) | Acc: (94.64%) (20715/21888)\n",
            "Epoch: 31 | Batch_idx: 180 |  Loss: (0.9745) | Acc: (94.63%) (21925/23168)\n",
            "Epoch: 31 | Batch_idx: 190 |  Loss: (0.9745) | Acc: (94.64%) (23138/24448)\n",
            "Epoch: 31 | Batch_idx: 200 |  Loss: (0.9749) | Acc: (94.62%) (24345/25728)\n",
            "Epoch: 31 | Batch_idx: 210 |  Loss: (0.9749) | Acc: (94.61%) (25551/27008)\n",
            "Epoch: 31 | Batch_idx: 220 |  Loss: (0.9745) | Acc: (94.62%) (26765/28288)\n",
            "Epoch: 31 | Batch_idx: 230 |  Loss: (0.9736) | Acc: (94.68%) (27995/29568)\n",
            "Epoch: 31 | Batch_idx: 240 |  Loss: (0.9739) | Acc: (94.67%) (29203/30848)\n",
            "Epoch: 31 | Batch_idx: 250 |  Loss: (0.9741) | Acc: (94.64%) (30406/32128)\n",
            "Epoch: 31 | Batch_idx: 260 |  Loss: (0.9736) | Acc: (94.67%) (31626/33408)\n",
            "Epoch: 31 | Batch_idx: 270 |  Loss: (0.9740) | Acc: (94.64%) (32828/34688)\n",
            "Epoch: 31 | Batch_idx: 280 |  Loss: (0.9739) | Acc: (94.63%) (34037/35968)\n",
            "Epoch: 31 | Batch_idx: 290 |  Loss: (0.9738) | Acc: (94.63%) (35246/37248)\n",
            "Epoch: 31 | Batch_idx: 300 |  Loss: (0.9738) | Acc: (94.63%) (36459/38528)\n",
            "Epoch: 31 | Batch_idx: 310 |  Loss: (0.9738) | Acc: (94.64%) (37673/39808)\n",
            "Epoch: 31 | Batch_idx: 320 |  Loss: (0.9739) | Acc: (94.63%) (38883/41088)\n",
            "Epoch: 31 | Batch_idx: 330 |  Loss: (0.9742) | Acc: (94.62%) (40089/42368)\n",
            "Epoch: 31 | Batch_idx: 340 |  Loss: (0.9743) | Acc: (94.62%) (41298/43648)\n",
            "Epoch: 31 | Batch_idx: 350 |  Loss: (0.9744) | Acc: (94.60%) (42504/44928)\n",
            "Epoch: 31 | Batch_idx: 360 |  Loss: (0.9745) | Acc: (94.60%) (43711/46208)\n",
            "Epoch: 31 | Batch_idx: 370 |  Loss: (0.9748) | Acc: (94.59%) (44921/47488)\n",
            "Epoch: 31 | Batch_idx: 380 |  Loss: (0.9751) | Acc: (94.56%) (46114/48768)\n",
            "Epoch: 31 | Batch_idx: 390 |  Loss: (0.9756) | Acc: (94.53%) (47264/50000)\n",
            "# TEST : Loss: (1.0898) | Acc: (88.25%) (8825/10000)\n",
            "Epoch: 32 | Batch_idx: 0 |  Loss: (0.9379) | Acc: (96.88%) (124/128)\n",
            "Epoch: 32 | Batch_idx: 10 |  Loss: (0.9627) | Acc: (95.38%) (1343/1408)\n",
            "Epoch: 32 | Batch_idx: 20 |  Loss: (0.9639) | Acc: (95.13%) (2557/2688)\n",
            "Epoch: 32 | Batch_idx: 30 |  Loss: (0.9636) | Acc: (95.14%) (3775/3968)\n",
            "Epoch: 32 | Batch_idx: 40 |  Loss: (0.9687) | Acc: (94.80%) (4975/5248)\n",
            "Epoch: 32 | Batch_idx: 50 |  Loss: (0.9668) | Acc: (94.84%) (6191/6528)\n",
            "Epoch: 32 | Batch_idx: 60 |  Loss: (0.9673) | Acc: (94.83%) (7404/7808)\n",
            "Epoch: 32 | Batch_idx: 70 |  Loss: (0.9666) | Acc: (94.92%) (8626/9088)\n",
            "Epoch: 32 | Batch_idx: 80 |  Loss: (0.9668) | Acc: (94.94%) (9843/10368)\n",
            "Epoch: 32 | Batch_idx: 90 |  Loss: (0.9664) | Acc: (94.94%) (11059/11648)\n",
            "Epoch: 32 | Batch_idx: 100 |  Loss: (0.9657) | Acc: (94.98%) (12279/12928)\n",
            "Epoch: 32 | Batch_idx: 110 |  Loss: (0.9658) | Acc: (94.95%) (13491/14208)\n",
            "Epoch: 32 | Batch_idx: 120 |  Loss: (0.9672) | Acc: (94.87%) (14694/15488)\n",
            "Epoch: 32 | Batch_idx: 130 |  Loss: (0.9674) | Acc: (94.90%) (15913/16768)\n",
            "Epoch: 32 | Batch_idx: 140 |  Loss: (0.9686) | Acc: (94.84%) (17117/18048)\n",
            "Epoch: 32 | Batch_idx: 150 |  Loss: (0.9703) | Acc: (94.69%) (18301/19328)\n",
            "Epoch: 32 | Batch_idx: 160 |  Loss: (0.9717) | Acc: (94.59%) (19494/20608)\n",
            "Epoch: 32 | Batch_idx: 170 |  Loss: (0.9718) | Acc: (94.63%) (20712/21888)\n",
            "Epoch: 32 | Batch_idx: 180 |  Loss: (0.9728) | Acc: (94.60%) (21917/23168)\n",
            "Epoch: 32 | Batch_idx: 190 |  Loss: (0.9735) | Acc: (94.60%) (23128/24448)\n",
            "Epoch: 32 | Batch_idx: 200 |  Loss: (0.9740) | Acc: (94.61%) (24340/25728)\n",
            "Epoch: 32 | Batch_idx: 210 |  Loss: (0.9749) | Acc: (94.55%) (25537/27008)\n",
            "Epoch: 32 | Batch_idx: 220 |  Loss: (0.9753) | Acc: (94.52%) (26737/28288)\n",
            "Epoch: 32 | Batch_idx: 230 |  Loss: (0.9750) | Acc: (94.52%) (27947/29568)\n",
            "Epoch: 32 | Batch_idx: 240 |  Loss: (0.9754) | Acc: (94.50%) (29150/30848)\n",
            "Epoch: 32 | Batch_idx: 250 |  Loss: (0.9755) | Acc: (94.50%) (30360/32128)\n",
            "Epoch: 32 | Batch_idx: 260 |  Loss: (0.9759) | Acc: (94.49%) (31566/33408)\n",
            "Epoch: 32 | Batch_idx: 270 |  Loss: (0.9760) | Acc: (94.47%) (32769/34688)\n",
            "Epoch: 32 | Batch_idx: 280 |  Loss: (0.9765) | Acc: (94.44%) (33968/35968)\n",
            "Epoch: 32 | Batch_idx: 290 |  Loss: (0.9762) | Acc: (94.45%) (35182/37248)\n",
            "Epoch: 32 | Batch_idx: 300 |  Loss: (0.9767) | Acc: (94.42%) (36378/38528)\n",
            "Epoch: 32 | Batch_idx: 310 |  Loss: (0.9767) | Acc: (94.43%) (37590/39808)\n",
            "Epoch: 32 | Batch_idx: 320 |  Loss: (0.9765) | Acc: (94.43%) (38798/41088)\n",
            "Epoch: 32 | Batch_idx: 330 |  Loss: (0.9767) | Acc: (94.42%) (40002/42368)\n",
            "Epoch: 32 | Batch_idx: 340 |  Loss: (0.9772) | Acc: (94.38%) (41195/43648)\n",
            "Epoch: 32 | Batch_idx: 350 |  Loss: (0.9770) | Acc: (94.38%) (42405/44928)\n",
            "Epoch: 32 | Batch_idx: 360 |  Loss: (0.9772) | Acc: (94.38%) (43610/46208)\n",
            "Epoch: 32 | Batch_idx: 370 |  Loss: (0.9773) | Acc: (94.37%) (44815/47488)\n",
            "Epoch: 32 | Batch_idx: 380 |  Loss: (0.9772) | Acc: (94.37%) (46024/48768)\n",
            "Epoch: 32 | Batch_idx: 390 |  Loss: (0.9769) | Acc: (94.38%) (47191/50000)\n",
            "# TEST : Loss: (1.0759) | Acc: (89.31%) (8931/10000)\n",
            "Epoch: 33 | Batch_idx: 0 |  Loss: (0.9691) | Acc: (95.31%) (122/128)\n",
            "Epoch: 33 | Batch_idx: 10 |  Loss: (0.9696) | Acc: (94.60%) (1332/1408)\n",
            "Epoch: 33 | Batch_idx: 20 |  Loss: (0.9652) | Acc: (94.83%) (2549/2688)\n",
            "Epoch: 33 | Batch_idx: 30 |  Loss: (0.9671) | Acc: (94.76%) (3760/3968)\n",
            "Epoch: 33 | Batch_idx: 40 |  Loss: (0.9672) | Acc: (94.89%) (4980/5248)\n",
            "Epoch: 33 | Batch_idx: 50 |  Loss: (0.9646) | Acc: (95.02%) (6203/6528)\n",
            "Epoch: 33 | Batch_idx: 60 |  Loss: (0.9640) | Acc: (95.08%) (7424/7808)\n",
            "Epoch: 33 | Batch_idx: 70 |  Loss: (0.9642) | Acc: (95.09%) (8642/9088)\n",
            "Epoch: 33 | Batch_idx: 80 |  Loss: (0.9656) | Acc: (94.97%) (9846/10368)\n",
            "Epoch: 33 | Batch_idx: 90 |  Loss: (0.9665) | Acc: (95.01%) (11067/11648)\n",
            "Epoch: 33 | Batch_idx: 100 |  Loss: (0.9667) | Acc: (94.99%) (12280/12928)\n",
            "Epoch: 33 | Batch_idx: 110 |  Loss: (0.9664) | Acc: (94.98%) (13495/14208)\n",
            "Epoch: 33 | Batch_idx: 120 |  Loss: (0.9680) | Acc: (94.87%) (14694/15488)\n",
            "Epoch: 33 | Batch_idx: 130 |  Loss: (0.9679) | Acc: (94.88%) (15910/16768)\n",
            "Epoch: 33 | Batch_idx: 140 |  Loss: (0.9691) | Acc: (94.86%) (17121/18048)\n",
            "Epoch: 33 | Batch_idx: 150 |  Loss: (0.9687) | Acc: (94.92%) (18346/19328)\n",
            "Epoch: 33 | Batch_idx: 160 |  Loss: (0.9689) | Acc: (94.89%) (19555/20608)\n",
            "Epoch: 33 | Batch_idx: 170 |  Loss: (0.9687) | Acc: (94.91%) (20773/21888)\n",
            "Epoch: 33 | Batch_idx: 180 |  Loss: (0.9683) | Acc: (94.92%) (21991/23168)\n",
            "Epoch: 33 | Batch_idx: 190 |  Loss: (0.9690) | Acc: (94.87%) (23195/24448)\n",
            "Epoch: 33 | Batch_idx: 200 |  Loss: (0.9692) | Acc: (94.90%) (24417/25728)\n",
            "Epoch: 33 | Batch_idx: 210 |  Loss: (0.9700) | Acc: (94.85%) (25617/27008)\n",
            "Epoch: 33 | Batch_idx: 220 |  Loss: (0.9701) | Acc: (94.81%) (26821/28288)\n",
            "Epoch: 33 | Batch_idx: 230 |  Loss: (0.9707) | Acc: (94.77%) (28022/29568)\n",
            "Epoch: 33 | Batch_idx: 240 |  Loss: (0.9709) | Acc: (94.78%) (29239/30848)\n",
            "Epoch: 33 | Batch_idx: 250 |  Loss: (0.9712) | Acc: (94.76%) (30443/32128)\n",
            "Epoch: 33 | Batch_idx: 260 |  Loss: (0.9712) | Acc: (94.75%) (31653/33408)\n",
            "Epoch: 33 | Batch_idx: 270 |  Loss: (0.9715) | Acc: (94.74%) (32863/34688)\n",
            "Epoch: 33 | Batch_idx: 280 |  Loss: (0.9718) | Acc: (94.71%) (34064/35968)\n",
            "Epoch: 33 | Batch_idx: 290 |  Loss: (0.9717) | Acc: (94.71%) (35278/37248)\n",
            "Epoch: 33 | Batch_idx: 300 |  Loss: (0.9722) | Acc: (94.66%) (36472/38528)\n",
            "Epoch: 33 | Batch_idx: 310 |  Loss: (0.9723) | Acc: (94.66%) (37682/39808)\n",
            "Epoch: 33 | Batch_idx: 320 |  Loss: (0.9726) | Acc: (94.65%) (38889/41088)\n",
            "Epoch: 33 | Batch_idx: 330 |  Loss: (0.9726) | Acc: (94.65%) (40100/42368)\n",
            "Epoch: 33 | Batch_idx: 340 |  Loss: (0.9726) | Acc: (94.65%) (41315/43648)\n",
            "Epoch: 33 | Batch_idx: 350 |  Loss: (0.9734) | Acc: (94.61%) (42507/44928)\n",
            "Epoch: 33 | Batch_idx: 360 |  Loss: (0.9735) | Acc: (94.61%) (43719/46208)\n",
            "Epoch: 33 | Batch_idx: 370 |  Loss: (0.9736) | Acc: (94.60%) (44924/47488)\n",
            "Epoch: 33 | Batch_idx: 380 |  Loss: (0.9736) | Acc: (94.61%) (46138/48768)\n",
            "Epoch: 33 | Batch_idx: 390 |  Loss: (0.9738) | Acc: (94.61%) (47305/50000)\n",
            "# TEST : Loss: (1.0881) | Acc: (88.55%) (8855/10000)\n",
            "Epoch: 34 | Batch_idx: 0 |  Loss: (0.9628) | Acc: (96.09%) (123/128)\n",
            "Epoch: 34 | Batch_idx: 10 |  Loss: (0.9645) | Acc: (94.74%) (1334/1408)\n",
            "Epoch: 34 | Batch_idx: 20 |  Loss: (0.9654) | Acc: (94.75%) (2547/2688)\n",
            "Epoch: 34 | Batch_idx: 30 |  Loss: (0.9654) | Acc: (94.88%) (3765/3968)\n",
            "Epoch: 34 | Batch_idx: 40 |  Loss: (0.9654) | Acc: (94.93%) (4982/5248)\n",
            "Epoch: 34 | Batch_idx: 50 |  Loss: (0.9667) | Acc: (94.87%) (6193/6528)\n",
            "Epoch: 34 | Batch_idx: 60 |  Loss: (0.9676) | Acc: (94.93%) (7412/7808)\n",
            "Epoch: 34 | Batch_idx: 70 |  Loss: (0.9681) | Acc: (94.92%) (8626/9088)\n",
            "Epoch: 34 | Batch_idx: 80 |  Loss: (0.9672) | Acc: (94.92%) (9841/10368)\n",
            "Epoch: 34 | Batch_idx: 90 |  Loss: (0.9679) | Acc: (94.93%) (11057/11648)\n",
            "Epoch: 34 | Batch_idx: 100 |  Loss: (0.9675) | Acc: (94.99%) (12280/12928)\n",
            "Epoch: 34 | Batch_idx: 110 |  Loss: (0.9690) | Acc: (94.90%) (13483/14208)\n",
            "Epoch: 34 | Batch_idx: 120 |  Loss: (0.9698) | Acc: (94.87%) (14693/15488)\n",
            "Epoch: 34 | Batch_idx: 130 |  Loss: (0.9690) | Acc: (94.88%) (15910/16768)\n",
            "Epoch: 34 | Batch_idx: 140 |  Loss: (0.9703) | Acc: (94.85%) (17118/18048)\n",
            "Epoch: 34 | Batch_idx: 150 |  Loss: (0.9694) | Acc: (94.89%) (18340/19328)\n",
            "Epoch: 34 | Batch_idx: 160 |  Loss: (0.9691) | Acc: (94.89%) (19555/20608)\n",
            "Epoch: 34 | Batch_idx: 170 |  Loss: (0.9708) | Acc: (94.78%) (20745/21888)\n",
            "Epoch: 34 | Batch_idx: 180 |  Loss: (0.9709) | Acc: (94.82%) (21969/23168)\n",
            "Epoch: 34 | Batch_idx: 190 |  Loss: (0.9711) | Acc: (94.80%) (23177/24448)\n",
            "Epoch: 34 | Batch_idx: 200 |  Loss: (0.9704) | Acc: (94.83%) (24398/25728)\n",
            "Epoch: 34 | Batch_idx: 210 |  Loss: (0.9717) | Acc: (94.75%) (25590/27008)\n",
            "Epoch: 34 | Batch_idx: 220 |  Loss: (0.9718) | Acc: (94.75%) (26802/28288)\n",
            "Epoch: 34 | Batch_idx: 230 |  Loss: (0.9714) | Acc: (94.75%) (28016/29568)\n",
            "Epoch: 34 | Batch_idx: 240 |  Loss: (0.9723) | Acc: (94.70%) (29214/30848)\n",
            "Epoch: 34 | Batch_idx: 250 |  Loss: (0.9720) | Acc: (94.72%) (30431/32128)\n",
            "Epoch: 34 | Batch_idx: 260 |  Loss: (0.9719) | Acc: (94.73%) (31646/33408)\n",
            "Epoch: 34 | Batch_idx: 270 |  Loss: (0.9717) | Acc: (94.74%) (32865/34688)\n",
            "Epoch: 34 | Batch_idx: 280 |  Loss: (0.9722) | Acc: (94.70%) (34062/35968)\n",
            "Epoch: 34 | Batch_idx: 290 |  Loss: (0.9718) | Acc: (94.73%) (35284/37248)\n",
            "Epoch: 34 | Batch_idx: 300 |  Loss: (0.9721) | Acc: (94.71%) (36491/38528)\n",
            "Epoch: 34 | Batch_idx: 310 |  Loss: (0.9721) | Acc: (94.72%) (37705/39808)\n",
            "Epoch: 34 | Batch_idx: 320 |  Loss: (0.9721) | Acc: (94.73%) (38923/41088)\n",
            "Epoch: 34 | Batch_idx: 330 |  Loss: (0.9719) | Acc: (94.74%) (40141/42368)\n",
            "Epoch: 34 | Batch_idx: 340 |  Loss: (0.9722) | Acc: (94.72%) (41344/43648)\n",
            "Epoch: 34 | Batch_idx: 350 |  Loss: (0.9721) | Acc: (94.73%) (42562/44928)\n",
            "Epoch: 34 | Batch_idx: 360 |  Loss: (0.9722) | Acc: (94.73%) (43773/46208)\n",
            "Epoch: 34 | Batch_idx: 370 |  Loss: (0.9720) | Acc: (94.74%) (44992/47488)\n",
            "Epoch: 34 | Batch_idx: 380 |  Loss: (0.9720) | Acc: (94.74%) (46204/48768)\n",
            "Epoch: 34 | Batch_idx: 390 |  Loss: (0.9727) | Acc: (94.71%) (47354/50000)\n",
            "# TEST : Loss: (1.0999) | Acc: (88.09%) (8809/10000)\n",
            "Epoch: 35 | Batch_idx: 0 |  Loss: (0.9893) | Acc: (95.31%) (122/128)\n",
            "Epoch: 35 | Batch_idx: 10 |  Loss: (0.9661) | Acc: (95.10%) (1339/1408)\n",
            "Epoch: 35 | Batch_idx: 20 |  Loss: (0.9662) | Acc: (95.20%) (2559/2688)\n",
            "Epoch: 35 | Batch_idx: 30 |  Loss: (0.9646) | Acc: (95.16%) (3776/3968)\n",
            "Epoch: 35 | Batch_idx: 40 |  Loss: (0.9599) | Acc: (95.39%) (5006/5248)\n",
            "Epoch: 35 | Batch_idx: 50 |  Loss: (0.9601) | Acc: (95.34%) (6224/6528)\n",
            "Epoch: 35 | Batch_idx: 60 |  Loss: (0.9616) | Acc: (95.27%) (7439/7808)\n",
            "Epoch: 35 | Batch_idx: 70 |  Loss: (0.9601) | Acc: (95.39%) (8669/9088)\n",
            "Epoch: 35 | Batch_idx: 80 |  Loss: (0.9594) | Acc: (95.41%) (9892/10368)\n",
            "Epoch: 35 | Batch_idx: 90 |  Loss: (0.9594) | Acc: (95.40%) (11112/11648)\n",
            "Epoch: 35 | Batch_idx: 100 |  Loss: (0.9598) | Acc: (95.36%) (12328/12928)\n",
            "Epoch: 35 | Batch_idx: 110 |  Loss: (0.9585) | Acc: (95.44%) (13560/14208)\n",
            "Epoch: 35 | Batch_idx: 120 |  Loss: (0.9575) | Acc: (95.51%) (14792/15488)\n",
            "Epoch: 35 | Batch_idx: 130 |  Loss: (0.9579) | Acc: (95.47%) (16009/16768)\n",
            "Epoch: 35 | Batch_idx: 140 |  Loss: (0.9580) | Acc: (95.48%) (17232/18048)\n",
            "Epoch: 35 | Batch_idx: 150 |  Loss: (0.9585) | Acc: (95.47%) (18453/19328)\n",
            "Epoch: 35 | Batch_idx: 160 |  Loss: (0.9583) | Acc: (95.46%) (19673/20608)\n",
            "Epoch: 35 | Batch_idx: 170 |  Loss: (0.9600) | Acc: (95.38%) (20877/21888)\n",
            "Epoch: 35 | Batch_idx: 180 |  Loss: (0.9604) | Acc: (95.37%) (22096/23168)\n",
            "Epoch: 35 | Batch_idx: 190 |  Loss: (0.9605) | Acc: (95.37%) (23317/24448)\n",
            "Epoch: 35 | Batch_idx: 200 |  Loss: (0.9605) | Acc: (95.36%) (24533/25728)\n",
            "Epoch: 35 | Batch_idx: 210 |  Loss: (0.9610) | Acc: (95.32%) (25745/27008)\n",
            "Epoch: 35 | Batch_idx: 220 |  Loss: (0.9607) | Acc: (95.33%) (26967/28288)\n",
            "Epoch: 35 | Batch_idx: 230 |  Loss: (0.9613) | Acc: (95.32%) (28185/29568)\n",
            "Epoch: 35 | Batch_idx: 240 |  Loss: (0.9618) | Acc: (95.31%) (29400/30848)\n",
            "Epoch: 35 | Batch_idx: 250 |  Loss: (0.9622) | Acc: (95.28%) (30611/32128)\n",
            "Epoch: 35 | Batch_idx: 260 |  Loss: (0.9622) | Acc: (95.28%) (31830/33408)\n",
            "Epoch: 35 | Batch_idx: 270 |  Loss: (0.9625) | Acc: (95.27%) (33047/34688)\n",
            "Epoch: 35 | Batch_idx: 280 |  Loss: (0.9630) | Acc: (95.23%) (34254/35968)\n",
            "Epoch: 35 | Batch_idx: 290 |  Loss: (0.9638) | Acc: (95.18%) (35451/37248)\n",
            "Epoch: 35 | Batch_idx: 300 |  Loss: (0.9649) | Acc: (95.09%) (36637/38528)\n",
            "Epoch: 35 | Batch_idx: 310 |  Loss: (0.9652) | Acc: (95.08%) (37851/39808)\n",
            "Epoch: 35 | Batch_idx: 320 |  Loss: (0.9653) | Acc: (95.09%) (39069/41088)\n",
            "Epoch: 35 | Batch_idx: 330 |  Loss: (0.9659) | Acc: (95.04%) (40266/42368)\n",
            "Epoch: 35 | Batch_idx: 340 |  Loss: (0.9664) | Acc: (95.01%) (41471/43648)\n",
            "Epoch: 35 | Batch_idx: 350 |  Loss: (0.9665) | Acc: (95.00%) (42682/44928)\n",
            "Epoch: 35 | Batch_idx: 360 |  Loss: (0.9669) | Acc: (94.97%) (43885/46208)\n",
            "Epoch: 35 | Batch_idx: 370 |  Loss: (0.9673) | Acc: (94.95%) (45091/47488)\n",
            "Epoch: 35 | Batch_idx: 380 |  Loss: (0.9677) | Acc: (94.92%) (46289/48768)\n",
            "Epoch: 35 | Batch_idx: 390 |  Loss: (0.9681) | Acc: (94.89%) (47445/50000)\n",
            "# TEST : Loss: (1.0881) | Acc: (88.61%) (8861/10000)\n",
            "Epoch: 36 | Batch_idx: 0 |  Loss: (0.9875) | Acc: (92.19%) (118/128)\n",
            "Epoch: 36 | Batch_idx: 10 |  Loss: (0.9790) | Acc: (93.68%) (1319/1408)\n",
            "Epoch: 36 | Batch_idx: 20 |  Loss: (0.9710) | Acc: (94.49%) (2540/2688)\n",
            "Epoch: 36 | Batch_idx: 30 |  Loss: (0.9692) | Acc: (94.73%) (3759/3968)\n",
            "Epoch: 36 | Batch_idx: 40 |  Loss: (0.9674) | Acc: (94.86%) (4978/5248)\n",
            "Epoch: 36 | Batch_idx: 50 |  Loss: (0.9668) | Acc: (94.94%) (6198/6528)\n",
            "Epoch: 36 | Batch_idx: 60 |  Loss: (0.9647) | Acc: (94.99%) (7417/7808)\n",
            "Epoch: 36 | Batch_idx: 70 |  Loss: (0.9649) | Acc: (94.96%) (8630/9088)\n",
            "Epoch: 36 | Batch_idx: 80 |  Loss: (0.9658) | Acc: (94.95%) (9844/10368)\n",
            "Epoch: 36 | Batch_idx: 90 |  Loss: (0.9626) | Acc: (95.18%) (11086/11648)\n",
            "Epoch: 36 | Batch_idx: 100 |  Loss: (0.9630) | Acc: (95.18%) (12305/12928)\n",
            "Epoch: 36 | Batch_idx: 110 |  Loss: (0.9626) | Acc: (95.21%) (13527/14208)\n",
            "Epoch: 36 | Batch_idx: 120 |  Loss: (0.9624) | Acc: (95.21%) (14746/15488)\n",
            "Epoch: 36 | Batch_idx: 130 |  Loss: (0.9613) | Acc: (95.28%) (15976/16768)\n",
            "Epoch: 36 | Batch_idx: 140 |  Loss: (0.9624) | Acc: (95.23%) (17188/18048)\n",
            "Epoch: 36 | Batch_idx: 150 |  Loss: (0.9619) | Acc: (95.28%) (18415/19328)\n",
            "Epoch: 36 | Batch_idx: 160 |  Loss: (0.9628) | Acc: (95.23%) (19624/20608)\n",
            "Epoch: 36 | Batch_idx: 170 |  Loss: (0.9635) | Acc: (95.14%) (20825/21888)\n",
            "Epoch: 36 | Batch_idx: 180 |  Loss: (0.9648) | Acc: (95.08%) (22027/23168)\n",
            "Epoch: 36 | Batch_idx: 190 |  Loss: (0.9649) | Acc: (95.08%) (23246/24448)\n",
            "Epoch: 36 | Batch_idx: 200 |  Loss: (0.9652) | Acc: (95.09%) (24465/25728)\n",
            "Epoch: 36 | Batch_idx: 210 |  Loss: (0.9657) | Acc: (95.05%) (25671/27008)\n",
            "Epoch: 36 | Batch_idx: 220 |  Loss: (0.9663) | Acc: (95.00%) (26873/28288)\n",
            "Epoch: 36 | Batch_idx: 230 |  Loss: (0.9662) | Acc: (95.01%) (28093/29568)\n",
            "Epoch: 36 | Batch_idx: 240 |  Loss: (0.9659) | Acc: (95.04%) (29318/30848)\n",
            "Epoch: 36 | Batch_idx: 250 |  Loss: (0.9659) | Acc: (95.03%) (30530/32128)\n",
            "Epoch: 36 | Batch_idx: 260 |  Loss: (0.9660) | Acc: (95.03%) (31749/33408)\n",
            "Epoch: 36 | Batch_idx: 270 |  Loss: (0.9668) | Acc: (95.00%) (32954/34688)\n",
            "Epoch: 36 | Batch_idx: 280 |  Loss: (0.9674) | Acc: (94.95%) (34150/35968)\n",
            "Epoch: 36 | Batch_idx: 290 |  Loss: (0.9669) | Acc: (94.98%) (35377/37248)\n",
            "Epoch: 36 | Batch_idx: 300 |  Loss: (0.9668) | Acc: (94.98%) (36592/38528)\n",
            "Epoch: 36 | Batch_idx: 310 |  Loss: (0.9669) | Acc: (94.98%) (37808/39808)\n",
            "Epoch: 36 | Batch_idx: 320 |  Loss: (0.9672) | Acc: (94.98%) (39024/41088)\n",
            "Epoch: 36 | Batch_idx: 330 |  Loss: (0.9676) | Acc: (94.96%) (40234/42368)\n",
            "Epoch: 36 | Batch_idx: 340 |  Loss: (0.9679) | Acc: (94.95%) (41444/43648)\n",
            "Epoch: 36 | Batch_idx: 350 |  Loss: (0.9680) | Acc: (94.93%) (42652/44928)\n",
            "Epoch: 36 | Batch_idx: 360 |  Loss: (0.9677) | Acc: (94.94%) (43870/46208)\n",
            "Epoch: 36 | Batch_idx: 370 |  Loss: (0.9676) | Acc: (94.94%) (45084/47488)\n",
            "Epoch: 36 | Batch_idx: 380 |  Loss: (0.9676) | Acc: (94.94%) (46299/48768)\n",
            "Epoch: 36 | Batch_idx: 390 |  Loss: (0.9678) | Acc: (94.93%) (47464/50000)\n",
            "# TEST : Loss: (1.0772) | Acc: (89.17%) (8917/10000)\n",
            "Epoch: 37 | Batch_idx: 0 |  Loss: (0.9365) | Acc: (96.88%) (124/128)\n",
            "Epoch: 37 | Batch_idx: 10 |  Loss: (0.9671) | Acc: (95.31%) (1342/1408)\n",
            "Epoch: 37 | Batch_idx: 20 |  Loss: (0.9656) | Acc: (95.39%) (2564/2688)\n",
            "Epoch: 37 | Batch_idx: 30 |  Loss: (0.9638) | Acc: (95.29%) (3781/3968)\n",
            "Epoch: 37 | Batch_idx: 40 |  Loss: (0.9612) | Acc: (95.41%) (5007/5248)\n",
            "Epoch: 37 | Batch_idx: 50 |  Loss: (0.9594) | Acc: (95.44%) (6230/6528)\n",
            "Epoch: 37 | Batch_idx: 60 |  Loss: (0.9587) | Acc: (95.49%) (7456/7808)\n",
            "Epoch: 37 | Batch_idx: 70 |  Loss: (0.9581) | Acc: (95.55%) (8684/9088)\n",
            "Epoch: 37 | Batch_idx: 80 |  Loss: (0.9580) | Acc: (95.58%) (9910/10368)\n",
            "Epoch: 37 | Batch_idx: 90 |  Loss: (0.9575) | Acc: (95.61%) (11137/11648)\n",
            "Epoch: 37 | Batch_idx: 100 |  Loss: (0.9574) | Acc: (95.61%) (12361/12928)\n",
            "Epoch: 37 | Batch_idx: 110 |  Loss: (0.9589) | Acc: (95.54%) (13574/14208)\n",
            "Epoch: 37 | Batch_idx: 120 |  Loss: (0.9576) | Acc: (95.62%) (14810/15488)\n",
            "Epoch: 37 | Batch_idx: 130 |  Loss: (0.9575) | Acc: (95.64%) (16037/16768)\n",
            "Epoch: 37 | Batch_idx: 140 |  Loss: (0.9585) | Acc: (95.60%) (17254/18048)\n",
            "Epoch: 37 | Batch_idx: 150 |  Loss: (0.9605) | Acc: (95.46%) (18450/19328)\n",
            "Epoch: 37 | Batch_idx: 160 |  Loss: (0.9603) | Acc: (95.48%) (19676/20608)\n",
            "Epoch: 37 | Batch_idx: 170 |  Loss: (0.9604) | Acc: (95.46%) (20895/21888)\n",
            "Epoch: 37 | Batch_idx: 180 |  Loss: (0.9603) | Acc: (95.47%) (22119/23168)\n",
            "Epoch: 37 | Batch_idx: 190 |  Loss: (0.9613) | Acc: (95.43%) (23330/24448)\n",
            "Epoch: 37 | Batch_idx: 200 |  Loss: (0.9621) | Acc: (95.35%) (24532/25728)\n",
            "Epoch: 37 | Batch_idx: 210 |  Loss: (0.9620) | Acc: (95.35%) (25752/27008)\n",
            "Epoch: 37 | Batch_idx: 220 |  Loss: (0.9624) | Acc: (95.33%) (26966/28288)\n",
            "Epoch: 37 | Batch_idx: 230 |  Loss: (0.9621) | Acc: (95.35%) (28193/29568)\n",
            "Epoch: 37 | Batch_idx: 240 |  Loss: (0.9620) | Acc: (95.34%) (29410/30848)\n",
            "Epoch: 37 | Batch_idx: 250 |  Loss: (0.9630) | Acc: (95.29%) (30616/32128)\n",
            "Epoch: 37 | Batch_idx: 260 |  Loss: (0.9629) | Acc: (95.31%) (31841/33408)\n",
            "Epoch: 37 | Batch_idx: 270 |  Loss: (0.9632) | Acc: (95.28%) (33052/34688)\n",
            "Epoch: 37 | Batch_idx: 280 |  Loss: (0.9634) | Acc: (95.26%) (34264/35968)\n",
            "Epoch: 37 | Batch_idx: 290 |  Loss: (0.9641) | Acc: (95.23%) (35473/37248)\n",
            "Epoch: 37 | Batch_idx: 300 |  Loss: (0.9649) | Acc: (95.18%) (36672/38528)\n",
            "Epoch: 37 | Batch_idx: 310 |  Loss: (0.9645) | Acc: (95.18%) (37890/39808)\n",
            "Epoch: 37 | Batch_idx: 320 |  Loss: (0.9647) | Acc: (95.17%) (39104/41088)\n",
            "Epoch: 37 | Batch_idx: 330 |  Loss: (0.9650) | Acc: (95.14%) (40308/42368)\n",
            "Epoch: 37 | Batch_idx: 340 |  Loss: (0.9652) | Acc: (95.13%) (41522/43648)\n",
            "Epoch: 37 | Batch_idx: 350 |  Loss: (0.9653) | Acc: (95.12%) (42736/44928)\n",
            "Epoch: 37 | Batch_idx: 360 |  Loss: (0.9657) | Acc: (95.11%) (43949/46208)\n",
            "Epoch: 37 | Batch_idx: 370 |  Loss: (0.9659) | Acc: (95.09%) (45154/47488)\n",
            "Epoch: 37 | Batch_idx: 380 |  Loss: (0.9664) | Acc: (95.05%) (46354/48768)\n",
            "Epoch: 37 | Batch_idx: 390 |  Loss: (0.9665) | Acc: (95.04%) (47522/50000)\n",
            "# TEST : Loss: (1.0870) | Acc: (88.66%) (8866/10000)\n",
            "Epoch: 38 | Batch_idx: 0 |  Loss: (0.9743) | Acc: (95.31%) (122/128)\n",
            "Epoch: 38 | Batch_idx: 10 |  Loss: (0.9625) | Acc: (96.09%) (1353/1408)\n",
            "Epoch: 38 | Batch_idx: 20 |  Loss: (0.9657) | Acc: (95.57%) (2569/2688)\n",
            "Epoch: 38 | Batch_idx: 30 |  Loss: (0.9668) | Acc: (95.29%) (3781/3968)\n",
            "Epoch: 38 | Batch_idx: 40 |  Loss: (0.9678) | Acc: (95.22%) (4997/5248)\n",
            "Epoch: 38 | Batch_idx: 50 |  Loss: (0.9654) | Acc: (95.45%) (6231/6528)\n",
            "Epoch: 38 | Batch_idx: 60 |  Loss: (0.9638) | Acc: (95.49%) (7456/7808)\n",
            "Epoch: 38 | Batch_idx: 70 |  Loss: (0.9617) | Acc: (95.61%) (8689/9088)\n",
            "Epoch: 38 | Batch_idx: 80 |  Loss: (0.9610) | Acc: (95.53%) (9905/10368)\n",
            "Epoch: 38 | Batch_idx: 90 |  Loss: (0.9600) | Acc: (95.51%) (11125/11648)\n",
            "Epoch: 38 | Batch_idx: 100 |  Loss: (0.9612) | Acc: (95.41%) (12335/12928)\n",
            "Epoch: 38 | Batch_idx: 110 |  Loss: (0.9605) | Acc: (95.49%) (13567/14208)\n",
            "Epoch: 38 | Batch_idx: 120 |  Loss: (0.9623) | Acc: (95.38%) (14772/15488)\n",
            "Epoch: 38 | Batch_idx: 130 |  Loss: (0.9632) | Acc: (95.33%) (15985/16768)\n",
            "Epoch: 38 | Batch_idx: 140 |  Loss: (0.9638) | Acc: (95.31%) (17201/18048)\n",
            "Epoch: 38 | Batch_idx: 150 |  Loss: (0.9638) | Acc: (95.33%) (18425/19328)\n",
            "Epoch: 38 | Batch_idx: 160 |  Loss: (0.9641) | Acc: (95.26%) (19632/20608)\n",
            "Epoch: 38 | Batch_idx: 170 |  Loss: (0.9651) | Acc: (95.21%) (20840/21888)\n",
            "Epoch: 38 | Batch_idx: 180 |  Loss: (0.9653) | Acc: (95.20%) (22055/23168)\n",
            "Epoch: 38 | Batch_idx: 190 |  Loss: (0.9651) | Acc: (95.23%) (23282/24448)\n",
            "Epoch: 38 | Batch_idx: 200 |  Loss: (0.9649) | Acc: (95.23%) (24500/25728)\n",
            "Epoch: 38 | Batch_idx: 210 |  Loss: (0.9653) | Acc: (95.18%) (25705/27008)\n",
            "Epoch: 38 | Batch_idx: 220 |  Loss: (0.9656) | Acc: (95.14%) (26914/28288)\n",
            "Epoch: 38 | Batch_idx: 230 |  Loss: (0.9651) | Acc: (95.18%) (28142/29568)\n",
            "Epoch: 38 | Batch_idx: 240 |  Loss: (0.9654) | Acc: (95.18%) (29362/30848)\n",
            "Epoch: 38 | Batch_idx: 250 |  Loss: (0.9653) | Acc: (95.17%) (30577/32128)\n",
            "Epoch: 38 | Batch_idx: 260 |  Loss: (0.9653) | Acc: (95.19%) (31800/33408)\n",
            "Epoch: 38 | Batch_idx: 270 |  Loss: (0.9651) | Acc: (95.21%) (33026/34688)\n",
            "Epoch: 38 | Batch_idx: 280 |  Loss: (0.9645) | Acc: (95.24%) (34256/35968)\n",
            "Epoch: 38 | Batch_idx: 290 |  Loss: (0.9645) | Acc: (95.22%) (35467/37248)\n",
            "Epoch: 38 | Batch_idx: 300 |  Loss: (0.9644) | Acc: (95.23%) (36691/38528)\n",
            "Epoch: 38 | Batch_idx: 310 |  Loss: (0.9644) | Acc: (95.23%) (37908/39808)\n",
            "Epoch: 38 | Batch_idx: 320 |  Loss: (0.9646) | Acc: (95.23%) (39129/41088)\n",
            "Epoch: 38 | Batch_idx: 330 |  Loss: (0.9644) | Acc: (95.25%) (40354/42368)\n",
            "Epoch: 38 | Batch_idx: 340 |  Loss: (0.9646) | Acc: (95.25%) (41573/43648)\n",
            "Epoch: 38 | Batch_idx: 350 |  Loss: (0.9652) | Acc: (95.20%) (42773/44928)\n",
            "Epoch: 38 | Batch_idx: 360 |  Loss: (0.9651) | Acc: (95.21%) (43993/46208)\n",
            "Epoch: 38 | Batch_idx: 370 |  Loss: (0.9649) | Acc: (95.20%) (45209/47488)\n",
            "Epoch: 38 | Batch_idx: 380 |  Loss: (0.9650) | Acc: (95.19%) (46420/48768)\n",
            "Epoch: 38 | Batch_idx: 390 |  Loss: (0.9651) | Acc: (95.17%) (47585/50000)\n",
            "# TEST : Loss: (1.0882) | Acc: (88.83%) (8883/10000)\n",
            "Epoch: 39 | Batch_idx: 0 |  Loss: (0.9781) | Acc: (92.97%) (119/128)\n",
            "Epoch: 39 | Batch_idx: 10 |  Loss: (0.9522) | Acc: (96.31%) (1356/1408)\n",
            "Epoch: 39 | Batch_idx: 20 |  Loss: (0.9553) | Acc: (95.72%) (2573/2688)\n",
            "Epoch: 39 | Batch_idx: 30 |  Loss: (0.9525) | Acc: (95.99%) (3809/3968)\n",
            "Epoch: 39 | Batch_idx: 40 |  Loss: (0.9553) | Acc: (95.90%) (5033/5248)\n",
            "Epoch: 39 | Batch_idx: 50 |  Loss: (0.9553) | Acc: (95.91%) (6261/6528)\n",
            "Epoch: 39 | Batch_idx: 60 |  Loss: (0.9577) | Acc: (95.72%) (7474/7808)\n",
            "Epoch: 39 | Batch_idx: 70 |  Loss: (0.9568) | Acc: (95.65%) (8693/9088)\n",
            "Epoch: 39 | Batch_idx: 80 |  Loss: (0.9560) | Acc: (95.70%) (9922/10368)\n",
            "Epoch: 39 | Batch_idx: 90 |  Loss: (0.9568) | Acc: (95.71%) (11148/11648)\n",
            "Epoch: 39 | Batch_idx: 100 |  Loss: (0.9579) | Acc: (95.59%) (12358/12928)\n",
            "Epoch: 39 | Batch_idx: 110 |  Loss: (0.9587) | Acc: (95.56%) (13577/14208)\n",
            "Epoch: 39 | Batch_idx: 120 |  Loss: (0.9593) | Acc: (95.50%) (14791/15488)\n",
            "Epoch: 39 | Batch_idx: 130 |  Loss: (0.9594) | Acc: (95.48%) (16010/16768)\n",
            "Epoch: 39 | Batch_idx: 140 |  Loss: (0.9598) | Acc: (95.48%) (17232/18048)\n",
            "Epoch: 39 | Batch_idx: 150 |  Loss: (0.9602) | Acc: (95.44%) (18446/19328)\n",
            "Epoch: 39 | Batch_idx: 160 |  Loss: (0.9606) | Acc: (95.41%) (19663/20608)\n",
            "Epoch: 39 | Batch_idx: 170 |  Loss: (0.9603) | Acc: (95.44%) (20891/21888)\n",
            "Epoch: 39 | Batch_idx: 180 |  Loss: (0.9603) | Acc: (95.43%) (22109/23168)\n",
            "Epoch: 39 | Batch_idx: 190 |  Loss: (0.9606) | Acc: (95.42%) (23329/24448)\n",
            "Epoch: 39 | Batch_idx: 200 |  Loss: (0.9603) | Acc: (95.43%) (24551/25728)\n",
            "Epoch: 39 | Batch_idx: 210 |  Loss: (0.9603) | Acc: (95.42%) (25772/27008)\n",
            "Epoch: 39 | Batch_idx: 220 |  Loss: (0.9609) | Acc: (95.38%) (26980/28288)\n",
            "Epoch: 39 | Batch_idx: 230 |  Loss: (0.9611) | Acc: (95.37%) (28199/29568)\n",
            "Epoch: 39 | Batch_idx: 240 |  Loss: (0.9608) | Acc: (95.38%) (29423/30848)\n",
            "Epoch: 39 | Batch_idx: 250 |  Loss: (0.9606) | Acc: (95.39%) (30646/32128)\n",
            "Epoch: 39 | Batch_idx: 260 |  Loss: (0.9608) | Acc: (95.38%) (31865/33408)\n",
            "Epoch: 39 | Batch_idx: 270 |  Loss: (0.9603) | Acc: (95.40%) (33091/34688)\n",
            "Epoch: 39 | Batch_idx: 280 |  Loss: (0.9607) | Acc: (95.39%) (34310/35968)\n",
            "Epoch: 39 | Batch_idx: 290 |  Loss: (0.9607) | Acc: (95.39%) (35531/37248)\n",
            "Epoch: 39 | Batch_idx: 300 |  Loss: (0.9606) | Acc: (95.38%) (36749/38528)\n",
            "Epoch: 39 | Batch_idx: 310 |  Loss: (0.9607) | Acc: (95.36%) (37959/39808)\n",
            "Epoch: 39 | Batch_idx: 320 |  Loss: (0.9607) | Acc: (95.35%) (39179/41088)\n",
            "Epoch: 39 | Batch_idx: 330 |  Loss: (0.9612) | Acc: (95.34%) (40394/42368)\n",
            "Epoch: 39 | Batch_idx: 340 |  Loss: (0.9615) | Acc: (95.31%) (41601/43648)\n",
            "Epoch: 39 | Batch_idx: 350 |  Loss: (0.9617) | Acc: (95.29%) (42813/44928)\n",
            "Epoch: 39 | Batch_idx: 360 |  Loss: (0.9621) | Acc: (95.26%) (44016/46208)\n",
            "Epoch: 39 | Batch_idx: 370 |  Loss: (0.9629) | Acc: (95.22%) (45218/47488)\n",
            "Epoch: 39 | Batch_idx: 380 |  Loss: (0.9629) | Acc: (95.21%) (46434/48768)\n",
            "Epoch: 39 | Batch_idx: 390 |  Loss: (0.9632) | Acc: (95.21%) (47604/50000)\n",
            "# TEST : Loss: (1.0843) | Acc: (89.11%) (8911/10000)\n",
            "Epoch: 40 | Batch_idx: 0 |  Loss: (1.0006) | Acc: (91.41%) (117/128)\n",
            "Epoch: 40 | Batch_idx: 10 |  Loss: (0.9656) | Acc: (94.74%) (1334/1408)\n",
            "Epoch: 40 | Batch_idx: 20 |  Loss: (0.9642) | Acc: (95.01%) (2554/2688)\n",
            "Epoch: 40 | Batch_idx: 30 |  Loss: (0.9656) | Acc: (95.11%) (3774/3968)\n",
            "Epoch: 40 | Batch_idx: 40 |  Loss: (0.9605) | Acc: (95.39%) (5006/5248)\n",
            "Epoch: 40 | Batch_idx: 50 |  Loss: (0.9606) | Acc: (95.36%) (6225/6528)\n",
            "Epoch: 40 | Batch_idx: 60 |  Loss: (0.9606) | Acc: (95.31%) (7442/7808)\n",
            "Epoch: 40 | Batch_idx: 70 |  Loss: (0.9609) | Acc: (95.33%) (8664/9088)\n",
            "Epoch: 40 | Batch_idx: 80 |  Loss: (0.9599) | Acc: (95.42%) (9893/10368)\n",
            "Epoch: 40 | Batch_idx: 90 |  Loss: (0.9595) | Acc: (95.45%) (11118/11648)\n",
            "Epoch: 40 | Batch_idx: 100 |  Loss: (0.9586) | Acc: (95.47%) (12343/12928)\n",
            "Epoch: 40 | Batch_idx: 110 |  Loss: (0.9594) | Acc: (95.40%) (13555/14208)\n",
            "Epoch: 40 | Batch_idx: 120 |  Loss: (0.9591) | Acc: (95.46%) (14785/15488)\n",
            "Epoch: 40 | Batch_idx: 130 |  Loss: (0.9605) | Acc: (95.38%) (15994/16768)\n",
            "Epoch: 40 | Batch_idx: 140 |  Loss: (0.9609) | Acc: (95.36%) (17211/18048)\n",
            "Epoch: 40 | Batch_idx: 150 |  Loss: (0.9608) | Acc: (95.34%) (18428/19328)\n",
            "Epoch: 40 | Batch_idx: 160 |  Loss: (0.9613) | Acc: (95.29%) (19637/20608)\n",
            "Epoch: 40 | Batch_idx: 170 |  Loss: (0.9614) | Acc: (95.29%) (20858/21888)\n",
            "Epoch: 40 | Batch_idx: 180 |  Loss: (0.9617) | Acc: (95.27%) (22073/23168)\n",
            "Epoch: 40 | Batch_idx: 190 |  Loss: (0.9627) | Acc: (95.21%) (23277/24448)\n",
            "Epoch: 40 | Batch_idx: 200 |  Loss: (0.9633) | Acc: (95.18%) (24487/25728)\n",
            "Epoch: 40 | Batch_idx: 210 |  Loss: (0.9641) | Acc: (95.12%) (25689/27008)\n",
            "Epoch: 40 | Batch_idx: 220 |  Loss: (0.9636) | Acc: (95.16%) (26918/28288)\n",
            "Epoch: 40 | Batch_idx: 230 |  Loss: (0.9638) | Acc: (95.14%) (28130/29568)\n",
            "Epoch: 40 | Batch_idx: 240 |  Loss: (0.9635) | Acc: (95.13%) (29345/30848)\n",
            "Epoch: 40 | Batch_idx: 250 |  Loss: (0.9638) | Acc: (95.09%) (30549/32128)\n",
            "Epoch: 40 | Batch_idx: 260 |  Loss: (0.9639) | Acc: (95.08%) (31765/33408)\n",
            "Epoch: 40 | Batch_idx: 270 |  Loss: (0.9640) | Acc: (95.08%) (32981/34688)\n",
            "Epoch: 40 | Batch_idx: 280 |  Loss: (0.9646) | Acc: (95.04%) (34184/35968)\n",
            "Epoch: 40 | Batch_idx: 290 |  Loss: (0.9640) | Acc: (95.08%) (35415/37248)\n",
            "Epoch: 40 | Batch_idx: 300 |  Loss: (0.9640) | Acc: (95.07%) (36628/38528)\n",
            "Epoch: 40 | Batch_idx: 310 |  Loss: (0.9644) | Acc: (95.04%) (37834/39808)\n",
            "Epoch: 40 | Batch_idx: 320 |  Loss: (0.9647) | Acc: (95.02%) (39040/41088)\n",
            "Epoch: 40 | Batch_idx: 330 |  Loss: (0.9650) | Acc: (95.01%) (40253/42368)\n",
            "Epoch: 40 | Batch_idx: 340 |  Loss: (0.9645) | Acc: (95.03%) (41478/43648)\n",
            "Epoch: 40 | Batch_idx: 350 |  Loss: (0.9647) | Acc: (95.02%) (42691/44928)\n",
            "Epoch: 40 | Batch_idx: 360 |  Loss: (0.9649) | Acc: (95.01%) (43901/46208)\n",
            "Epoch: 40 | Batch_idx: 370 |  Loss: (0.9653) | Acc: (95.00%) (45114/47488)\n",
            "Epoch: 40 | Batch_idx: 380 |  Loss: (0.9655) | Acc: (94.98%) (46321/48768)\n",
            "Epoch: 40 | Batch_idx: 390 |  Loss: (0.9656) | Acc: (94.96%) (47481/50000)\n",
            "# TEST : Loss: (1.0760) | Acc: (89.43%) (8943/10000)\n",
            "Epoch: 41 | Batch_idx: 0 |  Loss: (0.9254) | Acc: (97.66%) (125/128)\n",
            "Epoch: 41 | Batch_idx: 10 |  Loss: (0.9682) | Acc: (94.82%) (1335/1408)\n",
            "Epoch: 41 | Batch_idx: 20 |  Loss: (0.9645) | Acc: (95.28%) (2561/2688)\n",
            "Epoch: 41 | Batch_idx: 30 |  Loss: (0.9634) | Acc: (95.26%) (3780/3968)\n",
            "Epoch: 41 | Batch_idx: 40 |  Loss: (0.9630) | Acc: (95.27%) (5000/5248)\n",
            "Epoch: 41 | Batch_idx: 50 |  Loss: (0.9626) | Acc: (95.34%) (6224/6528)\n",
            "Epoch: 41 | Batch_idx: 60 |  Loss: (0.9636) | Acc: (95.26%) (7438/7808)\n",
            "Epoch: 41 | Batch_idx: 70 |  Loss: (0.9639) | Acc: (95.18%) (8650/9088)\n",
            "Epoch: 41 | Batch_idx: 80 |  Loss: (0.9640) | Acc: (95.20%) (9870/10368)\n",
            "Epoch: 41 | Batch_idx: 90 |  Loss: (0.9652) | Acc: (95.10%) (11077/11648)\n",
            "Epoch: 41 | Batch_idx: 100 |  Loss: (0.9639) | Acc: (95.18%) (12305/12928)\n",
            "Epoch: 41 | Batch_idx: 110 |  Loss: (0.9636) | Acc: (95.15%) (13519/14208)\n",
            "Epoch: 41 | Batch_idx: 120 |  Loss: (0.9635) | Acc: (95.18%) (14742/15488)\n",
            "Epoch: 41 | Batch_idx: 130 |  Loss: (0.9636) | Acc: (95.18%) (15959/16768)\n",
            "Epoch: 41 | Batch_idx: 140 |  Loss: (0.9636) | Acc: (95.14%) (17170/18048)\n",
            "Epoch: 41 | Batch_idx: 150 |  Loss: (0.9631) | Acc: (95.15%) (18390/19328)\n",
            "Epoch: 41 | Batch_idx: 160 |  Loss: (0.9629) | Acc: (95.14%) (19606/20608)\n",
            "Epoch: 41 | Batch_idx: 170 |  Loss: (0.9634) | Acc: (95.12%) (20820/21888)\n",
            "Epoch: 41 | Batch_idx: 180 |  Loss: (0.9632) | Acc: (95.12%) (22038/23168)\n",
            "Epoch: 41 | Batch_idx: 190 |  Loss: (0.9632) | Acc: (95.14%) (23259/24448)\n",
            "Epoch: 41 | Batch_idx: 200 |  Loss: (0.9630) | Acc: (95.16%) (24483/25728)\n",
            "Epoch: 41 | Batch_idx: 210 |  Loss: (0.9632) | Acc: (95.16%) (25701/27008)\n",
            "Epoch: 41 | Batch_idx: 220 |  Loss: (0.9627) | Acc: (95.20%) (26929/28288)\n",
            "Epoch: 41 | Batch_idx: 230 |  Loss: (0.9621) | Acc: (95.25%) (28163/29568)\n",
            "Epoch: 41 | Batch_idx: 240 |  Loss: (0.9623) | Acc: (95.22%) (29375/30848)\n",
            "Epoch: 41 | Batch_idx: 250 |  Loss: (0.9622) | Acc: (95.24%) (30598/32128)\n",
            "Epoch: 41 | Batch_idx: 260 |  Loss: (0.9621) | Acc: (95.23%) (31815/33408)\n",
            "Epoch: 41 | Batch_idx: 270 |  Loss: (0.9626) | Acc: (95.21%) (33026/34688)\n",
            "Epoch: 41 | Batch_idx: 280 |  Loss: (0.9627) | Acc: (95.20%) (34242/35968)\n",
            "Epoch: 41 | Batch_idx: 290 |  Loss: (0.9632) | Acc: (95.18%) (35452/37248)\n",
            "Epoch: 41 | Batch_idx: 300 |  Loss: (0.9634) | Acc: (95.18%) (36670/38528)\n",
            "Epoch: 41 | Batch_idx: 310 |  Loss: (0.9632) | Acc: (95.21%) (37901/39808)\n",
            "Epoch: 41 | Batch_idx: 320 |  Loss: (0.9636) | Acc: (95.16%) (39098/41088)\n",
            "Epoch: 41 | Batch_idx: 330 |  Loss: (0.9638) | Acc: (95.14%) (40310/42368)\n",
            "Epoch: 41 | Batch_idx: 340 |  Loss: (0.9639) | Acc: (95.13%) (41521/43648)\n",
            "Epoch: 41 | Batch_idx: 350 |  Loss: (0.9641) | Acc: (95.11%) (42733/44928)\n",
            "Epoch: 41 | Batch_idx: 360 |  Loss: (0.9641) | Acc: (95.11%) (43950/46208)\n",
            "Epoch: 41 | Batch_idx: 370 |  Loss: (0.9642) | Acc: (95.11%) (45165/47488)\n",
            "Epoch: 41 | Batch_idx: 380 |  Loss: (0.9644) | Acc: (95.11%) (46384/48768)\n",
            "Epoch: 41 | Batch_idx: 390 |  Loss: (0.9644) | Acc: (95.12%) (47559/50000)\n",
            "# TEST : Loss: (1.0923) | Acc: (88.39%) (8839/10000)\n",
            "Epoch: 42 | Batch_idx: 0 |  Loss: (0.9308) | Acc: (97.66%) (125/128)\n",
            "Epoch: 42 | Batch_idx: 10 |  Loss: (0.9544) | Acc: (95.81%) (1349/1408)\n",
            "Epoch: 42 | Batch_idx: 20 |  Loss: (0.9535) | Acc: (95.83%) (2576/2688)\n",
            "Epoch: 42 | Batch_idx: 30 |  Loss: (0.9586) | Acc: (95.64%) (3795/3968)\n",
            "Epoch: 42 | Batch_idx: 40 |  Loss: (0.9581) | Acc: (95.54%) (5014/5248)\n",
            "Epoch: 42 | Batch_idx: 50 |  Loss: (0.9593) | Acc: (95.54%) (6237/6528)\n",
            "Epoch: 42 | Batch_idx: 60 |  Loss: (0.9589) | Acc: (95.54%) (7460/7808)\n",
            "Epoch: 42 | Batch_idx: 70 |  Loss: (0.9580) | Acc: (95.61%) (8689/9088)\n",
            "Epoch: 42 | Batch_idx: 80 |  Loss: (0.9598) | Acc: (95.55%) (9907/10368)\n",
            "Epoch: 42 | Batch_idx: 90 |  Loss: (0.9608) | Acc: (95.50%) (11124/11648)\n",
            "Epoch: 42 | Batch_idx: 100 |  Loss: (0.9591) | Acc: (95.58%) (12357/12928)\n",
            "Epoch: 42 | Batch_idx: 110 |  Loss: (0.9583) | Acc: (95.59%) (13582/14208)\n",
            "Epoch: 42 | Batch_idx: 120 |  Loss: (0.9588) | Acc: (95.54%) (14797/15488)\n",
            "Epoch: 42 | Batch_idx: 130 |  Loss: (0.9585) | Acc: (95.50%) (16014/16768)\n",
            "Epoch: 42 | Batch_idx: 140 |  Loss: (0.9584) | Acc: (95.52%) (17240/18048)\n",
            "Epoch: 42 | Batch_idx: 150 |  Loss: (0.9576) | Acc: (95.57%) (18472/19328)\n",
            "Epoch: 42 | Batch_idx: 160 |  Loss: (0.9576) | Acc: (95.57%) (19695/20608)\n",
            "Epoch: 42 | Batch_idx: 170 |  Loss: (0.9573) | Acc: (95.58%) (20921/21888)\n",
            "Epoch: 42 | Batch_idx: 180 |  Loss: (0.9569) | Acc: (95.60%) (22149/23168)\n",
            "Epoch: 42 | Batch_idx: 190 |  Loss: (0.9574) | Acc: (95.57%) (23366/24448)\n",
            "Epoch: 42 | Batch_idx: 200 |  Loss: (0.9570) | Acc: (95.58%) (24591/25728)\n",
            "Epoch: 42 | Batch_idx: 210 |  Loss: (0.9574) | Acc: (95.56%) (25809/27008)\n",
            "Epoch: 42 | Batch_idx: 220 |  Loss: (0.9575) | Acc: (95.55%) (27029/28288)\n",
            "Epoch: 42 | Batch_idx: 230 |  Loss: (0.9578) | Acc: (95.51%) (28241/29568)\n",
            "Epoch: 42 | Batch_idx: 240 |  Loss: (0.9577) | Acc: (95.51%) (29463/30848)\n",
            "Epoch: 42 | Batch_idx: 250 |  Loss: (0.9576) | Acc: (95.50%) (30683/32128)\n",
            "Epoch: 42 | Batch_idx: 260 |  Loss: (0.9575) | Acc: (95.51%) (31907/33408)\n",
            "Epoch: 42 | Batch_idx: 270 |  Loss: (0.9582) | Acc: (95.47%) (33115/34688)\n",
            "Epoch: 42 | Batch_idx: 280 |  Loss: (0.9585) | Acc: (95.44%) (34329/35968)\n",
            "Epoch: 42 | Batch_idx: 290 |  Loss: (0.9584) | Acc: (95.43%) (35545/37248)\n",
            "Epoch: 42 | Batch_idx: 300 |  Loss: (0.9586) | Acc: (95.44%) (36772/38528)\n",
            "Epoch: 42 | Batch_idx: 310 |  Loss: (0.9590) | Acc: (95.41%) (37982/39808)\n",
            "Epoch: 42 | Batch_idx: 320 |  Loss: (0.9592) | Acc: (95.39%) (39194/41088)\n",
            "Epoch: 42 | Batch_idx: 330 |  Loss: (0.9592) | Acc: (95.38%) (40409/42368)\n",
            "Epoch: 42 | Batch_idx: 340 |  Loss: (0.9592) | Acc: (95.38%) (41631/43648)\n",
            "Epoch: 42 | Batch_idx: 350 |  Loss: (0.9595) | Acc: (95.36%) (42845/44928)\n",
            "Epoch: 42 | Batch_idx: 360 |  Loss: (0.9595) | Acc: (95.37%) (44068/46208)\n",
            "Epoch: 42 | Batch_idx: 370 |  Loss: (0.9596) | Acc: (95.36%) (45285/47488)\n",
            "Epoch: 42 | Batch_idx: 380 |  Loss: (0.9602) | Acc: (95.32%) (46487/48768)\n",
            "Epoch: 42 | Batch_idx: 390 |  Loss: (0.9606) | Acc: (95.31%) (47653/50000)\n",
            "# TEST : Loss: (1.0706) | Acc: (89.82%) (8982/10000)\n",
            "Epoch: 43 | Batch_idx: 0 |  Loss: (0.9024) | Acc: (98.44%) (126/128)\n",
            "Epoch: 43 | Batch_idx: 10 |  Loss: (0.9586) | Acc: (95.67%) (1347/1408)\n",
            "Epoch: 43 | Batch_idx: 20 |  Loss: (0.9598) | Acc: (95.61%) (2570/2688)\n",
            "Epoch: 43 | Batch_idx: 30 |  Loss: (0.9607) | Acc: (95.34%) (3783/3968)\n",
            "Epoch: 43 | Batch_idx: 40 |  Loss: (0.9583) | Acc: (95.50%) (5012/5248)\n",
            "Epoch: 43 | Batch_idx: 50 |  Loss: (0.9587) | Acc: (95.44%) (6230/6528)\n",
            "Epoch: 43 | Batch_idx: 60 |  Loss: (0.9567) | Acc: (95.59%) (7464/7808)\n",
            "Epoch: 43 | Batch_idx: 70 |  Loss: (0.9540) | Acc: (95.76%) (8703/9088)\n",
            "Epoch: 43 | Batch_idx: 80 |  Loss: (0.9538) | Acc: (95.77%) (9929/10368)\n",
            "Epoch: 43 | Batch_idx: 90 |  Loss: (0.9548) | Acc: (95.72%) (11149/11648)\n",
            "Epoch: 43 | Batch_idx: 100 |  Loss: (0.9549) | Acc: (95.70%) (12372/12928)\n",
            "Epoch: 43 | Batch_idx: 110 |  Loss: (0.9549) | Acc: (95.71%) (13599/14208)\n",
            "Epoch: 43 | Batch_idx: 120 |  Loss: (0.9542) | Acc: (95.76%) (14831/15488)\n",
            "Epoch: 43 | Batch_idx: 130 |  Loss: (0.9545) | Acc: (95.77%) (16058/16768)\n",
            "Epoch: 43 | Batch_idx: 140 |  Loss: (0.9549) | Acc: (95.71%) (17274/18048)\n",
            "Epoch: 43 | Batch_idx: 150 |  Loss: (0.9550) | Acc: (95.70%) (18497/19328)\n",
            "Epoch: 43 | Batch_idx: 160 |  Loss: (0.9560) | Acc: (95.61%) (19703/20608)\n",
            "Epoch: 43 | Batch_idx: 170 |  Loss: (0.9567) | Acc: (95.55%) (20915/21888)\n",
            "Epoch: 43 | Batch_idx: 180 |  Loss: (0.9577) | Acc: (95.53%) (22132/23168)\n",
            "Epoch: 43 | Batch_idx: 190 |  Loss: (0.9580) | Acc: (95.51%) (23350/24448)\n",
            "Epoch: 43 | Batch_idx: 200 |  Loss: (0.9591) | Acc: (95.45%) (24557/25728)\n",
            "Epoch: 43 | Batch_idx: 210 |  Loss: (0.9602) | Acc: (95.38%) (25760/27008)\n",
            "Epoch: 43 | Batch_idx: 220 |  Loss: (0.9607) | Acc: (95.35%) (26973/28288)\n",
            "Epoch: 43 | Batch_idx: 230 |  Loss: (0.9609) | Acc: (95.33%) (28187/29568)\n",
            "Epoch: 43 | Batch_idx: 240 |  Loss: (0.9606) | Acc: (95.35%) (29415/30848)\n",
            "Epoch: 43 | Batch_idx: 250 |  Loss: (0.9603) | Acc: (95.36%) (30637/32128)\n",
            "Epoch: 43 | Batch_idx: 260 |  Loss: (0.9597) | Acc: (95.40%) (31870/33408)\n",
            "Epoch: 43 | Batch_idx: 270 |  Loss: (0.9598) | Acc: (95.39%) (33089/34688)\n",
            "Epoch: 43 | Batch_idx: 280 |  Loss: (0.9599) | Acc: (95.37%) (34304/35968)\n",
            "Epoch: 43 | Batch_idx: 290 |  Loss: (0.9601) | Acc: (95.36%) (35520/37248)\n",
            "Epoch: 43 | Batch_idx: 300 |  Loss: (0.9600) | Acc: (95.35%) (36737/38528)\n",
            "Epoch: 43 | Batch_idx: 310 |  Loss: (0.9604) | Acc: (95.33%) (37947/39808)\n",
            "Epoch: 43 | Batch_idx: 320 |  Loss: (0.9605) | Acc: (95.32%) (39167/41088)\n",
            "Epoch: 43 | Batch_idx: 330 |  Loss: (0.9607) | Acc: (95.30%) (40376/42368)\n",
            "Epoch: 43 | Batch_idx: 340 |  Loss: (0.9609) | Acc: (95.30%) (41596/43648)\n",
            "Epoch: 43 | Batch_idx: 350 |  Loss: (0.9607) | Acc: (95.32%) (42825/44928)\n",
            "Epoch: 43 | Batch_idx: 360 |  Loss: (0.9610) | Acc: (95.31%) (44042/46208)\n",
            "Epoch: 43 | Batch_idx: 370 |  Loss: (0.9613) | Acc: (95.30%) (45258/47488)\n",
            "Epoch: 43 | Batch_idx: 380 |  Loss: (0.9616) | Acc: (95.28%) (46466/48768)\n",
            "Epoch: 43 | Batch_idx: 390 |  Loss: (0.9619) | Acc: (95.26%) (47630/50000)\n",
            "# TEST : Loss: (1.0635) | Acc: (89.98%) (8998/10000)\n",
            "Epoch: 44 | Batch_idx: 0 |  Loss: (0.9659) | Acc: (96.09%) (123/128)\n",
            "Epoch: 44 | Batch_idx: 10 |  Loss: (0.9360) | Acc: (97.09%) (1367/1408)\n",
            "Epoch: 44 | Batch_idx: 20 |  Loss: (0.9418) | Acc: (96.54%) (2595/2688)\n",
            "Epoch: 44 | Batch_idx: 30 |  Loss: (0.9474) | Acc: (96.19%) (3817/3968)\n",
            "Epoch: 44 | Batch_idx: 40 |  Loss: (0.9451) | Acc: (96.30%) (5054/5248)\n",
            "Epoch: 44 | Batch_idx: 50 |  Loss: (0.9479) | Acc: (96.12%) (6275/6528)\n",
            "Epoch: 44 | Batch_idx: 60 |  Loss: (0.9484) | Acc: (96.09%) (7503/7808)\n",
            "Epoch: 44 | Batch_idx: 70 |  Loss: (0.9494) | Acc: (95.96%) (8721/9088)\n",
            "Epoch: 44 | Batch_idx: 80 |  Loss: (0.9488) | Acc: (95.95%) (9948/10368)\n",
            "Epoch: 44 | Batch_idx: 90 |  Loss: (0.9489) | Acc: (95.95%) (11176/11648)\n",
            "Epoch: 44 | Batch_idx: 100 |  Loss: (0.9481) | Acc: (95.99%) (12409/12928)\n",
            "Epoch: 44 | Batch_idx: 110 |  Loss: (0.9489) | Acc: (95.93%) (13630/14208)\n",
            "Epoch: 44 | Batch_idx: 120 |  Loss: (0.9497) | Acc: (95.93%) (14857/15488)\n",
            "Epoch: 44 | Batch_idx: 130 |  Loss: (0.9507) | Acc: (95.86%) (16074/16768)\n",
            "Epoch: 44 | Batch_idx: 140 |  Loss: (0.9524) | Acc: (95.79%) (17288/18048)\n",
            "Epoch: 44 | Batch_idx: 150 |  Loss: (0.9521) | Acc: (95.82%) (18521/19328)\n",
            "Epoch: 44 | Batch_idx: 160 |  Loss: (0.9523) | Acc: (95.85%) (19752/20608)\n",
            "Epoch: 44 | Batch_idx: 170 |  Loss: (0.9528) | Acc: (95.81%) (20970/21888)\n",
            "Epoch: 44 | Batch_idx: 180 |  Loss: (0.9526) | Acc: (95.83%) (22202/23168)\n",
            "Epoch: 44 | Batch_idx: 190 |  Loss: (0.9536) | Acc: (95.76%) (23412/24448)\n",
            "Epoch: 44 | Batch_idx: 200 |  Loss: (0.9539) | Acc: (95.73%) (24629/25728)\n",
            "Epoch: 44 | Batch_idx: 210 |  Loss: (0.9546) | Acc: (95.69%) (25845/27008)\n",
            "Epoch: 44 | Batch_idx: 220 |  Loss: (0.9551) | Acc: (95.68%) (27065/28288)\n",
            "Epoch: 44 | Batch_idx: 230 |  Loss: (0.9551) | Acc: (95.69%) (28293/29568)\n",
            "Epoch: 44 | Batch_idx: 240 |  Loss: (0.9558) | Acc: (95.67%) (29511/30848)\n",
            "Epoch: 44 | Batch_idx: 250 |  Loss: (0.9558) | Acc: (95.67%) (30738/32128)\n",
            "Epoch: 44 | Batch_idx: 260 |  Loss: (0.9565) | Acc: (95.62%) (31945/33408)\n",
            "Epoch: 44 | Batch_idx: 270 |  Loss: (0.9574) | Acc: (95.57%) (33152/34688)\n",
            "Epoch: 44 | Batch_idx: 280 |  Loss: (0.9580) | Acc: (95.53%) (34359/35968)\n",
            "Epoch: 44 | Batch_idx: 290 |  Loss: (0.9585) | Acc: (95.49%) (35568/37248)\n",
            "Epoch: 44 | Batch_idx: 300 |  Loss: (0.9588) | Acc: (95.48%) (36787/38528)\n",
            "Epoch: 44 | Batch_idx: 310 |  Loss: (0.9591) | Acc: (95.45%) (37997/39808)\n",
            "Epoch: 44 | Batch_idx: 320 |  Loss: (0.9594) | Acc: (95.44%) (39215/41088)\n",
            "Epoch: 44 | Batch_idx: 330 |  Loss: (0.9597) | Acc: (95.44%) (40436/42368)\n",
            "Epoch: 44 | Batch_idx: 340 |  Loss: (0.9595) | Acc: (95.45%) (41661/43648)\n",
            "Epoch: 44 | Batch_idx: 350 |  Loss: (0.9598) | Acc: (95.43%) (42875/44928)\n",
            "Epoch: 44 | Batch_idx: 360 |  Loss: (0.9602) | Acc: (95.40%) (44083/46208)\n",
            "Epoch: 44 | Batch_idx: 370 |  Loss: (0.9605) | Acc: (95.37%) (45290/47488)\n",
            "Epoch: 44 | Batch_idx: 380 |  Loss: (0.9602) | Acc: (95.38%) (46515/48768)\n",
            "Epoch: 44 | Batch_idx: 390 |  Loss: (0.9601) | Acc: (95.38%) (47692/50000)\n",
            "# TEST : Loss: (1.0741) | Acc: (89.20%) (8920/10000)\n",
            "Epoch: 45 | Batch_idx: 0 |  Loss: (0.9997) | Acc: (92.97%) (119/128)\n",
            "Epoch: 45 | Batch_idx: 10 |  Loss: (0.9518) | Acc: (95.81%) (1349/1408)\n",
            "Epoch: 45 | Batch_idx: 20 |  Loss: (0.9504) | Acc: (96.13%) (2584/2688)\n",
            "Epoch: 45 | Batch_idx: 30 |  Loss: (0.9553) | Acc: (95.77%) (3800/3968)\n",
            "Epoch: 45 | Batch_idx: 40 |  Loss: (0.9535) | Acc: (95.90%) (5033/5248)\n",
            "Epoch: 45 | Batch_idx: 50 |  Loss: (0.9538) | Acc: (95.83%) (6256/6528)\n",
            "Epoch: 45 | Batch_idx: 60 |  Loss: (0.9544) | Acc: (95.74%) (7475/7808)\n",
            "Epoch: 45 | Batch_idx: 70 |  Loss: (0.9538) | Acc: (95.81%) (8707/9088)\n",
            "Epoch: 45 | Batch_idx: 80 |  Loss: (0.9537) | Acc: (95.80%) (9933/10368)\n",
            "Epoch: 45 | Batch_idx: 90 |  Loss: (0.9540) | Acc: (95.79%) (11158/11648)\n",
            "Epoch: 45 | Batch_idx: 100 |  Loss: (0.9532) | Acc: (95.83%) (12389/12928)\n",
            "Epoch: 45 | Batch_idx: 110 |  Loss: (0.9536) | Acc: (95.83%) (13616/14208)\n",
            "Epoch: 45 | Batch_idx: 120 |  Loss: (0.9532) | Acc: (95.82%) (14841/15488)\n",
            "Epoch: 45 | Batch_idx: 130 |  Loss: (0.9519) | Acc: (95.91%) (16083/16768)\n",
            "Epoch: 45 | Batch_idx: 140 |  Loss: (0.9516) | Acc: (95.92%) (17312/18048)\n",
            "Epoch: 45 | Batch_idx: 150 |  Loss: (0.9510) | Acc: (95.96%) (18548/19328)\n",
            "Epoch: 45 | Batch_idx: 160 |  Loss: (0.9506) | Acc: (96.00%) (19783/20608)\n",
            "Epoch: 45 | Batch_idx: 170 |  Loss: (0.9508) | Acc: (95.99%) (21010/21888)\n",
            "Epoch: 45 | Batch_idx: 180 |  Loss: (0.9516) | Acc: (95.95%) (22229/23168)\n",
            "Epoch: 45 | Batch_idx: 190 |  Loss: (0.9522) | Acc: (95.94%) (23456/24448)\n",
            "Epoch: 45 | Batch_idx: 200 |  Loss: (0.9528) | Acc: (95.86%) (24663/25728)\n",
            "Epoch: 45 | Batch_idx: 210 |  Loss: (0.9534) | Acc: (95.82%) (25878/27008)\n",
            "Epoch: 45 | Batch_idx: 220 |  Loss: (0.9547) | Acc: (95.76%) (27089/28288)\n",
            "Epoch: 45 | Batch_idx: 230 |  Loss: (0.9548) | Acc: (95.74%) (28309/29568)\n",
            "Epoch: 45 | Batch_idx: 240 |  Loss: (0.9552) | Acc: (95.72%) (29528/30848)\n",
            "Epoch: 45 | Batch_idx: 250 |  Loss: (0.9554) | Acc: (95.72%) (30752/32128)\n",
            "Epoch: 45 | Batch_idx: 260 |  Loss: (0.9560) | Acc: (95.66%) (31958/33408)\n",
            "Epoch: 45 | Batch_idx: 270 |  Loss: (0.9562) | Acc: (95.64%) (33174/34688)\n",
            "Epoch: 45 | Batch_idx: 280 |  Loss: (0.9566) | Acc: (95.60%) (34385/35968)\n",
            "Epoch: 45 | Batch_idx: 290 |  Loss: (0.9567) | Acc: (95.59%) (35606/37248)\n",
            "Epoch: 45 | Batch_idx: 300 |  Loss: (0.9565) | Acc: (95.61%) (36837/38528)\n",
            "Epoch: 45 | Batch_idx: 310 |  Loss: (0.9566) | Acc: (95.61%) (38061/39808)\n",
            "Epoch: 45 | Batch_idx: 320 |  Loss: (0.9566) | Acc: (95.61%) (39285/41088)\n",
            "Epoch: 45 | Batch_idx: 330 |  Loss: (0.9566) | Acc: (95.60%) (40505/42368)\n",
            "Epoch: 45 | Batch_idx: 340 |  Loss: (0.9569) | Acc: (95.60%) (41726/43648)\n",
            "Epoch: 45 | Batch_idx: 350 |  Loss: (0.9569) | Acc: (95.59%) (42945/44928)\n",
            "Epoch: 45 | Batch_idx: 360 |  Loss: (0.9571) | Acc: (95.58%) (44164/46208)\n",
            "Epoch: 45 | Batch_idx: 370 |  Loss: (0.9570) | Acc: (95.59%) (45394/47488)\n",
            "Epoch: 45 | Batch_idx: 380 |  Loss: (0.9568) | Acc: (95.61%) (46625/48768)\n",
            "Epoch: 45 | Batch_idx: 390 |  Loss: (0.9573) | Acc: (95.58%) (47790/50000)\n",
            "# TEST : Loss: (1.0737) | Acc: (89.49%) (8949/10000)\n",
            "Epoch: 46 | Batch_idx: 0 |  Loss: (0.9632) | Acc: (95.31%) (122/128)\n",
            "Epoch: 46 | Batch_idx: 10 |  Loss: (0.9790) | Acc: (94.32%) (1328/1408)\n",
            "Epoch: 46 | Batch_idx: 20 |  Loss: (0.9637) | Acc: (95.16%) (2558/2688)\n",
            "Epoch: 46 | Batch_idx: 30 |  Loss: (0.9638) | Acc: (95.19%) (3777/3968)\n",
            "Epoch: 46 | Batch_idx: 40 |  Loss: (0.9688) | Acc: (94.86%) (4978/5248)\n",
            "Epoch: 46 | Batch_idx: 50 |  Loss: (0.9648) | Acc: (95.19%) (6214/6528)\n",
            "Epoch: 46 | Batch_idx: 60 |  Loss: (0.9622) | Acc: (95.31%) (7442/7808)\n",
            "Epoch: 46 | Batch_idx: 70 |  Loss: (0.9588) | Acc: (95.51%) (8680/9088)\n",
            "Epoch: 46 | Batch_idx: 80 |  Loss: (0.9590) | Acc: (95.47%) (9898/10368)\n",
            "Epoch: 46 | Batch_idx: 90 |  Loss: (0.9588) | Acc: (95.44%) (11117/11648)\n",
            "Epoch: 46 | Batch_idx: 100 |  Loss: (0.9584) | Acc: (95.41%) (12335/12928)\n",
            "Epoch: 46 | Batch_idx: 110 |  Loss: (0.9584) | Acc: (95.41%) (13556/14208)\n",
            "Epoch: 46 | Batch_idx: 120 |  Loss: (0.9595) | Acc: (95.35%) (14768/15488)\n",
            "Epoch: 46 | Batch_idx: 130 |  Loss: (0.9598) | Acc: (95.31%) (15982/16768)\n",
            "Epoch: 46 | Batch_idx: 140 |  Loss: (0.9602) | Acc: (95.31%) (17202/18048)\n",
            "Epoch: 46 | Batch_idx: 150 |  Loss: (0.9604) | Acc: (95.34%) (18428/19328)\n",
            "Epoch: 46 | Batch_idx: 160 |  Loss: (0.9606) | Acc: (95.32%) (19644/20608)\n",
            "Epoch: 46 | Batch_idx: 170 |  Loss: (0.9605) | Acc: (95.34%) (20868/21888)\n",
            "Epoch: 46 | Batch_idx: 180 |  Loss: (0.9605) | Acc: (95.34%) (22089/23168)\n",
            "Epoch: 46 | Batch_idx: 190 |  Loss: (0.9602) | Acc: (95.35%) (23310/24448)\n",
            "Epoch: 46 | Batch_idx: 200 |  Loss: (0.9594) | Acc: (95.41%) (24546/25728)\n",
            "Epoch: 46 | Batch_idx: 210 |  Loss: (0.9598) | Acc: (95.39%) (25763/27008)\n",
            "Epoch: 46 | Batch_idx: 220 |  Loss: (0.9596) | Acc: (95.39%) (26985/28288)\n",
            "Epoch: 46 | Batch_idx: 230 |  Loss: (0.9593) | Acc: (95.38%) (28201/29568)\n",
            "Epoch: 46 | Batch_idx: 240 |  Loss: (0.9594) | Acc: (95.39%) (29427/30848)\n",
            "Epoch: 46 | Batch_idx: 250 |  Loss: (0.9594) | Acc: (95.38%) (30645/32128)\n",
            "Epoch: 46 | Batch_idx: 260 |  Loss: (0.9596) | Acc: (95.36%) (31859/33408)\n",
            "Epoch: 46 | Batch_idx: 270 |  Loss: (0.9605) | Acc: (95.30%) (33059/34688)\n",
            "Epoch: 46 | Batch_idx: 280 |  Loss: (0.9605) | Acc: (95.32%) (34284/35968)\n",
            "Epoch: 46 | Batch_idx: 290 |  Loss: (0.9608) | Acc: (95.30%) (35497/37248)\n",
            "Epoch: 46 | Batch_idx: 300 |  Loss: (0.9612) | Acc: (95.28%) (36709/38528)\n",
            "Epoch: 46 | Batch_idx: 310 |  Loss: (0.9616) | Acc: (95.26%) (37920/39808)\n",
            "Epoch: 46 | Batch_idx: 320 |  Loss: (0.9611) | Acc: (95.28%) (39148/41088)\n",
            "Epoch: 46 | Batch_idx: 330 |  Loss: (0.9614) | Acc: (95.27%) (40365/42368)\n",
            "Epoch: 46 | Batch_idx: 340 |  Loss: (0.9616) | Acc: (95.26%) (41581/43648)\n",
            "Epoch: 46 | Batch_idx: 350 |  Loss: (0.9619) | Acc: (95.25%) (42792/44928)\n",
            "Epoch: 46 | Batch_idx: 360 |  Loss: (0.9621) | Acc: (95.24%) (44009/46208)\n",
            "Epoch: 46 | Batch_idx: 370 |  Loss: (0.9628) | Acc: (95.18%) (45201/47488)\n",
            "Epoch: 46 | Batch_idx: 380 |  Loss: (0.9627) | Acc: (95.19%) (46421/48768)\n",
            "Epoch: 46 | Batch_idx: 390 |  Loss: (0.9627) | Acc: (95.20%) (47600/50000)\n",
            "# TEST : Loss: (1.0889) | Acc: (88.60%) (8860/10000)\n",
            "Epoch: 47 | Batch_idx: 0 |  Loss: (0.9617) | Acc: (93.75%) (120/128)\n",
            "Epoch: 47 | Batch_idx: 10 |  Loss: (0.9580) | Acc: (95.10%) (1339/1408)\n",
            "Epoch: 47 | Batch_idx: 20 |  Loss: (0.9537) | Acc: (95.39%) (2564/2688)\n",
            "Epoch: 47 | Batch_idx: 30 |  Loss: (0.9589) | Acc: (95.16%) (3776/3968)\n",
            "Epoch: 47 | Batch_idx: 40 |  Loss: (0.9553) | Acc: (95.48%) (5011/5248)\n",
            "Epoch: 47 | Batch_idx: 50 |  Loss: (0.9544) | Acc: (95.60%) (6241/6528)\n",
            "Epoch: 47 | Batch_idx: 60 |  Loss: (0.9538) | Acc: (95.70%) (7472/7808)\n",
            "Epoch: 47 | Batch_idx: 70 |  Loss: (0.9527) | Acc: (95.76%) (8703/9088)\n",
            "Epoch: 47 | Batch_idx: 80 |  Loss: (0.9531) | Acc: (95.71%) (9923/10368)\n",
            "Epoch: 47 | Batch_idx: 90 |  Loss: (0.9539) | Acc: (95.72%) (11150/11648)\n",
            "Epoch: 47 | Batch_idx: 100 |  Loss: (0.9536) | Acc: (95.71%) (12374/12928)\n",
            "Epoch: 47 | Batch_idx: 110 |  Loss: (0.9539) | Acc: (95.69%) (13595/14208)\n",
            "Epoch: 47 | Batch_idx: 120 |  Loss: (0.9534) | Acc: (95.73%) (14827/15488)\n",
            "Epoch: 47 | Batch_idx: 130 |  Loss: (0.9538) | Acc: (95.69%) (16045/16768)\n",
            "Epoch: 47 | Batch_idx: 140 |  Loss: (0.9537) | Acc: (95.71%) (17273/18048)\n",
            "Epoch: 47 | Batch_idx: 150 |  Loss: (0.9544) | Acc: (95.66%) (18490/19328)\n",
            "Epoch: 47 | Batch_idx: 160 |  Loss: (0.9551) | Acc: (95.64%) (19710/20608)\n",
            "Epoch: 47 | Batch_idx: 170 |  Loss: (0.9552) | Acc: (95.65%) (20935/21888)\n",
            "Epoch: 47 | Batch_idx: 180 |  Loss: (0.9547) | Acc: (95.69%) (22169/23168)\n",
            "Epoch: 47 | Batch_idx: 190 |  Loss: (0.9552) | Acc: (95.62%) (23377/24448)\n",
            "Epoch: 47 | Batch_idx: 200 |  Loss: (0.9554) | Acc: (95.60%) (24596/25728)\n",
            "Epoch: 47 | Batch_idx: 210 |  Loss: (0.9558) | Acc: (95.57%) (25812/27008)\n",
            "Epoch: 47 | Batch_idx: 220 |  Loss: (0.9554) | Acc: (95.61%) (27045/28288)\n",
            "Epoch: 47 | Batch_idx: 230 |  Loss: (0.9554) | Acc: (95.58%) (28260/29568)\n",
            "Epoch: 47 | Batch_idx: 240 |  Loss: (0.9554) | Acc: (95.59%) (29489/30848)\n",
            "Epoch: 47 | Batch_idx: 250 |  Loss: (0.9552) | Acc: (95.63%) (30724/32128)\n",
            "Epoch: 47 | Batch_idx: 260 |  Loss: (0.9552) | Acc: (95.62%) (31944/33408)\n",
            "Epoch: 47 | Batch_idx: 270 |  Loss: (0.9560) | Acc: (95.58%) (33154/34688)\n",
            "Epoch: 47 | Batch_idx: 280 |  Loss: (0.9563) | Acc: (95.57%) (34376/35968)\n",
            "Epoch: 47 | Batch_idx: 290 |  Loss: (0.9566) | Acc: (95.57%) (35597/37248)\n",
            "Epoch: 47 | Batch_idx: 300 |  Loss: (0.9567) | Acc: (95.57%) (36820/38528)\n",
            "Epoch: 47 | Batch_idx: 310 |  Loss: (0.9566) | Acc: (95.57%) (38043/39808)\n",
            "Epoch: 47 | Batch_idx: 320 |  Loss: (0.9567) | Acc: (95.57%) (39267/41088)\n",
            "Epoch: 47 | Batch_idx: 330 |  Loss: (0.9574) | Acc: (95.53%) (40473/42368)\n",
            "Epoch: 47 | Batch_idx: 340 |  Loss: (0.9585) | Acc: (95.47%) (41671/43648)\n",
            "Epoch: 47 | Batch_idx: 350 |  Loss: (0.9583) | Acc: (95.47%) (42894/44928)\n",
            "Epoch: 47 | Batch_idx: 360 |  Loss: (0.9586) | Acc: (95.46%) (44108/46208)\n",
            "Epoch: 47 | Batch_idx: 370 |  Loss: (0.9583) | Acc: (95.47%) (45337/47488)\n",
            "Epoch: 47 | Batch_idx: 380 |  Loss: (0.9583) | Acc: (95.47%) (46561/48768)\n",
            "Epoch: 47 | Batch_idx: 390 |  Loss: (0.9583) | Acc: (95.48%) (47739/50000)\n",
            "# TEST : Loss: (1.0627) | Acc: (89.85%) (8985/10000)\n",
            "Epoch: 48 | Batch_idx: 0 |  Loss: (0.9925) | Acc: (92.19%) (118/128)\n",
            "Epoch: 48 | Batch_idx: 10 |  Loss: (0.9600) | Acc: (95.17%) (1340/1408)\n",
            "Epoch: 48 | Batch_idx: 20 |  Loss: (0.9580) | Acc: (95.09%) (2556/2688)\n",
            "Epoch: 48 | Batch_idx: 30 |  Loss: (0.9537) | Acc: (95.59%) (3793/3968)\n",
            "Epoch: 48 | Batch_idx: 40 |  Loss: (0.9559) | Acc: (95.56%) (5015/5248)\n",
            "Epoch: 48 | Batch_idx: 50 |  Loss: (0.9522) | Acc: (95.83%) (6256/6528)\n",
            "Epoch: 48 | Batch_idx: 60 |  Loss: (0.9513) | Acc: (95.82%) (7482/7808)\n",
            "Epoch: 48 | Batch_idx: 70 |  Loss: (0.9511) | Acc: (95.84%) (8710/9088)\n",
            "Epoch: 48 | Batch_idx: 80 |  Loss: (0.9513) | Acc: (95.85%) (9938/10368)\n",
            "Epoch: 48 | Batch_idx: 90 |  Loss: (0.9509) | Acc: (95.92%) (11173/11648)\n",
            "Epoch: 48 | Batch_idx: 100 |  Loss: (0.9505) | Acc: (95.95%) (12404/12928)\n",
            "Epoch: 48 | Batch_idx: 110 |  Loss: (0.9494) | Acc: (96.02%) (13643/14208)\n",
            "Epoch: 48 | Batch_idx: 120 |  Loss: (0.9490) | Acc: (96.02%) (14871/15488)\n",
            "Epoch: 48 | Batch_idx: 130 |  Loss: (0.9500) | Acc: (95.96%) (16091/16768)\n",
            "Epoch: 48 | Batch_idx: 140 |  Loss: (0.9499) | Acc: (95.96%) (17318/18048)\n",
            "Epoch: 48 | Batch_idx: 150 |  Loss: (0.9506) | Acc: (95.90%) (18535/19328)\n",
            "Epoch: 48 | Batch_idx: 160 |  Loss: (0.9510) | Acc: (95.89%) (19761/20608)\n",
            "Epoch: 48 | Batch_idx: 170 |  Loss: (0.9512) | Acc: (95.89%) (20988/21888)\n",
            "Epoch: 48 | Batch_idx: 180 |  Loss: (0.9516) | Acc: (95.87%) (22211/23168)\n",
            "Epoch: 48 | Batch_idx: 190 |  Loss: (0.9508) | Acc: (95.91%) (23449/24448)\n",
            "Epoch: 48 | Batch_idx: 200 |  Loss: (0.9510) | Acc: (95.90%) (24673/25728)\n",
            "Epoch: 48 | Batch_idx: 210 |  Loss: (0.9517) | Acc: (95.84%) (25885/27008)\n",
            "Epoch: 48 | Batch_idx: 220 |  Loss: (0.9519) | Acc: (95.81%) (27103/28288)\n",
            "Epoch: 48 | Batch_idx: 230 |  Loss: (0.9521) | Acc: (95.82%) (28331/29568)\n",
            "Epoch: 48 | Batch_idx: 240 |  Loss: (0.9525) | Acc: (95.78%) (29545/30848)\n",
            "Epoch: 48 | Batch_idx: 250 |  Loss: (0.9538) | Acc: (95.69%) (30742/32128)\n",
            "Epoch: 48 | Batch_idx: 260 |  Loss: (0.9544) | Acc: (95.67%) (31961/33408)\n",
            "Epoch: 48 | Batch_idx: 270 |  Loss: (0.9554) | Acc: (95.63%) (33172/34688)\n",
            "Epoch: 48 | Batch_idx: 280 |  Loss: (0.9549) | Acc: (95.65%) (34405/35968)\n",
            "Epoch: 48 | Batch_idx: 290 |  Loss: (0.9552) | Acc: (95.65%) (35629/37248)\n",
            "Epoch: 48 | Batch_idx: 300 |  Loss: (0.9558) | Acc: (95.60%) (36831/38528)\n",
            "Epoch: 48 | Batch_idx: 310 |  Loss: (0.9557) | Acc: (95.59%) (38053/39808)\n",
            "Epoch: 48 | Batch_idx: 320 |  Loss: (0.9557) | Acc: (95.60%) (39281/41088)\n",
            "Epoch: 48 | Batch_idx: 330 |  Loss: (0.9560) | Acc: (95.58%) (40495/42368)\n",
            "Epoch: 48 | Batch_idx: 340 |  Loss: (0.9561) | Acc: (95.57%) (41714/43648)\n",
            "Epoch: 48 | Batch_idx: 350 |  Loss: (0.9563) | Acc: (95.56%) (42932/44928)\n",
            "Epoch: 48 | Batch_idx: 360 |  Loss: (0.9560) | Acc: (95.58%) (44167/46208)\n",
            "Epoch: 48 | Batch_idx: 370 |  Loss: (0.9562) | Acc: (95.57%) (45382/47488)\n",
            "Epoch: 48 | Batch_idx: 380 |  Loss: (0.9562) | Acc: (95.58%) (46611/48768)\n",
            "Epoch: 48 | Batch_idx: 390 |  Loss: (0.9567) | Acc: (95.56%) (47779/50000)\n",
            "# TEST : Loss: (1.0808) | Acc: (89.22%) (8922/10000)\n",
            "Epoch: 49 | Batch_idx: 0 |  Loss: (0.9765) | Acc: (92.19%) (118/128)\n",
            "Epoch: 49 | Batch_idx: 10 |  Loss: (0.9717) | Acc: (94.60%) (1332/1408)\n",
            "Epoch: 49 | Batch_idx: 20 |  Loss: (0.9720) | Acc: (94.61%) (2543/2688)\n",
            "Epoch: 49 | Batch_idx: 30 |  Loss: (0.9646) | Acc: (95.09%) (3773/3968)\n",
            "Epoch: 49 | Batch_idx: 40 |  Loss: (0.9630) | Acc: (95.29%) (5001/5248)\n",
            "Epoch: 49 | Batch_idx: 50 |  Loss: (0.9613) | Acc: (95.45%) (6231/6528)\n",
            "Epoch: 49 | Batch_idx: 60 |  Loss: (0.9585) | Acc: (95.53%) (7459/7808)\n",
            "Epoch: 49 | Batch_idx: 70 |  Loss: (0.9569) | Acc: (95.60%) (8688/9088)\n",
            "Epoch: 49 | Batch_idx: 80 |  Loss: (0.9549) | Acc: (95.71%) (9923/10368)\n",
            "Epoch: 49 | Batch_idx: 90 |  Loss: (0.9561) | Acc: (95.60%) (11136/11648)\n",
            "Epoch: 49 | Batch_idx: 100 |  Loss: (0.9570) | Acc: (95.54%) (12352/12928)\n",
            "Epoch: 49 | Batch_idx: 110 |  Loss: (0.9560) | Acc: (95.62%) (13585/14208)\n",
            "Epoch: 49 | Batch_idx: 120 |  Loss: (0.9548) | Acc: (95.66%) (14816/15488)\n",
            "Epoch: 49 | Batch_idx: 130 |  Loss: (0.9540) | Acc: (95.66%) (16041/16768)\n",
            "Epoch: 49 | Batch_idx: 140 |  Loss: (0.9544) | Acc: (95.67%) (17267/18048)\n",
            "Epoch: 49 | Batch_idx: 150 |  Loss: (0.9541) | Acc: (95.69%) (18495/19328)\n",
            "Epoch: 49 | Batch_idx: 160 |  Loss: (0.9533) | Acc: (95.74%) (19730/20608)\n",
            "Epoch: 49 | Batch_idx: 170 |  Loss: (0.9539) | Acc: (95.71%) (20950/21888)\n",
            "Epoch: 49 | Batch_idx: 180 |  Loss: (0.9537) | Acc: (95.74%) (22180/23168)\n",
            "Epoch: 49 | Batch_idx: 190 |  Loss: (0.9546) | Acc: (95.64%) (23382/24448)\n",
            "Epoch: 49 | Batch_idx: 200 |  Loss: (0.9544) | Acc: (95.63%) (24604/25728)\n",
            "Epoch: 49 | Batch_idx: 210 |  Loss: (0.9545) | Acc: (95.63%) (25827/27008)\n",
            "Epoch: 49 | Batch_idx: 220 |  Loss: (0.9551) | Acc: (95.56%) (27033/28288)\n",
            "Epoch: 49 | Batch_idx: 230 |  Loss: (0.9553) | Acc: (95.56%) (28255/29568)\n",
            "Epoch: 49 | Batch_idx: 240 |  Loss: (0.9555) | Acc: (95.56%) (29477/30848)\n",
            "Epoch: 49 | Batch_idx: 250 |  Loss: (0.9552) | Acc: (95.56%) (30702/32128)\n",
            "Epoch: 49 | Batch_idx: 260 |  Loss: (0.9551) | Acc: (95.56%) (31926/33408)\n",
            "Epoch: 49 | Batch_idx: 270 |  Loss: (0.9550) | Acc: (95.58%) (33155/34688)\n",
            "Epoch: 49 | Batch_idx: 280 |  Loss: (0.9551) | Acc: (95.59%) (34382/35968)\n",
            "Epoch: 49 | Batch_idx: 290 |  Loss: (0.9553) | Acc: (95.58%) (35600/37248)\n",
            "Epoch: 49 | Batch_idx: 300 |  Loss: (0.9559) | Acc: (95.55%) (36812/38528)\n",
            "Epoch: 49 | Batch_idx: 310 |  Loss: (0.9559) | Acc: (95.56%) (38039/39808)\n",
            "Epoch: 49 | Batch_idx: 320 |  Loss: (0.9567) | Acc: (95.50%) (39241/41088)\n",
            "Epoch: 49 | Batch_idx: 330 |  Loss: (0.9567) | Acc: (95.51%) (40464/42368)\n",
            "Epoch: 49 | Batch_idx: 340 |  Loss: (0.9570) | Acc: (95.49%) (41680/43648)\n",
            "Epoch: 49 | Batch_idx: 350 |  Loss: (0.9575) | Acc: (95.48%) (42898/44928)\n",
            "Epoch: 49 | Batch_idx: 360 |  Loss: (0.9578) | Acc: (95.47%) (44116/46208)\n",
            "Epoch: 49 | Batch_idx: 370 |  Loss: (0.9578) | Acc: (95.48%) (45340/47488)\n",
            "Epoch: 49 | Batch_idx: 380 |  Loss: (0.9581) | Acc: (95.45%) (46549/48768)\n",
            "Epoch: 49 | Batch_idx: 390 |  Loss: (0.9581) | Acc: (95.45%) (47724/50000)\n",
            "# TEST : Loss: (1.0800) | Acc: (89.21%) (8921/10000)\n"
          ]
        }
      ],
      "source": [
        "write = True\n",
        "if write:\n",
        "    with SummaryWriter() as writer:\n",
        "        run(writer)\n",
        "else:\n",
        "    run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_50GvdMYNuk",
        "outputId": "0d42ecd7-c98c-47ee-80d8-79eaf13ffbc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 hours 20 mins 24 secs for training\n"
          ]
        }
      ],
      "source": [
        "now = time.gmtime(time.time() - start_time)\n",
        "print('{} hours {} mins {} secs for training'.format(now.tm_hour, now.tm_min, now.tm_sec))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l81O19MyYNuk",
        "outputId": "6193587a-2383-4baf-8c74-03c3a595e39f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "updating: content/runs/ (stored 0%)\n",
            "  adding: content/runs/Jun09_22-23-31_2c1cfd24645c/ (stored 0%)\n",
            "  adding: content/runs/Jun09_22-23-31_2c1cfd24645c/events.out.tfevents.1654813416.2c1cfd24645c.1243.0 (deflated 68%)\n"
          ]
        }
      ],
      "source": [
        "!zip -ur /content/loss_accuracy_graph.zip /content/runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "ZmcFJbKgYNul",
        "outputId": "2421e030-4c47-476c-bcf9-9ee81fff9cea"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_35248ac8-b400-4559-8788-1e62ddd6f503\", \"loss_accuracy_graph.zip\", 133008)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/loss_accuracy_graph.zip\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "[2022] [학번] CVassignment3.ipynb의 사본",
      "provenance": []
    },
    "interpreter": {
      "hash": "bd9e381d39ab9633cca76cd3e15ee02b9664d8008f1b7c104e61dafb323a5b70"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 ('ML')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
